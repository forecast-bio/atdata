[
  {
    "objectID": "reference/protocols.html",
    "href": "reference/protocols.html",
    "title": "Protocols",
    "section": "",
    "text": "The protocols module defines abstract interfaces that enable interchangeable index backends (local Redis vs ATProto), data stores (S3 vs PDS blobs), and data sources (URL, S3, etc.).",
    "crumbs": [
      "Reference",
      "Protocols"
    ]
  },
  {
    "objectID": "reference/protocols.html#overview",
    "href": "reference/protocols.html#overview",
    "title": "Protocols",
    "section": "Overview",
    "text": "Overview\nBoth local and atmosphere implementations solve the same problem: indexed dataset storage with external data URLs. These protocols formalize that common interface:\n\nIndexEntry: Common interface for dataset index entries\nAbstractIndex: Protocol for index operations\nAbstractDataStore: Protocol for data storage operations\nDataSource: Protocol for streaming data from various backends",
    "crumbs": [
      "Reference",
      "Protocols"
    ]
  },
  {
    "objectID": "reference/protocols.html#indexentry-protocol",
    "href": "reference/protocols.html#indexentry-protocol",
    "title": "Protocols",
    "section": "IndexEntry Protocol",
    "text": "IndexEntry Protocol\nRepresents a dataset entry in any index:\n\nfrom atdata._protocols import IndexEntry\n\ndef process_entry(entry: IndexEntry) -&gt; None:\n    print(f\"Name: {entry.name}\")\n    print(f\"Schema: {entry.schema_ref}\")\n    print(f\"URLs: {entry.data_urls}\")\n    print(f\"Metadata: {entry.metadata}\")\n\n\nProperties\n\n\n\nProperty\nType\nDescription\n\n\n\n\nname\nstr\nHuman-readable dataset name\n\n\nschema_ref\nstr\nSchema reference (local:// or at://)\n\n\ndata_urls\nlist[str]\nWebDataset URLs for the data\n\n\nmetadata\ndict \\| None\nArbitrary metadata dictionary\n\n\n\n\n\nImplementations\n\nLocalDatasetEntry (from atdata.local)\nAtmosphereIndexEntry (from atdata.atmosphere)",
    "crumbs": [
      "Reference",
      "Protocols"
    ]
  },
  {
    "objectID": "reference/protocols.html#abstractindex-protocol",
    "href": "reference/protocols.html#abstractindex-protocol",
    "title": "Protocols",
    "section": "AbstractIndex Protocol",
    "text": "AbstractIndex Protocol\nDefines operations for managing schemas and datasets:\n\nfrom atdata._protocols import AbstractIndex\n\ndef list_all_datasets(index: AbstractIndex) -&gt; None:\n    \"\"\"Works with LocalIndex or AtmosphereIndex.\"\"\"\n    for entry in index.list_datasets():\n        print(f\"{entry.name}: {entry.schema_ref}\")\n\n\nDataset Operations\n\n# Insert a dataset\nentry = index.insert_dataset(\n    dataset,\n    name=\"my-dataset\",\n    schema_ref=\"local://schemas/MySample@1.0.0\",  # optional\n)\n\n# Get by name/reference\nentry = index.get_dataset(\"my-dataset\")\n\n# List all datasets\nfor entry in index.list_datasets():\n    print(entry.name)\n\n\n\nSchema Operations\n\n# Publish a schema\nschema_ref = index.publish_schema(\n    MySample,\n    version=\"1.0.0\",\n)\n\n# Get schema record\nschema = index.get_schema(schema_ref)\nprint(schema[\"name\"], schema[\"version\"])\n\n# List all schemas\nfor schema in index.list_schemas():\n    print(f\"{schema['name']}@{schema['version']}\")\n\n# Decode schema to Python type\nSampleType = index.decode_schema(schema_ref)\ndataset = atdata.Dataset[SampleType](entry.data_urls[0])\n\n\n\nImplementations\n\nLocalIndex / Index (from atdata.local)\nAtmosphereIndex (from atdata.atmosphere)",
    "crumbs": [
      "Reference",
      "Protocols"
    ]
  },
  {
    "objectID": "reference/protocols.html#abstractdatastore-protocol",
    "href": "reference/protocols.html#abstractdatastore-protocol",
    "title": "Protocols",
    "section": "AbstractDataStore Protocol",
    "text": "AbstractDataStore Protocol\nAbstracts over different storage backends:\n\nfrom atdata._protocols import AbstractDataStore\n\ndef write_dataset(store: AbstractDataStore, dataset) -&gt; list[str]:\n    \"\"\"Works with S3DataStore or future PDS blob store.\"\"\"\n    urls = store.write_shards(dataset, prefix=\"datasets/v1\")\n    return urls\n\n\nMethods\n\n# Write dataset shards\nurls = store.write_shards(\n    dataset,\n    prefix=\"datasets/mnist/v1\",\n    maxcount=10000,  # samples per shard\n)\n\n# Resolve URL for reading\nreadable_url = store.read_url(\"s3://bucket/path.tar\")\n\n# Check streaming support\nif store.supports_streaming():\n    # Can stream directly\n    pass\n\n\n\nImplementations\n\nS3DataStore (from atdata.local)",
    "crumbs": [
      "Reference",
      "Protocols"
    ]
  },
  {
    "objectID": "reference/protocols.html#datasource-protocol",
    "href": "reference/protocols.html#datasource-protocol",
    "title": "Protocols",
    "section": "DataSource Protocol",
    "text": "DataSource Protocol\nAbstracts over different data source backends for streaming dataset shards:\n\nfrom atdata._protocols import DataSource\n\ndef load_from_source(source: DataSource) -&gt; None:\n    \"\"\"Works with URLSource, S3Source, or custom implementations.\"\"\"\n    print(f\"Shards: {source.shard_list}\")\n\n    for shard_id, stream in source.shards():\n        print(f\"Reading {shard_id}\")\n        # stream is a file-like object\n\n\nMethods\n\n# Get list of shard identifiers\nshard_ids = source.shard_list  # ['data-000000.tar', 'data-000001.tar', ...]\n\n# Iterate over all shards with streams\nfor shard_id, stream in source.shards():\n    # stream is IO[bytes], can be passed to tar reader\n    process_shard(stream)\n\n# Open a specific shard\nstream = source.open_shard(\"data-000001.tar\")\n\n\n\nImplementations\n\nURLSource (from atdata) - WebDataset-compatible URLs (local, HTTP, etc.)\nS3Source (from atdata) - S3 and S3-compatible storage with boto3\n\n\n\nCreating Custom Data Sources\nImplement the DataSource protocol for custom backends:\n\nfrom typing import Iterator, IO\nfrom atdata._protocols import DataSource\n\nclass MyCustomSource:\n    \"\"\"Custom data source for proprietary storage.\"\"\"\n\n    def __init__(self, config: dict):\n        self._config = config\n        self._shards = [\"shard-001.tar\", \"shard-002.tar\"]\n\n    @property\n    def shard_list(self) -&gt; list[str]:\n        return self._shards\n\n    def shards(self) -&gt; Iterator[tuple[str, IO[bytes]]]:\n        for shard_id in self._shards:\n            stream = self._open(shard_id)\n            yield shard_id, stream\n\n    def open_shard(self, shard_id: str) -&gt; IO[bytes]:\n        if shard_id not in self._shards:\n            raise KeyError(f\"Shard not found: {shard_id}\")\n        return self._open(shard_id)\n\n    def _open(self, shard_id: str) -&gt; IO[bytes]:\n        # Implementation-specific logic\n        ...\n\n# Use with Dataset\nsource = MyCustomSource({\"endpoint\": \"...\"})\ndataset = atdata.Dataset[MySample](source)",
    "crumbs": [
      "Reference",
      "Protocols"
    ]
  },
  {
    "objectID": "reference/protocols.html#using-protocols-for-polymorphism",
    "href": "reference/protocols.html#using-protocols-for-polymorphism",
    "title": "Protocols",
    "section": "Using Protocols for Polymorphism",
    "text": "Using Protocols for Polymorphism\nWrite code that works with any backend:\n\nfrom atdata._protocols import AbstractIndex, IndexEntry\nfrom atdata import Dataset\n\ndef backup_all_datasets(\n    source: AbstractIndex,\n    target: AbstractIndex,\n) -&gt; None:\n    \"\"\"Copy all datasets from source index to target.\"\"\"\n    for entry in source.list_datasets():\n        # Decode schema from source\n        SampleType = source.decode_schema(entry.schema_ref)\n\n        # Publish schema to target\n        target_schema = target.publish_schema(SampleType)\n\n        # Load and re-insert dataset\n        ds = Dataset[SampleType](entry.data_urls[0])\n        target.insert_dataset(\n            ds,\n            name=entry.name,\n            schema_ref=target_schema,\n        )",
    "crumbs": [
      "Reference",
      "Protocols"
    ]
  },
  {
    "objectID": "reference/protocols.html#schema-reference-formats",
    "href": "reference/protocols.html#schema-reference-formats",
    "title": "Protocols",
    "section": "Schema Reference Formats",
    "text": "Schema Reference Formats\nSchema references vary by backend:\n\n\n\n\n\n\n\n\nBackend\nFormat\nExample\n\n\n\n\nLocal\natdata://local/sampleSchema/{Class}@{version}\natdata://local/sampleSchema/ImageSample@1.0.0\n\n\nAtmosphere\nat://{did}/{collection}/{rkey}\nat://did:plc:abc123/ac.foundation.dataset.sampleSchema/xyz\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLegacy local://schemas/ URIs are still supported for backward compatibility.",
    "crumbs": [
      "Reference",
      "Protocols"
    ]
  },
  {
    "objectID": "reference/protocols.html#type-checking",
    "href": "reference/protocols.html#type-checking",
    "title": "Protocols",
    "section": "Type Checking",
    "text": "Type Checking\nProtocols are runtime-checkable:\n\nfrom atdata._protocols import IndexEntry, AbstractIndex\n\n# Check if object implements protocol\nentry = index.get_dataset(\"test\")\nassert isinstance(entry, IndexEntry)\n\n# Type hints work with protocols\ndef process(index: AbstractIndex) -&gt; None:\n    ...  # IDE provides autocomplete",
    "crumbs": [
      "Reference",
      "Protocols"
    ]
  },
  {
    "objectID": "reference/protocols.html#complete-example",
    "href": "reference/protocols.html#complete-example",
    "title": "Protocols",
    "section": "Complete Example",
    "text": "Complete Example\n\nimport atdata\nfrom atdata.local import LocalIndex, S3DataStore\nfrom atdata.atmosphere import AtmosphereClient, AtmosphereIndex\nfrom atdata._protocols import AbstractIndex\nimport numpy as np\nfrom numpy.typing import NDArray\n\n# Define sample type\n@atdata.packable\nclass FeatureSample:\n    features: NDArray\n    label: int\n\n# Function works with any index\ndef count_datasets(index: AbstractIndex) -&gt; int:\n    return sum(1 for _ in index.list_datasets())\n\n# Use with local index\nlocal_index = LocalIndex()\nprint(f\"Local datasets: {count_datasets(local_index)}\")\n\n# Use with atmosphere index\nclient = AtmosphereClient()\nclient.login(\"handle.bsky.social\", \"app-password\")\natm_index = AtmosphereIndex(client)\nprint(f\"Atmosphere datasets: {count_datasets(atm_index)}\")\n\n# Migrate from local to atmosphere\ndef migrate_dataset(\n    name: str,\n    source: AbstractIndex,\n    target: AbstractIndex,\n) -&gt; None:\n    entry = source.get_dataset(name)\n    SampleType = source.decode_schema(entry.schema_ref)\n\n    # Publish schema\n    schema_ref = target.publish_schema(SampleType)\n\n    # Create dataset and insert\n    ds = atdata.Dataset[SampleType](entry.data_urls[0])\n    target.insert_dataset(ds, name=name, schema_ref=schema_ref)\n\nmigrate_dataset(\"my-features\", local_index, atm_index)",
    "crumbs": [
      "Reference",
      "Protocols"
    ]
  },
  {
    "objectID": "reference/protocols.html#related",
    "href": "reference/protocols.html#related",
    "title": "Protocols",
    "section": "Related",
    "text": "Related\n\nLocal Storage - LocalIndex and S3DataStore\nAtmosphere - AtmosphereIndex\nPromotion - Local to atmosphere migration\nload_dataset - Using indexes with load_dataset()",
    "crumbs": [
      "Reference",
      "Protocols"
    ]
  },
  {
    "objectID": "reference/datasets.html",
    "href": "reference/datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "The Dataset class provides typed iteration over WebDataset tar files with automatic batching and lens transformations.",
    "crumbs": [
      "Reference",
      "Datasets"
    ]
  },
  {
    "objectID": "reference/datasets.html#creating-a-dataset",
    "href": "reference/datasets.html#creating-a-dataset",
    "title": "Datasets",
    "section": "Creating a Dataset",
    "text": "Creating a Dataset\n\nimport atdata\nfrom numpy.typing import NDArray\n\n@atdata.packable\nclass ImageSample:\n    image: NDArray\n    label: str\n\n# Single shard (string URL - most common)\ndataset = atdata.Dataset[ImageSample](\"data-000000.tar\")\n\n# Multiple shards with brace notation\ndataset = atdata.Dataset[ImageSample](\"data-{000000..000009}.tar\")\n\nThe type parameter [ImageSample] specifies what sample type the dataset contains. This enables type-safe iteration and automatic deserialization.",
    "crumbs": [
      "Reference",
      "Datasets"
    ]
  },
  {
    "objectID": "reference/datasets.html#data-sources",
    "href": "reference/datasets.html#data-sources",
    "title": "Datasets",
    "section": "Data Sources",
    "text": "Data Sources\nDatasets can be created from different data sources using the DataSource protocol:\n\nURL Source (default)\nWhen you pass a string to Dataset, it automatically wraps it in a URLSource:\n\n# These are equivalent:\ndataset = atdata.Dataset[ImageSample](\"data-{000000..000009}.tar\")\ndataset = atdata.Dataset[ImageSample](atdata.URLSource(\"data-{000000..000009}.tar\"))\n\n\n\nS3 Source\nFor private S3 buckets or S3-compatible storage (Cloudflare R2, MinIO), use S3Source:\n\n# From explicit credentials\nsource = atdata.S3Source(\n    bucket=\"my-bucket\",\n    keys=[\"data-000000.tar\", \"data-000001.tar\"],\n    endpoint=\"https://my-r2-account.r2.cloudflarestorage.com\",\n    access_key=\"AKID...\",\n    secret_key=\"SECRET...\",\n)\ndataset = atdata.Dataset[ImageSample](source)\n\n# From S3 URLs\nsource = atdata.S3Source.from_urls([\n    \"s3://my-bucket/data-000000.tar\",\n    \"s3://my-bucket/data-000001.tar\",\n])\ndataset = atdata.Dataset[ImageSample](source)\n\n\n\n\n\n\n\nNote\n\n\n\nS3Source uses boto3 for streaming, enabling authentication with private buckets. For public S3 URLs, a string URL with URLSource works directly.",
    "crumbs": [
      "Reference",
      "Datasets"
    ]
  },
  {
    "objectID": "reference/datasets.html#iteration-modes",
    "href": "reference/datasets.html#iteration-modes",
    "title": "Datasets",
    "section": "Iteration Modes",
    "text": "Iteration Modes\n\nOrdered Iteration\nIterate through samples in their original order:\n\n# With batching (default batch_size=1)\nfor batch in dataset.ordered(batch_size=32):\n    images = batch.image  # numpy array (32, H, W, C)\n    labels = batch.label  # list of 32 strings\n\n# Without batching (raw samples)\nfor sample in dataset.ordered(batch_size=None):\n    print(sample.label)\n\n\n\nShuffled Iteration\nIterate with randomized order at both shard and sample levels:\n\nfor batch in dataset.shuffled(batch_size=32):\n    # Samples are shuffled\n    process(batch)\n\n# Control shuffle buffer sizes\nfor batch in dataset.shuffled(\n    buffer_shards=100,    # Shards to buffer (default: 100)\n    buffer_samples=10000, # Samples to buffer (default: 10,000)\n    batch_size=32,\n):\n    process(batch)\n\n\n\n\n\n\n\nTip\n\n\n\nLarger buffer sizes increase randomness but use more memory. For training, buffer_samples=10000 is usually a good balance.",
    "crumbs": [
      "Reference",
      "Datasets"
    ]
  },
  {
    "objectID": "reference/datasets.html#samplebatch",
    "href": "reference/datasets.html#samplebatch",
    "title": "Datasets",
    "section": "SampleBatch",
    "text": "SampleBatch\nWhen iterating with a batch_size, each iteration yields a SampleBatch with automatic attribute aggregation.\n\n@atdata.packable\nclass Sample:\n    features: NDArray  # shape (256,)\n    label: str\n    score: float\n\nfor batch in dataset.ordered(batch_size=16):\n    # NDArray fields are stacked with a batch dimension\n    features = batch.features  # numpy array (16, 256)\n\n    # Other fields become lists\n    labels = batch.label       # list of 16 strings\n    scores = batch.score       # list of 16 floats\n\nResults are cached, so accessing the same attribute multiple times is efficient.",
    "crumbs": [
      "Reference",
      "Datasets"
    ]
  },
  {
    "objectID": "reference/datasets.html#type-transformations-with-lenses",
    "href": "reference/datasets.html#type-transformations-with-lenses",
    "title": "Datasets",
    "section": "Type Transformations with Lenses",
    "text": "Type Transformations with Lenses\nView a dataset through a different sample type using registered lenses:\n\n@atdata.packable\nclass SimplifiedSample:\n    label: str\n\n@atdata.lens\ndef simplify(src: ImageSample) -&gt; SimplifiedSample:\n    return SimplifiedSample(label=src.label)\n\n# Transform dataset to different type\nsimple_ds = dataset.as_type(SimplifiedSample)\n\nfor batch in simple_ds.ordered(batch_size=16):\n    print(batch.label)  # Only label field available\n\nSee Lenses for details on defining transformations.",
    "crumbs": [
      "Reference",
      "Datasets"
    ]
  },
  {
    "objectID": "reference/datasets.html#dataset-properties",
    "href": "reference/datasets.html#dataset-properties",
    "title": "Datasets",
    "section": "Dataset Properties",
    "text": "Dataset Properties\n\nShard List\nGet the list of individual tar files:\n\ndataset = atdata.Dataset[Sample](\"data-{000000..000009}.tar\")\nshards = dataset.shard_list\n# ['data-000000.tar', 'data-000001.tar', ..., 'data-000009.tar']\n\n\n\nMetadata\nDatasets can have associated metadata from a URL:\n\ndataset = atdata.Dataset[Sample](\n    \"data-{000000..000009}.tar\",\n    metadata_url=\"https://example.com/metadata.msgpack\"\n)\n\n# Fetched and cached on first access\nmetadata = dataset.metadata  # dict or None",
    "crumbs": [
      "Reference",
      "Datasets"
    ]
  },
  {
    "objectID": "reference/datasets.html#writing-datasets",
    "href": "reference/datasets.html#writing-datasets",
    "title": "Datasets",
    "section": "Writing Datasets",
    "text": "Writing Datasets\nUse WebDataset’s TarWriter or ShardWriter to create datasets:\n\nimport webdataset as wds\nimport numpy as np\n\nsamples = [\n    ImageSample(image=np.random.rand(224, 224, 3).astype(np.float32), label=\"cat\")\n    for _ in range(100)\n]\n\n# Single tar file\nwith wds.writer.TarWriter(\"data-000000.tar\") as sink:\n    for i, sample in enumerate(samples):\n        sink.write({**sample.as_wds, \"__key__\": f\"sample_{i:06d}\"})\n\n# Multiple shards with automatic splitting\nwith wds.writer.ShardWriter(\"data-%06d.tar\", maxcount=1000) as sink:\n    for i, sample in enumerate(samples):\n        sink.write({**sample.as_wds, \"__key__\": f\"sample_{i:06d}\"})",
    "crumbs": [
      "Reference",
      "Datasets"
    ]
  },
  {
    "objectID": "reference/datasets.html#parquet-export",
    "href": "reference/datasets.html#parquet-export",
    "title": "Datasets",
    "section": "Parquet Export",
    "text": "Parquet Export\nExport dataset contents to parquet format:\n\n# Export entire dataset\ndataset.to_parquet(\"output.parquet\")\n\n# Export with custom field mapping\ndef extract_fields(sample):\n    return {\"label\": sample.label, \"score\": sample.confidence}\n\ndataset.to_parquet(\"output.parquet\", sample_map=extract_fields)\n\n# Export in segments\ndataset.to_parquet(\"output.parquet\", maxcount=10000)\n# Creates output-000000.parquet, output-000001.parquet, etc.",
    "crumbs": [
      "Reference",
      "Datasets"
    ]
  },
  {
    "objectID": "reference/datasets.html#url-formats",
    "href": "reference/datasets.html#url-formats",
    "title": "Datasets",
    "section": "URL Formats",
    "text": "URL Formats\nWhen using string URLs (via URLSource), WebDataset supports various formats:\n\n\n\n\n\n\n\nFormat\nExample\n\n\n\n\nLocal files\n./data/file.tar, /absolute/path/file-{000000..000009}.tar\n\n\nHTTP/HTTPS\nhttps://example.com/data-{000000..000009}.tar\n\n\nGoogle Cloud\ngs://bucket/path/file.tar\n\n\n\nFor S3 with authentication, use S3Source instead of s3:// URLs.",
    "crumbs": [
      "Reference",
      "Datasets"
    ]
  },
  {
    "objectID": "reference/datasets.html#dataset-properties-1",
    "href": "reference/datasets.html#dataset-properties-1",
    "title": "Datasets",
    "section": "Dataset Properties",
    "text": "Dataset Properties\n\nSource\nAccess the underlying DataSource:\n\ndataset = atdata.Dataset[Sample](\"data.tar\")\nsource = dataset.source  # URLSource instance\nprint(source.shard_list)  # ['data.tar']\n\n\n\nSample Type\nGet the type parameter used to create the dataset:\n\ndataset = atdata.Dataset[ImageSample](\"data.tar\")\nprint(dataset.sample_type)  # &lt;class 'ImageSample'&gt;\nprint(dataset.batch_type)   # SampleBatch[ImageSample]",
    "crumbs": [
      "Reference",
      "Datasets"
    ]
  },
  {
    "objectID": "reference/datasets.html#related",
    "href": "reference/datasets.html#related",
    "title": "Datasets",
    "section": "Related",
    "text": "Related\n\nPackable Samples - Defining typed samples\nLenses - Type transformations\nload_dataset - HuggingFace-style loading API\nProtocols - DataSource protocol details",
    "crumbs": [
      "Reference",
      "Datasets"
    ]
  },
  {
    "objectID": "reference/architecture.html",
    "href": "reference/architecture.html",
    "title": "Architecture Overview",
    "section": "",
    "text": "atdata is designed around a simple but powerful idea: typed, serializable samples that can flow seamlessly between local development, team storage, and a federated network. This page explains the architectural decisions and how the components work together.",
    "crumbs": [
      "Reference",
      "Architecture Overview"
    ]
  },
  {
    "objectID": "reference/architecture.html#design-philosophy",
    "href": "reference/architecture.html#design-philosophy",
    "title": "Architecture Overview",
    "section": "Design Philosophy",
    "text": "Design Philosophy\n\nThe Problem\nMachine learning workflows involve datasets at every stage—training data, validation sets, embeddings, features, and model outputs. These datasets are often:\n\nUntyped: Raw files with implicit schemas, leading to runtime errors\nSiloed: Stuck in one location (local disk, team bucket, or cloud storage)\nUndiscoverable: No standard way to find and share datasets across teams or organizations\n\n\n\nThe Solution\natdata provides a three-layer architecture that addresses each problem:\n┌─────────────────────────────────────────────────────────────┐\n│  Layer 3: Federation (ATProto Atmosphere)                   │\n│  - Decentralized discovery and sharing                      │\n│  - Content-addressable identifiers                          │\n│  - Cross-organization dataset federation                    │\n└─────────────────────────────────────────────────────────────┘\n                              ↑\n                        Promotion\n                              ↑\n┌─────────────────────────────────────────────────────────────┐\n│  Layer 2: Team Storage (Redis + S3)                         │\n│  - Shared index for team discovery                          │\n│  - Scalable object storage for data                         │\n│  - Schema registry for type consistency                     │\n└─────────────────────────────────────────────────────────────┘\n                              ↑\n                         Insert\n                              ↑\n┌─────────────────────────────────────────────────────────────┐\n│  Layer 1: Local Development                                 │\n│  - Typed samples with automatic serialization               │\n│  - WebDataset tar files for efficient storage               │\n│  - Lens transformations for schema flexibility              │\n└─────────────────────────────────────────────────────────────┘",
    "crumbs": [
      "Reference",
      "Architecture Overview"
    ]
  },
  {
    "objectID": "reference/architecture.html#core-components",
    "href": "reference/architecture.html#core-components",
    "title": "Architecture Overview",
    "section": "Core Components",
    "text": "Core Components\n\nPackableSample: The Foundation\nEverything in atdata starts with PackableSample—a base class that makes Python dataclasses serializable with msgpack:\n\n@atdata.packable\nclass ImageSample:\n    image: NDArray       # Automatically converted to/from bytes\n    label: str           # Standard msgpack serialization\n    confidence: float\n\nKey features:\n\nAutomatic NDArray handling: Numpy arrays are serialized efficiently\nType safety: Field types are preserved and validated\nRound-trip fidelity: Serialize → deserialize always produces identical data\n\nThe @packable decorator is syntactic sugar that:\n\nConverts your class to a dataclass\nAdds PackableSample as a base class\nRegisters a lens from DictSample for flexible loading\n\n\n\nDataset: Typed Iteration\nThe Dataset[T] class wraps WebDataset tar archives with type information:\n\ndataset = atdata.Dataset[ImageSample](\"data-{000000..000009}.tar\")\n\nfor batch in dataset.shuffled(batch_size=32):\n    images = batch.image  # Stacked NDArray: (32, H, W, C)\n    labels = batch.label  # List of 32 strings\n\nWhy WebDataset?\nWebDataset is a battle-tested format for large-scale ML training:\n\nStreaming: No need to download entire datasets\nSharding: Data split across multiple tar files for parallelism\nShuffling: Two-level shuffling (shard + sample) for training\n\natdata adds:\n\nType safety: Know the schema at compile time\nBatch aggregation: NDArrays are automatically stacked\nLens transformations: View data through different schemas\n\n\n\nSampleBatch: Automatic Aggregation\nWhen iterating with batch_size, atdata returns SampleBatch[T] objects that aggregate sample attributes:\n\nbatch = SampleBatch[ImageSample](samples)\n\n# NDArray fields → stacked numpy array with batch dimension\nbatch.image.shape  # (batch_size, H, W, C)\n\n# Other fields → list\nbatch.label  # [\"cat\", \"dog\", \"bird\", ...]\n\nThis eliminates boilerplate collation code and works automatically for any PackableSample type.\n\n\nLens: Schema Transformations\nLenses enable viewing datasets through different schemas without duplicating data:\n\n@atdata.packable\nclass SimplifiedSample:\n    label: str\n\n@atdata.lens\ndef simplify(src: ImageSample) -&gt; SimplifiedSample:\n    return SimplifiedSample(label=src.label)\n\n# View dataset through simplified schema\nsimple_ds = dataset.as_type(SimplifiedSample)\n\nWhen to use lenses:\n\nReducing fields: Drop unnecessary data for specific tasks\nTransforming data: Compute derived fields on-the-fly\nSchema migration: Handle version differences between datasets\n\nLenses are registered globally in a LensNetwork, enabling automatic discovery of transformation paths.",
    "crumbs": [
      "Reference",
      "Architecture Overview"
    ]
  },
  {
    "objectID": "reference/architecture.html#storage-backends",
    "href": "reference/architecture.html#storage-backends",
    "title": "Architecture Overview",
    "section": "Storage Backends",
    "text": "Storage Backends\n\nLocal Index (Redis + S3)\nFor team-scale usage, atdata provides a two-component storage system:\nRedis Index: Stores metadata and enables fast lookups\n\nDataset entries (name, schema, URLs, metadata)\nSchema registry (type definitions)\nCID-based content addressing\n\nS3 DataStore: Stores actual data files\n\nWebDataset tar shards\nAny S3-compatible storage (AWS, MinIO, Cloudflare R2)\n\n\nstore = S3DataStore(credentials=creds, bucket=\"datasets\")\nindex = LocalIndex(data_store=store)\n\n# Insert dataset: writes to S3, indexes in Redis\nentry = index.insert_dataset(dataset, name=\"training-v1\")\n\nWhy this split?\n\nSeparation of concerns: Metadata queries don’t touch data storage\nFlexibility: Use any S3-compatible storage\nScalability: Redis handles high-throughput lookups; S3 handles large files\n\n\n\nAtmosphere Index (ATProto)\nFor public or cross-organization sharing, atdata integrates with the AT Protocol:\nATProto PDS: Your Personal Data Server stores records\n\nSchema definitions\nDataset index records\nLens transformation records\n\nPDSBlobStore: Optional blob storage on your PDS\n\nStore actual data shards as ATProto blobs\nFully decentralized—no external dependencies\n\n\nclient = AtmosphereClient()\nclient.login(\"handle.bsky.social\", \"app-password\")\n\nstore = PDSBlobStore(client)\nindex = AtmosphereIndex(client, data_store=store)\n\n# Publish: creates ATProto records, uploads blobs\nentry = index.insert_dataset(dataset, name=\"public-features\")",
    "crumbs": [
      "Reference",
      "Architecture Overview"
    ]
  },
  {
    "objectID": "reference/architecture.html#protocol-abstractions",
    "href": "reference/architecture.html#protocol-abstractions",
    "title": "Architecture Overview",
    "section": "Protocol Abstractions",
    "text": "Protocol Abstractions\natdata uses protocols (structural typing) to enable backend interoperability:\n\nAbstractIndex\nCommon interface for both LocalIndex and AtmosphereIndex:\n\ndef process_dataset(index: AbstractIndex, name: str):\n    entry = index.get_dataset(name)\n    schema = index.decode_schema(entry.schema_ref)\n    # Works with either LocalIndex or AtmosphereIndex\n\nKey methods:\n\ninsert_dataset() / get_dataset(): Dataset CRUD\npublish_schema() / decode_schema(): Schema management\nlist_datasets() / list_schemas(): Discovery\n\n\n\nAbstractDataStore\nCommon interface for S3DataStore and PDSBlobStore:\n\ndef write_to_store(store: AbstractDataStore, dataset: Dataset):\n    urls = store.write_shards(dataset, prefix=\"data/v1\")\n    # Works with S3 or PDS blob storage\n\n\n\nDataSource\nCommon interface for data streaming:\n\nURLSource: WebDataset-compatible URLs\nS3Source: S3 with explicit credentials\nBlobSource: ATProto PDS blobs",
    "crumbs": [
      "Reference",
      "Architecture Overview"
    ]
  },
  {
    "objectID": "reference/architecture.html#data-flow-local-to-federation",
    "href": "reference/architecture.html#data-flow-local-to-federation",
    "title": "Architecture Overview",
    "section": "Data Flow: Local to Federation",
    "text": "Data Flow: Local to Federation\nA typical workflow progresses through three stages:\n\nStage 1: Local Development\n\n# Define type and create samples\n@atdata.packable\nclass MySample:\n    features: NDArray\n    label: str\n\n# Write to local tar\nwith wds.writer.TarWriter(\"data.tar\") as sink:\n    for sample in samples:\n        sink.write(sample.as_wds)\n\n# Iterate locally\ndataset = atdata.Dataset[MySample](\"data.tar\")\n\n\n\nStage 2: Team Storage\n\n# Set up team storage\nstore = S3DataStore(credentials=team_creds, bucket=\"team-datasets\")\nindex = LocalIndex(data_store=store)\n\n# Publish schema and insert\nindex.publish_schema(MySample, version=\"1.0.0\")\nentry = index.insert_dataset(dataset, name=\"my-features\")\n\n# Team members can now load via index\nds = load_dataset(\"@local/my-features\", index=index)\n\n\n\nStage 3: Federation\n\n# Promote to atmosphere\nclient = AtmosphereClient()\nclient.login(\"handle.bsky.social\", \"app-password\")\n\nat_uri = promote_to_atmosphere(entry, index, client)\n\n# Anyone can now discover and load\n# ds = load_dataset(\"@handle.bsky.social/my-features\")",
    "crumbs": [
      "Reference",
      "Architecture Overview"
    ]
  },
  {
    "objectID": "reference/architecture.html#content-addressing",
    "href": "reference/architecture.html#content-addressing",
    "title": "Architecture Overview",
    "section": "Content Addressing",
    "text": "Content Addressing\natdata uses CIDs (Content Identifiers) for content-addressable storage:\n\nSchema CIDs: Hash of schema definition\nEntry CIDs: Hash of (schema_ref, data_urls)\nBlob CIDs: Hash of data content\n\nBenefits:\n\nDeduplication: Identical content has identical CID\nIntegrity: Verify data matches expected hash\nATProto compatibility: CIDs are native to the AT Protocol",
    "crumbs": [
      "Reference",
      "Architecture Overview"
    ]
  },
  {
    "objectID": "reference/architecture.html#extension-points",
    "href": "reference/architecture.html#extension-points",
    "title": "Architecture Overview",
    "section": "Extension Points",
    "text": "Extension Points\natdata is designed for extensibility:\n\nCustom DataSources\nImplement the DataSource protocol to add new storage backends:\n\nclass MyCustomSource:\n    def list_shards(self) -&gt; list[str]: ...\n    def open_shard(self, shard_id: str) -&gt; IO[bytes]: ...\n\n    @property\n    def shards(self) -&gt; Iterator[tuple[str, IO[bytes]]]: ...\n\n\n\nCustom Lenses\nRegister transformations between any PackableSample types:\n\n@atdata.lens\ndef my_transform(src: SourceType) -&gt; TargetType:\n    return TargetType(...)\n\n@my_transform.putter\ndef my_transform_put(view: TargetType, src: SourceType) -&gt; SourceType:\n    return SourceType(...)\n\n\n\nSchema Extensions\nThe schema format supports custom metadata for domain-specific needs:\n\nindex.publish_schema(\n    MySample,\n    version=\"1.0.0\",\n    metadata={\"domain\": \"chemistry\", \"units\": \"mol/L\"},\n)",
    "crumbs": [
      "Reference",
      "Architecture Overview"
    ]
  },
  {
    "objectID": "reference/architecture.html#summary",
    "href": "reference/architecture.html#summary",
    "title": "Architecture Overview",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\n\nComponent\nPurpose\nKey Classes\n\n\n\n\nSamples\nTyped, serializable data\nPackableSample, @packable\n\n\nDatasets\nTyped iteration over WebDataset\nDataset[T], SampleBatch[T]\n\n\nLenses\nSchema transformations\nLens, @lens, LensNetwork\n\n\nLocal Storage\nTeam-scale index + data\nLocalIndex, S3DataStore\n\n\nAtmosphere\nFederated sharing\nAtmosphereIndex, PDSBlobStore\n\n\nProtocols\nBackend abstraction\nAbstractIndex, AbstractDataStore, DataSource\n\n\n\nThe architecture enables a smooth progression from local experimentation to team collaboration to public federation, all while maintaining type safety and efficient data handling.",
    "crumbs": [
      "Reference",
      "Architecture Overview"
    ]
  },
  {
    "objectID": "reference/architecture.html#related",
    "href": "reference/architecture.html#related",
    "title": "Architecture Overview",
    "section": "Related",
    "text": "Related\n\nPackable Samples - Defining sample types\nDatasets - Dataset iteration and batching\nLocal Storage - Redis + S3 backend\nAtmosphere - ATProto federation\nProtocols - Abstract interfaces",
    "crumbs": [
      "Reference",
      "Architecture Overview"
    ]
  },
  {
    "objectID": "reference/atmosphere.html",
    "href": "reference/atmosphere.html",
    "title": "Atmosphere (ATProto Integration)",
    "section": "",
    "text": "The atmosphere module enables publishing and discovering datasets on the ATProto network, creating a federated ecosystem for typed datasets.",
    "crumbs": [
      "Reference",
      "Atmosphere (ATProto Integration)"
    ]
  },
  {
    "objectID": "reference/atmosphere.html#installation",
    "href": "reference/atmosphere.html#installation",
    "title": "Atmosphere (ATProto Integration)",
    "section": "Installation",
    "text": "Installation\npip install atdata[atmosphere]\n# or\npip install atproto",
    "crumbs": [
      "Reference",
      "Atmosphere (ATProto Integration)"
    ]
  },
  {
    "objectID": "reference/atmosphere.html#overview",
    "href": "reference/atmosphere.html#overview",
    "title": "Atmosphere (ATProto Integration)",
    "section": "Overview",
    "text": "Overview\nATProto integration publishes datasets, schemas, and lenses as records in the ac.foundation.dataset.* namespace. This enables:\n\nDiscovery through the ATProto network\nFederation across different hosts\nVerifiability through content-addressable records",
    "crumbs": [
      "Reference",
      "Atmosphere (ATProto Integration)"
    ]
  },
  {
    "objectID": "reference/atmosphere.html#atmosphereclient",
    "href": "reference/atmosphere.html#atmosphereclient",
    "title": "Atmosphere (ATProto Integration)",
    "section": "AtmosphereClient",
    "text": "AtmosphereClient\nThe client handles authentication and record operations:\n\nfrom atdata.atmosphere import AtmosphereClient\n\nclient = AtmosphereClient()\n\n# Login with app-specific password (not your main password!)\nclient.login(\"alice.bsky.social\", \"app-password\")\n\nprint(client.did)     # 'did:plc:...'\nprint(client.handle)  # 'alice.bsky.social'\n\n\n\n\n\n\n\nWarning\n\n\n\nAlways use an app-specific password, not your main Bluesky password. Create app passwords at bsky.app/settings/app-passwords.\n\n\n\nSession Management\nSave and restore sessions to avoid re-authentication:\n\n# Export session for later\nsession_string = client.export_session()\n\n# Later: restore session\nnew_client = AtmosphereClient()\nnew_client.login_with_session(session_string)\n\n\n\nCustom PDS\nConnect to a custom PDS instead of bsky.social:\n\nclient = AtmosphereClient(base_url=\"https://pds.example.com\")",
    "crumbs": [
      "Reference",
      "Atmosphere (ATProto Integration)"
    ]
  },
  {
    "objectID": "reference/atmosphere.html#pdsblobstore",
    "href": "reference/atmosphere.html#pdsblobstore",
    "title": "Atmosphere (ATProto Integration)",
    "section": "PDSBlobStore",
    "text": "PDSBlobStore\nStore dataset shards as ATProto blobs for fully decentralized storage:\n\nfrom atdata.atmosphere import AtmosphereClient, PDSBlobStore\n\nclient = AtmosphereClient()\nclient.login(\"handle.bsky.social\", \"app-password\")\n\nstore = PDSBlobStore(client)\n\n# Write shards as blobs\nurls = store.write_shards(dataset, prefix=\"my-data/v1\")\n# Returns: ['at://did:plc:.../blob/bafyrei...', ...]\n\n# Transform AT URIs to HTTP URLs for reading\nhttp_url = store.read_url(urls[0])\n# Returns: 'https://pds.example.com/xrpc/com.atproto.sync.getBlob?...'\n\n# Create a BlobSource for streaming\nsource = store.create_source(urls)\nds = atdata.Dataset[MySample](source)\n\n\nSize Limits\nPDS blobs typically have size limits (often 50MB-5GB depending on the PDS). Use maxcount and maxsize parameters to control shard sizes:\n\nurls = store.write_shards(\n    dataset,\n    prefix=\"large-data/v1\",\n    maxcount=5000,    # Max 5000 samples per shard\n    maxsize=50e6,     # Max 50MB per shard\n)",
    "crumbs": [
      "Reference",
      "Atmosphere (ATProto Integration)"
    ]
  },
  {
    "objectID": "reference/atmosphere.html#blobsource",
    "href": "reference/atmosphere.html#blobsource",
    "title": "Atmosphere (ATProto Integration)",
    "section": "BlobSource",
    "text": "BlobSource\nRead datasets stored as PDS blobs:\n\nfrom atdata import BlobSource\n\n# From blob references\nsource = BlobSource.from_refs([\n    {\"did\": \"did:plc:abc123\", \"cid\": \"bafyrei111\"},\n    {\"did\": \"did:plc:abc123\", \"cid\": \"bafyrei222\"},\n])\n\n# Or from PDSBlobStore\nsource = store.create_source(urls)\n\n# Use with Dataset\nds = atdata.Dataset[MySample](source)\nfor batch in ds.ordered(batch_size=32):\n    process(batch)",
    "crumbs": [
      "Reference",
      "Atmosphere (ATProto Integration)"
    ]
  },
  {
    "objectID": "reference/atmosphere.html#atmosphereindex",
    "href": "reference/atmosphere.html#atmosphereindex",
    "title": "Atmosphere (ATProto Integration)",
    "section": "AtmosphereIndex",
    "text": "AtmosphereIndex\nThe unified interface for ATProto operations, implementing the AbstractIndex protocol:\n\nfrom atdata.atmosphere import AtmosphereClient, AtmosphereIndex, PDSBlobStore\n\nclient = AtmosphereClient()\nclient.login(\"handle.bsky.social\", \"app-password\")\n\n# Without blob storage (use external URLs)\nindex = AtmosphereIndex(client)\n\n# With PDS blob storage (recommended for full decentralization)\nstore = PDSBlobStore(client)\nindex = AtmosphereIndex(client, data_store=store)\n\n\nPublishing Schemas\n\nimport atdata\nfrom numpy.typing import NDArray\n\n@atdata.packable\nclass ImageSample:\n    image: NDArray\n    label: str\n    confidence: float\n\n# Publish schema\nschema_uri = index.publish_schema(\n    ImageSample,\n    version=\"1.0.0\",\n    description=\"Image classification sample\",\n)\n# Returns: \"at://did:plc:.../ac.foundation.dataset.sampleSchema/...\"\n\n\n\nPublishing Datasets\n\ndataset = atdata.Dataset[ImageSample](\"data-{000000..000009}.tar\")\n\nentry = index.insert_dataset(\n    dataset,\n    name=\"imagenet-subset\",\n    schema_ref=schema_uri,           # Optional - auto-publishes if omitted\n    description=\"ImageNet subset\",\n    tags=[\"images\", \"classification\"],\n    license=\"MIT\",\n)\n\nprint(entry.uri)        # AT URI of the record\nprint(entry.data_urls)  # WebDataset URLs\n\n\n\nListing and Retrieving\n\n# List your datasets\nfor entry in index.list_datasets():\n    print(f\"{entry.name}: {entry.schema_ref}\")\n\n# List from another user\nfor entry in index.list_datasets(repo=\"did:plc:other-user\"):\n    print(entry.name)\n\n# Get specific dataset\nentry = index.get_dataset(\"at://did:plc:.../ac.foundation.dataset.record/...\")\n\n# List schemas\nfor schema in index.list_schemas():\n    print(f\"{schema['name']} v{schema['version']}\")\n\n# Decode schema to Python type\nSampleType = index.decode_schema(schema_uri)",
    "crumbs": [
      "Reference",
      "Atmosphere (ATProto Integration)"
    ]
  },
  {
    "objectID": "reference/atmosphere.html#lower-level-publishers",
    "href": "reference/atmosphere.html#lower-level-publishers",
    "title": "Atmosphere (ATProto Integration)",
    "section": "Lower-Level Publishers",
    "text": "Lower-Level Publishers\nFor more control, use the individual publisher classes:\n\nSchemaPublisher\n\nfrom atdata.atmosphere import SchemaPublisher\n\npublisher = SchemaPublisher(client)\n\nuri = publisher.publish(\n    ImageSample,\n    name=\"ImageSample\",\n    version=\"1.0.0\",\n    description=\"Image with label\",\n    metadata={\"source\": \"training\"},\n)\n\n\n\nDatasetPublisher\n\nfrom atdata.atmosphere import DatasetPublisher\n\npublisher = DatasetPublisher(client)\n\nuri = publisher.publish(\n    dataset,\n    name=\"training-images\",\n    schema_uri=schema_uri,           # Required if auto_publish_schema=False\n    auto_publish_schema=True,        # Publish schema automatically\n    description=\"Training images\",\n    tags=[\"training\", \"images\"],\n    license=\"MIT\",\n)\n\n\nBlob Storage\nThere are two approaches to storing data as ATProto blobs:\nApproach 1: PDSBlobStore (Recommended)\nUse PDSBlobStore with AtmosphereIndex for automatic shard management:\n\nfrom atdata.atmosphere import PDSBlobStore, AtmosphereIndex\n\nstore = PDSBlobStore(client)\nindex = AtmosphereIndex(client, data_store=store)\n\n# Dataset shards are automatically uploaded as blobs\nentry = index.insert_dataset(\n    dataset,\n    name=\"my-dataset\",\n    schema_ref=schema_uri,\n)\n\n# Later: load using BlobSource\nsource = store.create_source(entry.data_urls)\nds = atdata.Dataset[MySample](source)\n\nApproach 2: Manual Blob Publishing\nFor more control, use DatasetPublisher.publish_with_blobs() directly:\n\nimport io\nimport webdataset as wds\n\n# Create tar data in memory\ntar_buffer = io.BytesIO()\nwith wds.writer.TarWriter(tar_buffer) as sink:\n    for i, sample in enumerate(samples):\n        sink.write({**sample.as_wds, \"__key__\": f\"{i:06d}\"})\n\n# Publish with blob storage\nuri = publisher.publish_with_blobs(\n    blobs=[tar_buffer.getvalue()],\n    schema_uri=schema_uri,\n    name=\"small-dataset\",\n    description=\"Dataset stored in ATProto blobs\",\n    tags=[\"small\", \"demo\"],\n)\n\nLoading Blob-Stored Datasets\n\nfrom atdata.atmosphere import DatasetLoader\nfrom atdata import BlobSource\n\nloader = DatasetLoader(client)\n\n# Check storage type\nstorage_type = loader.get_storage_type(uri)  # \"external\" or \"blobs\"\n\nif storage_type == \"blobs\":\n    # Get blob URLs and create BlobSource\n    blob_urls = loader.get_blob_urls(uri)\n    # Parse to blob refs for BlobSource\n    # Or use loader.to_dataset() which handles this automatically\n\n# to_dataset() handles both storage types automatically\ndataset = loader.to_dataset(uri, MySample)\nfor batch in dataset.ordered(batch_size=32):\n    process(batch)\n\n\n\n\nLensPublisher\n\nfrom atdata.atmosphere import LensPublisher\n\npublisher = LensPublisher(client)\n\n# With code references\nuri = publisher.publish(\n    name=\"simplify\",\n    source_schema=full_schema_uri,\n    target_schema=simple_schema_uri,\n    description=\"Extract label only\",\n    getter_code={\n        \"repository\": \"https://github.com/org/repo\",\n        \"commit\": \"abc123def...\",\n        \"path\": \"transforms/simplify.py:simplify_getter\",\n    },\n    putter_code={\n        \"repository\": \"https://github.com/org/repo\",\n        \"commit\": \"abc123def...\",\n        \"path\": \"transforms/simplify.py:simplify_putter\",\n    },\n)\n\n# Or publish from a Lens object\nfrom atdata.lens import lens\n\n@lens\ndef simplify(src: FullSample) -&gt; SimpleSample:\n    return SimpleSample(label=src.label)\n\nuri = publisher.publish_from_lens(\n    simplify,\n    source_schema=full_schema_uri,\n    target_schema=simple_schema_uri,\n)",
    "crumbs": [
      "Reference",
      "Atmosphere (ATProto Integration)"
    ]
  },
  {
    "objectID": "reference/atmosphere.html#lower-level-loaders",
    "href": "reference/atmosphere.html#lower-level-loaders",
    "title": "Atmosphere (ATProto Integration)",
    "section": "Lower-Level Loaders",
    "text": "Lower-Level Loaders\nFor direct access to records, use the loader classes:\n\nSchemaLoader\n\nfrom atdata.atmosphere import SchemaLoader\n\nloader = SchemaLoader(client)\n\n# Get a specific schema\nschema = loader.get(\"at://did:plc:abc/ac.foundation.dataset.sampleSchema/xyz\")\nprint(schema[\"name\"], schema[\"version\"])\n\n# List all schemas from a repository\nfor schema in loader.list_all(repo=\"did:plc:other-user\"):\n    print(schema[\"name\"])\n\n\n\nDatasetLoader\n\nfrom atdata.atmosphere import DatasetLoader\n\nloader = DatasetLoader(client)\n\n# Get a specific dataset record\nrecord = loader.get(\"at://did:plc:abc/ac.foundation.dataset.record/xyz\")\n\n# Check storage type\nstorage_type = loader.get_storage_type(uri)  # \"external\" or \"blobs\"\n\n# Get URLs based on storage type\nif storage_type == \"external\":\n    urls = loader.get_urls(uri)\nelse:\n    urls = loader.get_blob_urls(uri)\n\n# Get metadata\nmetadata = loader.get_metadata(uri)\n\n# Create a Dataset object directly\ndataset = loader.to_dataset(uri, MySampleType)\nfor batch in dataset.ordered(batch_size=32):\n    process(batch)\n\n\n\nLensLoader\n\nfrom atdata.atmosphere import LensLoader\n\nloader = LensLoader(client)\n\n# Get a specific lens record\nlens = loader.get(\"at://did:plc:abc/ac.foundation.dataset.lens/xyz\")\nprint(lens[\"name\"])\nprint(lens[\"sourceSchema\"], \"-&gt;\", lens[\"targetSchema\"])\n\n# List all lenses from a repository\nfor lens in loader.list_all():\n    print(lens[\"name\"])\n\n# Find lenses by schema\nlenses = loader.find_by_schemas(\n    source_schema_uri=\"at://did:plc:abc/ac.foundation.dataset.sampleSchema/source\",\n    target_schema_uri=\"at://did:plc:abc/ac.foundation.dataset.sampleSchema/target\",\n)",
    "crumbs": [
      "Reference",
      "Atmosphere (ATProto Integration)"
    ]
  },
  {
    "objectID": "reference/atmosphere.html#at-uris",
    "href": "reference/atmosphere.html#at-uris",
    "title": "Atmosphere (ATProto Integration)",
    "section": "AT URIs",
    "text": "AT URIs\nATProto records are identified by AT URIs:\n\nfrom atdata.atmosphere import AtUri\n\n# Parse an AT URI\nuri = AtUri.parse(\"at://did:plc:abc123/ac.foundation.dataset.sampleSchema/xyz\")\n\nprint(uri.authority)   # 'did:plc:abc123'\nprint(uri.collection)  # 'ac.foundation.dataset.sampleSchema'\nprint(uri.rkey)        # 'xyz'\n\n# Format back to string\nprint(str(uri))  # 'at://did:plc:abc123/ac.foundation.dataset.sampleSchema/xyz'",
    "crumbs": [
      "Reference",
      "Atmosphere (ATProto Integration)"
    ]
  },
  {
    "objectID": "reference/atmosphere.html#supported-field-types",
    "href": "reference/atmosphere.html#supported-field-types",
    "title": "Atmosphere (ATProto Integration)",
    "section": "Supported Field Types",
    "text": "Supported Field Types\nSchemas support these field types:\n\n\n\nPython Type\nATProto Type\n\n\n\n\nstr\nprimitive/str\n\n\nint\nprimitive/int\n\n\nfloat\nprimitive/float\n\n\nbool\nprimitive/bool\n\n\nbytes\nprimitive/bytes\n\n\nNDArray\nndarray (default dtype: float32)\n\n\nNDArray[np.float64]\nndarray (dtype: float64)\n\n\nlist[str]\narray with items\n\n\nT \\| None\nOptional field",
    "crumbs": [
      "Reference",
      "Atmosphere (ATProto Integration)"
    ]
  },
  {
    "objectID": "reference/atmosphere.html#complete-example",
    "href": "reference/atmosphere.html#complete-example",
    "title": "Atmosphere (ATProto Integration)",
    "section": "Complete Example",
    "text": "Complete Example\nThis example shows the full workflow using PDSBlobStore for decentralized storage:\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport atdata\nfrom atdata.atmosphere import AtmosphereClient, AtmosphereIndex, PDSBlobStore\nimport webdataset as wds\n\n# 1. Define and create samples\n@atdata.packable\nclass FeatureSample:\n    features: NDArray\n    label: int\n    source: str\n\nsamples = [\n    FeatureSample(\n        features=np.random.randn(128).astype(np.float32),\n        label=i % 10,\n        source=\"synthetic\",\n    )\n    for i in range(1000)\n]\n\n# 2. Write to tar\nwith wds.writer.TarWriter(\"features.tar\") as sink:\n    for i, s in enumerate(samples):\n        sink.write({**s.as_wds, \"__key__\": f\"{i:06d}\"})\n\n# 3. Authenticate and set up blob storage\nclient = AtmosphereClient()\nclient.login(\"myhandle.bsky.social\", \"app-password\")\n\nstore = PDSBlobStore(client)\nindex = AtmosphereIndex(client, data_store=store)\n\n# 4. Publish schema\nschema_uri = index.publish_schema(\n    FeatureSample,\n    version=\"1.0.0\",\n    description=\"Feature vectors with labels\",\n)\n\n# 5. Publish dataset (shards uploaded as blobs)\ndataset = atdata.Dataset[FeatureSample](\"features.tar\")\nentry = index.insert_dataset(\n    dataset,\n    name=\"synthetic-features-v1\",\n    schema_ref=schema_uri,\n    tags=[\"features\", \"synthetic\"],\n)\n\nprint(f\"Published: {entry.uri}\")\nprint(f\"Blob URLs: {entry.data_urls}\")\n\n# 6. Later: discover and load from blobs\nfor dataset_entry in index.list_datasets():\n    print(f\"Found: {dataset_entry.name}\")\n\n    # Reconstruct type from schema\n    SampleType = index.decode_schema(dataset_entry.schema_ref)\n\n    # Create source from blob URLs\n    source = store.create_source(dataset_entry.data_urls)\n\n    # Load dataset from blobs\n    ds = atdata.Dataset[SampleType](source)\n    for batch in ds.ordered(batch_size=32):\n        print(batch.features.shape)\n        break\n\nFor external URL storage (without PDSBlobStore):\n\n# Use AtmosphereIndex without data_store\nindex = AtmosphereIndex(client)\n\n# Dataset URLs will be stored as-is (external references)\nentry = index.insert_dataset(\n    dataset,\n    name=\"external-features\",\n    schema_ref=schema_uri,\n)\n\n# Load using standard URL source\nds = atdata.Dataset[FeatureSample](entry.data_urls[0])",
    "crumbs": [
      "Reference",
      "Atmosphere (ATProto Integration)"
    ]
  },
  {
    "objectID": "reference/atmosphere.html#related",
    "href": "reference/atmosphere.html#related",
    "title": "Atmosphere (ATProto Integration)",
    "section": "Related",
    "text": "Related\n\nLocal Storage - Redis + S3 backend\nPromotion - Promoting local datasets to ATProto\nProtocols - AbstractIndex interface\nPackable Samples - Defining sample types",
    "crumbs": [
      "Reference",
      "Atmosphere (ATProto Integration)"
    ]
  },
  {
    "objectID": "reference/local-storage.html",
    "href": "reference/local-storage.html",
    "title": "Local Storage",
    "section": "",
    "text": "The local storage module provides a Redis + S3 backend for storing and managing datasets before publishing to the ATProto federation.",
    "crumbs": [
      "Reference",
      "Local Storage"
    ]
  },
  {
    "objectID": "reference/local-storage.html#overview",
    "href": "reference/local-storage.html#overview",
    "title": "Local Storage",
    "section": "Overview",
    "text": "Overview\nLocal storage uses:\n\nRedis for indexing and tracking dataset metadata\nS3-compatible storage for dataset tar files\n\nThis enables development and small-scale deployment before promoting to the full ATProto infrastructure.",
    "crumbs": [
      "Reference",
      "Local Storage"
    ]
  },
  {
    "objectID": "reference/local-storage.html#localindex",
    "href": "reference/local-storage.html#localindex",
    "title": "Local Storage",
    "section": "LocalIndex",
    "text": "LocalIndex\nThe index tracks datasets in Redis:\n\nfrom atdata.local import LocalIndex\n\n# Default connection (localhost:6379)\nindex = LocalIndex()\n\n# Custom Redis connection\nimport redis\nr = redis.Redis(host='custom-host', port=6379)\nindex = LocalIndex(redis=r)\n\n# With connection kwargs\nindex = LocalIndex(host='custom-host', port=6379, db=1)\n\n\nAdding Entries\n\nimport atdata\nfrom numpy.typing import NDArray\n\n@atdata.packable\nclass ImageSample:\n    image: NDArray\n    label: str\n\ndataset = atdata.Dataset[ImageSample](\"data-{000000..000009}.tar\")\n\nentry = index.add_entry(\n    dataset,\n    name=\"my-dataset\",\n    schema_ref=\"atdata://local/sampleSchema/ImageSample@1.0.0\",  # optional\n    metadata={\"description\": \"Training images\"},              # optional\n)\n\nprint(entry.cid)        # Content identifier\nprint(entry.name)       # \"my-dataset\"\nprint(entry.data_urls)  # [\"data-{000000..000009}.tar\"]\n\n\n\nListing and Retrieving\n\n# Iterate all entries\nfor entry in index.entries:\n    print(f\"{entry.name}: {entry.cid}\")\n\n# Get as list\nall_entries = index.all_entries\n\n# Get by name\nentry = index.get_entry_by_name(\"my-dataset\")\n\n# Get by CID\nentry = index.get_entry(\"bafyrei...\")",
    "crumbs": [
      "Reference",
      "Local Storage"
    ]
  },
  {
    "objectID": "reference/local-storage.html#repo-deprecated",
    "href": "reference/local-storage.html#repo-deprecated",
    "title": "Local Storage",
    "section": "Repo (Deprecated)",
    "text": "Repo (Deprecated)\n\n\n\n\n\n\nWarning\n\n\n\nRepo is deprecated. Use LocalIndex with S3DataStore instead for new code.\n\n\nThe Repo class combines S3 storage with Redis indexing:\n\nfrom atdata.local import Repo\n\n# From credentials file\nrepo = Repo(\n    s3_credentials=\"path/to/.env\",\n    hive_path=\"my-bucket/datasets\",\n)\n\n# From credentials dict\nrepo = Repo(\n    s3_credentials={\n        \"AWS_ENDPOINT\": \"http://localhost:9000\",\n        \"AWS_ACCESS_KEY_ID\": \"minioadmin\",\n        \"AWS_SECRET_ACCESS_KEY\": \"minioadmin\",\n    },\n    hive_path=\"my-bucket/datasets\",\n)\n\nPreferred approach - Use LocalIndex with S3DataStore:\n\nfrom atdata.local import LocalIndex, S3DataStore\n\nstore = S3DataStore(\n    credentials={\n        \"AWS_ENDPOINT\": \"http://localhost:9000\",\n        \"AWS_ACCESS_KEY_ID\": \"minioadmin\",\n        \"AWS_SECRET_ACCESS_KEY\": \"minioadmin\",\n    },\n    bucket=\"my-bucket\",\n)\nindex = LocalIndex(data_store=store)\n\n# Insert dataset\nentry = index.insert_dataset(dataset, name=\"my-dataset\", prefix=\"datasets/v1\")\n\n\nCredentials File Format\nThe .env file should contain:\nAWS_ENDPOINT=http://localhost:9000\nAWS_ACCESS_KEY_ID=your-access-key\nAWS_SECRET_ACCESS_KEY=your-secret-key\n\n\n\n\n\n\nNote\n\n\n\nFor AWS S3, omit AWS_ENDPOINT to use the default endpoint.\n\n\n\n\nInserting Datasets\n\nimport webdataset as wds\nimport numpy as np\n\n# Create dataset from samples\nsamples = [ImageSample(\n    image=np.random.rand(224, 224, 3).astype(np.float32),\n    label=f\"sample_{i}\"\n) for i in range(1000)]\n\nwith wds.writer.TarWriter(\"temp.tar\") as sink:\n    for i, s in enumerate(samples):\n        sink.write({**s.as_wds, \"__key__\": f\"{i:06d}\"})\n\ndataset = atdata.Dataset[ImageSample](\"temp.tar\")\n\n# Insert into repo (writes to S3 + indexes in Redis)\nentry, stored_dataset = repo.insert(\n    dataset,\n    name=\"training-images-v1\",\n    cache_local=False,  # Stream directly to S3\n)\n\nprint(entry.cid)                # Content identifier\nprint(stored_dataset.url)       # S3 URL for the stored data\nprint(stored_dataset.shard_list)  # Individual shard URLs\n\n\n\nInsert Options\n\nentry, ds = repo.insert(\n    dataset,\n    name=\"my-dataset\",\n    cache_local=True,   # Write locally first, then copy (faster for some workloads)\n    maxcount=10000,     # Samples per shard\n    maxsize=100_000_000,  # Max shard size in bytes\n)",
    "crumbs": [
      "Reference",
      "Local Storage"
    ]
  },
  {
    "objectID": "reference/local-storage.html#localdatasetentry",
    "href": "reference/local-storage.html#localdatasetentry",
    "title": "Local Storage",
    "section": "LocalDatasetEntry",
    "text": "LocalDatasetEntry\nIndex entries provide content-addressable identification:\n\nentry = index.get_entry_by_name(\"my-dataset\")\n\n# Core properties (IndexEntry protocol)\nentry.name        # Human-readable name\nentry.schema_ref  # Schema reference\nentry.data_urls   # WebDataset URLs\nentry.metadata    # Arbitrary metadata dict or None\n\n# Content addressing\nentry.cid         # ATProto-compatible CID (content identifier)\n\n# Legacy compatibility\nentry.wds_url     # First data URL\nentry.sample_kind # Same as schema_ref\n\n\n\n\n\n\n\nTip\n\n\n\nThe CID is generated from the entry’s content (schema_ref + data_urls), ensuring identical data produces identical CIDs whether stored locally or in the atmosphere.",
    "crumbs": [
      "Reference",
      "Local Storage"
    ]
  },
  {
    "objectID": "reference/local-storage.html#schema-storage",
    "href": "reference/local-storage.html#schema-storage",
    "title": "Local Storage",
    "section": "Schema Storage",
    "text": "Schema Storage\nSchemas can be stored and retrieved from the index:\n\n# Publish a schema\nschema_ref = index.publish_schema(\n    ImageSample,\n    version=\"1.0.0\",\n    description=\"Image with label annotation\",\n)\n# Returns: \"atdata://local/sampleSchema/ImageSample@1.0.0\"\n\n# Retrieve schema record\nschema = index.get_schema(schema_ref)\n# {\n#     \"name\": \"ImageSample\",\n#     \"version\": \"1.0.0\",\n#     \"fields\": [...],\n#     \"description\": \"...\",\n#     \"createdAt\": \"...\",\n# }\n\n# List all schemas\nfor schema in index.list_schemas():\n    print(f\"{schema['name']}@{schema['version']}\")\n\n# Reconstruct sample type from schema\nSampleType = index.decode_schema(schema_ref)\ndataset = atdata.Dataset[SampleType](entry.data_urls[0])",
    "crumbs": [
      "Reference",
      "Local Storage"
    ]
  },
  {
    "objectID": "reference/local-storage.html#s3datastore",
    "href": "reference/local-storage.html#s3datastore",
    "title": "Local Storage",
    "section": "S3DataStore",
    "text": "S3DataStore\nFor direct S3 operations without Redis indexing:\n\nfrom atdata.local import S3DataStore\n\nstore = S3DataStore(\n    credentials=\"path/to/.env\",\n    bucket=\"my-bucket\",\n)\n\n# Write dataset shards\nurls = store.write_shards(\n    dataset,\n    prefix=\"datasets/v1\",\n    maxcount=10000,\n)\n# Returns: [\"s3://my-bucket/datasets/v1/data--uuid--000000.tar\", ...]\n\n# Check capabilities\nstore.supports_streaming()  # True",
    "crumbs": [
      "Reference",
      "Local Storage"
    ]
  },
  {
    "objectID": "reference/local-storage.html#complete-workflow-example",
    "href": "reference/local-storage.html#complete-workflow-example",
    "title": "Local Storage",
    "section": "Complete Workflow Example",
    "text": "Complete Workflow Example\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport atdata\nfrom atdata.local import LocalIndex, S3DataStore\nimport webdataset as wds\n\n# 1. Define sample type\n@atdata.packable\nclass TrainingSample:\n    features: NDArray\n    label: int\n    source: str\n\n# 2. Create samples\nsamples = [\n    TrainingSample(\n        features=np.random.randn(128).astype(np.float32),\n        label=i % 10,\n        source=\"synthetic\",\n    )\n    for i in range(10000)\n]\n\n# 3. Write to local tar\nwith wds.writer.TarWriter(\"local-data.tar\") as sink:\n    for i, sample in enumerate(samples):\n        sink.write({**sample.as_wds, \"__key__\": f\"{i:06d}\"})\n\n# 4. Set up index with S3 data store and insert\nstore = S3DataStore(\n    credentials={\n        \"AWS_ENDPOINT\": \"http://localhost:9000\",\n        \"AWS_ACCESS_KEY_ID\": \"minioadmin\",\n        \"AWS_SECRET_ACCESS_KEY\": \"minioadmin\",\n    },\n    bucket=\"datasets-bucket\",\n)\nindex = LocalIndex(data_store=store)\n\n# Publish schema and insert dataset\nindex.publish_schema(TrainingSample, version=\"1.0.0\")\nlocal_ds = atdata.Dataset[TrainingSample](\"local-data.tar\")\nentry = index.insert_dataset(local_ds, name=\"training-v1\", prefix=\"training\")\n\n# 5. Retrieve later\nentry = index.get_entry_by_name(\"training-v1\")\ndataset = atdata.Dataset[TrainingSample](entry.data_urls[0])\n\nfor batch in dataset.ordered(batch_size=32):\n    print(batch.features.shape)  # (32, 128)",
    "crumbs": [
      "Reference",
      "Local Storage"
    ]
  },
  {
    "objectID": "reference/local-storage.html#related",
    "href": "reference/local-storage.html#related",
    "title": "Local Storage",
    "section": "Related",
    "text": "Related\n\nDatasets - Dataset iteration and batching\nProtocols - AbstractIndex and IndexEntry interfaces\nPromotion - Promoting local datasets to ATProto\nAtmosphere - ATProto federation",
    "crumbs": [
      "Reference",
      "Local Storage"
    ]
  },
  {
    "objectID": "reference/uri-spec.html",
    "href": "reference/uri-spec.html",
    "title": "URI Specification",
    "section": "",
    "text": "The atdata:// URI scheme provides a unified way to address atdata resources across local development and the ATProto federation.",
    "crumbs": [
      "Reference",
      "URI Specification"
    ]
  },
  {
    "objectID": "reference/uri-spec.html#overview",
    "href": "reference/uri-spec.html#overview",
    "title": "URI Specification",
    "section": "Overview",
    "text": "Overview\nThe atdata URI scheme:\n\nFollows RFC 3986 syntax\nProvides consistent addressing for local and atmosphere resources\nEnables seamless promotion from development to production",
    "crumbs": [
      "Reference",
      "URI Specification"
    ]
  },
  {
    "objectID": "reference/uri-spec.html#uri-format",
    "href": "reference/uri-spec.html#uri-format",
    "title": "URI Specification",
    "section": "URI Format",
    "text": "URI Format\natdata://{authority}/{resource_type}/{name}@{version}\n\nAuthority\nThe authority identifies where the resource is stored:\n\n\n\nAuthority\nDescription\nExample\n\n\n\n\nlocal\nLocal Redis/S3 storage\natdata://local/...\n\n\n{handle}\nATProto handle\natdata://alice.bsky.social/...\n\n\n{did}\nATProto DID\natdata://did:plc:abc123/...\n\n\n\n\n\nResource Types\n\n\n\nResource Type\nDescription\n\n\n\n\nsampleSchema\nPackableSample type definitions\n\n\ndataset\nDataset entries (future)\n\n\nlens\nLens transformations (future)\n\n\n\n\n\nVersion Specifiers\nVersions follow semantic versioning and are specified with @:\n\n\n\nSpecifier\nDescription\nExample\n\n\n\n\n@{major}.{minor}.{patch}\nExact version\n@1.0.0, @2.1.3\n\n\n(none)\nLatest version\nResolves to highest semver",
    "crumbs": [
      "Reference",
      "URI Specification"
    ]
  },
  {
    "objectID": "reference/uri-spec.html#examples",
    "href": "reference/uri-spec.html#examples",
    "title": "URI Specification",
    "section": "Examples",
    "text": "Examples\n\nLocal Development\n\nfrom atdata.local import Index\n\nindex = Index()\n\n# Publish a schema (returns atdata:// URI)\nref = index.publish_schema(MySample, version=\"1.0.0\")\n# =&gt; \"atdata://local/sampleSchema/MySample@1.0.0\"\n\n# Auto-increment version\nref = index.publish_schema(MySample)\n# =&gt; \"atdata://local/sampleSchema/MySample@1.0.1\"\n\n# Retrieve by URI\nschema = index.get_schema(\"atdata://local/sampleSchema/MySample@1.0.0\")\n\n\n\nAtmosphere (ATProto Federation)\n\nfrom atdata.atmosphere import Client\n\nclient = Client()\n\n# Publish returns at:// URI that maps to atdata://\nref = client.publish_schema(MySample)\n# =&gt; \"at://did:plc:abc123/ac.foundation.dataset.sampleSchema/xyz\"\n\n# Can also be addressed as:\n# =&gt; \"atdata://did:plc:abc123/sampleSchema/MySample@1.0.0\"\n# =&gt; \"atdata://alice.bsky.social/sampleSchema/MySample@1.0.0\"",
    "crumbs": [
      "Reference",
      "URI Specification"
    ]
  },
  {
    "objectID": "reference/uri-spec.html#relationship-to-at-protocol-uris",
    "href": "reference/uri-spec.html#relationship-to-at-protocol-uris",
    "title": "URI Specification",
    "section": "Relationship to AT Protocol URIs",
    "text": "Relationship to AT Protocol URIs\nThe atdata:// scheme is inspired by and maps to ATProto’s at:// scheme:\n\n\n\n\n\n\n\natdata://\nat://\n\n\n\n\natdata://{did}/sampleSchema/{name}@{version}\nat://{did}/ac.foundation.dataset.sampleSchema/{rkey}\n\n\natdata://local/...\n(local only, no at:// equivalent)\n\n\n\nWhen publishing to the atmosphere, atdata URIs are automatically resolved to their corresponding at:// URIs for federation compatibility.",
    "crumbs": [
      "Reference",
      "URI Specification"
    ]
  },
  {
    "objectID": "reference/uri-spec.html#legacy-format",
    "href": "reference/uri-spec.html#legacy-format",
    "title": "URI Specification",
    "section": "Legacy Format",
    "text": "Legacy Format\nFor backwards compatibility, the local index also accepts the legacy format:\nlocal://schemas/{module.Class}@{version}\nThis format is deprecated and will be removed in a future version. Use atdata://local/sampleSchema/{name}@{version} instead.",
    "crumbs": [
      "Reference",
      "URI Specification"
    ]
  },
  {
    "objectID": "tutorials/quickstart.html",
    "href": "tutorials/quickstart.html",
    "title": "Quick Start",
    "section": "",
    "text": "This guide walks you through the basics of atdata: defining sample types, writing datasets, and iterating over them. You’ll learn the foundational patterns that enable type-safe, efficient dataset handling—the first layer of atdata’s three-layer architecture.",
    "crumbs": [
      "Getting Started",
      "Quick Start"
    ]
  },
  {
    "objectID": "tutorials/quickstart.html#where-this-fits",
    "href": "tutorials/quickstart.html#where-this-fits",
    "title": "Quick Start",
    "section": "Where This Fits",
    "text": "Where This Fits\natdata is built around a simple progression:\nLocal Development → Team Storage → Federation\nThis tutorial covers local development—the foundation. Everything you learn here (typed samples, efficient iteration, lens transformations) carries forward as you scale to team storage and federated sharing. The key insight is that your sample types remain the same across all three layers; only the storage backend changes.",
    "crumbs": [
      "Getting Started",
      "Quick Start"
    ]
  },
  {
    "objectID": "tutorials/quickstart.html#installation",
    "href": "tutorials/quickstart.html#installation",
    "title": "Quick Start",
    "section": "Installation",
    "text": "Installation\npip install atdata\n\n# With ATProto support\npip install atdata[atmosphere]",
    "crumbs": [
      "Getting Started",
      "Quick Start"
    ]
  },
  {
    "objectID": "tutorials/quickstart.html#define-a-sample-type",
    "href": "tutorials/quickstart.html#define-a-sample-type",
    "title": "Quick Start",
    "section": "Define a Sample Type",
    "text": "Define a Sample Type\nThe core abstraction in atdata is the PackableSample—a typed, serializable data structure. Unlike raw dictionaries or ad-hoc classes, PackableSamples provide:\n\nType safety: Know your schema at write time, not training time\nAutomatic serialization: msgpack encoding with efficient NDArray handling\nRound-trip fidelity: Data survives serialization without loss\n\nUse the @packable decorator to create a typed sample:\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport atdata\n\n@atdata.packable\nclass ImageSample:\n    \"\"\"A sample containing an image with label and confidence.\"\"\"\n    image: NDArray\n    label: str\n    confidence: float\n\nThe @packable decorator:\n\nConverts your class into a dataclass\nAdds automatic msgpack serialization\nHandles NDArray conversion to/from bytes",
    "crumbs": [
      "Getting Started",
      "Quick Start"
    ]
  },
  {
    "objectID": "tutorials/quickstart.html#create-sample-instances",
    "href": "tutorials/quickstart.html#create-sample-instances",
    "title": "Quick Start",
    "section": "Create Sample Instances",
    "text": "Create Sample Instances\n\n# Create a single sample\nsample = ImageSample(\n    image=np.random.rand(224, 224, 3).astype(np.float32),\n    label=\"cat\",\n    confidence=0.95,\n)\n\n# Check serialization\npacked_bytes = sample.packed\nprint(f\"Serialized size: {len(packed_bytes):,} bytes\")\n\n# Verify round-trip\nrestored = ImageSample.from_bytes(packed_bytes)\nassert np.allclose(sample.image, restored.image)\nprint(\"Round-trip successful!\")",
    "crumbs": [
      "Getting Started",
      "Quick Start"
    ]
  },
  {
    "objectID": "tutorials/quickstart.html#write-a-dataset",
    "href": "tutorials/quickstart.html#write-a-dataset",
    "title": "Quick Start",
    "section": "Write a Dataset",
    "text": "Write a Dataset\natdata uses WebDataset’s tar format for storage. This choice is deliberate:\n\nStreaming: Process data without downloading entire datasets\nSharding: Split large datasets across multiple files for parallel I/O\nProven: Battle-tested at scale by organizations like Google, NVIDIA, and OpenAI\n\nwrite_samples() handles serialization, sharding, and key generation in one call:\n\n# Create 100 samples\nsamples = [\n    ImageSample(\n        image=np.random.rand(224, 224, 3).astype(np.float32),\n        label=f\"class_{i % 10}\",\n        confidence=np.random.rand(),\n    )\n    for i in range(100)\n]\n\n# Write to sharded tar files (returns a typed Dataset immediately)\ndataset = atdata.write_samples(samples, \"my-dataset.tar\", maxcount=50)\nprint(f\"Wrote {len(dataset.list_shards())} shards\")",
    "crumbs": [
      "Getting Started",
      "Quick Start"
    ]
  },
  {
    "objectID": "tutorials/quickstart.html#load-and-iterate",
    "href": "tutorials/quickstart.html#load-and-iterate",
    "title": "Quick Start",
    "section": "Load and Iterate",
    "text": "Load and Iterate\nThe generic Dataset[T] class connects your sample type to WebDataset’s streaming infrastructure. When you specify Dataset[ImageSample], atdata knows how to deserialize the msgpack bytes back into fully-typed objects.\nAutomatic batch aggregation is a key feature: when you iterate with batch_size, atdata returns SampleBatch objects that intelligently combine samples:\n\nNDArray fields are stacked into a single array with a batch dimension\nOther fields become lists of values\n\nThis eliminates boilerplate collation code and works automatically with any PackableSample type.\nThe Dataset returned by write_samples is immediately iterable—or you can load from existing tar files with Dataset[T](url):\n\n# Iterate in order with batching\nfor batch in dataset.ordered(batch_size=16):\n    # NDArray fields are stacked\n    images = batch.image        # shape: (16, 224, 224, 3)\n\n    # Other fields become lists\n    labels = batch.label        # list of 16 strings\n    confidences = batch.confidence  # list of 16 floats\n\n    print(f\"Batch shape: {images.shape}\")\n    print(f\"Labels: {labels[:3]}...\")\n    break",
    "crumbs": [
      "Getting Started",
      "Quick Start"
    ]
  },
  {
    "objectID": "tutorials/quickstart.html#shuffled-iteration",
    "href": "tutorials/quickstart.html#shuffled-iteration",
    "title": "Quick Start",
    "section": "Shuffled Iteration",
    "text": "Shuffled Iteration\nProper shuffling is critical for training. WebDataset provides two-level shuffling:\n\nShard shuffling: Randomize the order of tar files\nSample shuffling: Randomize samples within a buffer\n\nThis approach balances randomness with streaming efficiency—you get well-shuffled data without needing random access to the entire dataset.\nFor training, use shuffled iteration:\n\nfor batch in dataset.shuffled(batch_size=32):\n    # Samples are shuffled at shard and sample level\n    images = batch.image\n    labels = batch.label\n\n    # Train your model\n    # model.train(images, labels)\n    break",
    "crumbs": [
      "Getting Started",
      "Quick Start"
    ]
  },
  {
    "objectID": "tutorials/quickstart.html#use-lenses-for-type-transformations",
    "href": "tutorials/quickstart.html#use-lenses-for-type-transformations",
    "title": "Quick Start",
    "section": "Use Lenses for Type Transformations",
    "text": "Use Lenses for Type Transformations\nLenses are bidirectional transformations between sample types. They solve a common problem: you have a dataset with a rich schema, but a particular task only needs a subset of fields—or needs derived fields computed on-the-fly.\nInstead of creating separate datasets for each use case (duplicating storage and maintenance burden), lenses let you view the same underlying data through different type schemas. This is inspired by functional programming concepts and enables:\n\nSchema reduction: Drop fields you don’t need\nSchema migration: Handle version differences between datasets\nDerived features: Compute fields on-the-fly during iteration\n\nView datasets through different schemas:\n\n# Define a simplified view type\n@atdata.packable\nclass SimplifiedSample:\n    label: str\n    confidence: float\n\n# Create a lens transformation\n@atdata.lens\ndef simplify(src: ImageSample) -&gt; SimplifiedSample:\n    return SimplifiedSample(label=src.label, confidence=src.confidence)\n\n# View dataset through lens\nsimple_ds = dataset.as_type(SimplifiedSample)\n\nfor batch in simple_ds.ordered(batch_size=8):\n    print(f\"Labels: {batch.label}\")\n    print(f\"Confidences: {batch.confidence}\")\n    break",
    "crumbs": [
      "Getting Started",
      "Quick Start"
    ]
  },
  {
    "objectID": "tutorials/quickstart.html#what-youve-learned",
    "href": "tutorials/quickstart.html#what-youve-learned",
    "title": "Quick Start",
    "section": "What You’ve Learned",
    "text": "What You’ve Learned\nYou now understand atdata’s foundational concepts:\n\n\n\nConcept\nPurpose\n\n\n\n\n@packable\nCreate typed, serializable sample classes\n\n\nwrite_samples()\nWrite and shard samples in one call\n\n\nDataset[T]\nTyped iteration over WebDataset tar files\n\n\nSampleBatch[T]\nAutomatic aggregation with NDArray stacking\n\n\n@lens\nTransform between sample types without data duplication\n\n\n\nThese patterns work identically whether your data lives on local disk, in team S3 storage, or published to the ATProto network. The next tutorials show how to scale beyond local files.",
    "crumbs": [
      "Getting Started",
      "Quick Start"
    ]
  },
  {
    "objectID": "tutorials/quickstart.html#next-steps",
    "href": "tutorials/quickstart.html#next-steps",
    "title": "Quick Start",
    "section": "Next Steps",
    "text": "Next Steps\n\n\n\n\n\n\nReady to Share with Your Team?\n\n\n\nThe Local Workflow tutorial shows how to set up Index-managed storage for team-wide dataset discovery and sharing.\n\n\n\nLocal Workflow - Store datasets with Redis + S3\nAtmosphere Publishing - Publish to ATProto federation\nPackable Samples - Deep dive into sample types\nDatasets - Advanced dataset operations",
    "crumbs": [
      "Getting Started",
      "Quick Start"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html",
    "href": "tutorials/atmosphere.html",
    "title": "Atmosphere Publishing",
    "section": "",
    "text": "This tutorial demonstrates how to use the atmosphere module to publish datasets to the AT Protocol network, enabling federated discovery and sharing. This is Layer 3 of atdata’s architecture—decentralized federation that enables cross-organization dataset sharing.",
    "crumbs": [
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#why-federation",
    "href": "tutorials/atmosphere.html#why-federation",
    "title": "Atmosphere Publishing",
    "section": "Why Federation?",
    "text": "Why Federation?\nManaged storage (Index with SQLite/Redis + local disk/S3) works well within an organization, but sharing across organizations introduces new challenges:\n\nDiscovery: How do researchers find relevant datasets across institutions?\nTrust: How do you verify a dataset is what it claims to be?\nDurability: What happens if the original publisher goes offline?\n\nThe AT Protocol (ATProto), developed by Bluesky, provides a foundation for decentralized social applications. atdata leverages ATProto’s infrastructure for dataset federation:\n\n\n\n\n\n\n\nATProto Feature\natdata Usage\n\n\n\n\nDIDs (Decentralized Identifiers)\nPublisher identity verification\n\n\nLexicons\nDataset/schema record schemas\n\n\nPDSes (Personal Data Servers)\nStorage for records and blobs\n\n\nRelays & AppViews\nDiscovery and aggregation\n\n\n\nThe key insight: your Bluesky identity (@handle.bsky.social) becomes your dataset publisher identity. Anyone can verify that a dataset was published by you, and can discover your datasets through the federated network.",
    "crumbs": [
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#prerequisites",
    "href": "tutorials/atmosphere.html#prerequisites",
    "title": "Atmosphere Publishing",
    "section": "Prerequisites",
    "text": "Prerequisites\n\npip install atdata[atmosphere]\nA Bluesky account with an app-specific password\n\n\n\n\n\n\n\nWarning\n\n\n\nAlways use an app-specific password, not your main Bluesky password.",
    "crumbs": [
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#setup",
    "href": "tutorials/atmosphere.html#setup",
    "title": "Atmosphere Publishing",
    "section": "Setup",
    "text": "Setup\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport atdata\nfrom atdata.atmosphere import (\n    Atmosphere,\n    PDSBlobStore,\n    SchemaPublisher,\n    SchemaLoader,\n    DatasetPublisher,\n    DatasetLoader,\n    AtUri,\n)\nfrom atdata import BlobSource",
    "crumbs": [
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#define-sample-types",
    "href": "tutorials/atmosphere.html#define-sample-types",
    "title": "Atmosphere Publishing",
    "section": "Define Sample Types",
    "text": "Define Sample Types\n\n@atdata.packable\nclass ImageSample:\n    \"\"\"A sample containing image data with metadata.\"\"\"\n    image: NDArray\n    label: str\n    confidence: float\n\n@atdata.packable\nclass TextEmbeddingSample:\n    \"\"\"A sample containing text with embedding vectors.\"\"\"\n    text: str\n    embedding: NDArray\n    source: str",
    "crumbs": [
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#type-introspection",
    "href": "tutorials/atmosphere.html#type-introspection",
    "title": "Atmosphere Publishing",
    "section": "Type Introspection",
    "text": "Type Introspection\nSee what information is available from a PackableSample type:\n\nfrom dataclasses import fields, is_dataclass\n\nprint(f\"Sample type: {ImageSample.__name__}\")\nprint(f\"Is dataclass: {is_dataclass(ImageSample)}\")\n\nprint(\"\\nFields:\")\nfor field in fields(ImageSample):\n    print(f\"  - {field.name}: {field.type}\")\n\n# Create and serialize a sample\nsample = ImageSample(\n    image=np.random.rand(224, 224, 3).astype(np.float32),\n    label=\"cat\",\n    confidence=0.95,\n)\n\npacked = sample.packed\nprint(f\"\\nSerialized size: {len(packed):,} bytes\")\n\n# Round-trip\nrestored = ImageSample.from_bytes(packed)\nprint(f\"Round-trip successful: {np.allclose(sample.image, restored.image)}\")",
    "crumbs": [
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#at-uri-parsing",
    "href": "tutorials/atmosphere.html#at-uri-parsing",
    "title": "Atmosphere Publishing",
    "section": "AT URI Parsing",
    "text": "AT URI Parsing\nEvery record in ATProto is identified by an AT URI, which encodes:\n\nAuthority: The DID or handle of the record owner\nCollection: The Lexicon type (like a table name)\nRkey: The record key (unique within the collection)\n\nUnderstanding AT URIs is essential for working with atmosphere datasets, as they’re how you reference schemas, datasets, and lenses.\nATProto records are identified by AT URIs:\n\nuris = [\n    \"at://did:plc:abc123/ac.foundation.dataset.sampleSchema/xyz789\",\n    \"at://alice.bsky.social/ac.foundation.dataset.record/my-dataset\",\n]\n\nfor uri_str in uris:\n    print(f\"\\nParsing: {uri_str}\")\n    uri = AtUri.parse(uri_str)\n    print(f\"  Authority:  {uri.authority}\")\n    print(f\"  Collection: {uri.collection}\")\n    print(f\"  Rkey:       {uri.rkey}\")",
    "crumbs": [
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#authentication",
    "href": "tutorials/atmosphere.html#authentication",
    "title": "Atmosphere Publishing",
    "section": "Authentication",
    "text": "Authentication\nThe Atmosphere class handles ATProto authentication. When you authenticate, you’re proving ownership of your decentralized identity (DID), which gives you permission to create and modify records in your Personal Data Server (PDS).\nConnect to ATProto:\n\n# Factory classmethod — returns an authenticated client\nclient = Atmosphere.login(\"your.handle.social\", \"your-app-password\")\n\n# Or from environment variables (ATDATA_HANDLE, ATDATA_PASSWORD)\nclient = Atmosphere.from_env()\n\nprint(f\"Authenticated as: {client.handle}\")\nprint(f\"DID: {client.did}\")",
    "crumbs": [
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#publish-a-schema",
    "href": "tutorials/atmosphere.html#publish-a-schema",
    "title": "Atmosphere Publishing",
    "section": "Publish a Schema",
    "text": "Publish a Schema\nWhen you publish a schema to ATProto, it becomes a public, immutable record that others can reference. The schema CID ensures that anyone can verify they’re using exactly the same type definition you published.\n\nschema_publisher = SchemaPublisher(client)\nschema_uri = schema_publisher.publish(\n    ImageSample,\n    name=\"ImageSample\",\n    version=\"1.0.0\",\n    description=\"Demo: Image sample with label and confidence\",\n)\nprint(f\"Schema URI: {schema_uri}\")",
    "crumbs": [
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#list-your-schemas",
    "href": "tutorials/atmosphere.html#list-your-schemas",
    "title": "Atmosphere Publishing",
    "section": "List Your Schemas",
    "text": "List Your Schemas\n\nschema_loader = SchemaLoader(client)\nschemas = schema_loader.list_all(limit=10)\nprint(f\"Found {len(schemas)} schema(s)\")\n\nfor schema in schemas:\n    print(f\"  - {schema.get('name', 'Unknown')}: v{schema.get('version', '?')}\")",
    "crumbs": [
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#publish-a-dataset",
    "href": "tutorials/atmosphere.html#publish-a-dataset",
    "title": "Atmosphere Publishing",
    "section": "Publish a Dataset",
    "text": "Publish a Dataset\n\nWith External URLs\n\ndataset_publisher = DatasetPublisher(client)\ndataset_uri = dataset_publisher.publish_with_urls(\n    urls=[\"s3://example-bucket/demo-data-{000000..000009}.tar\"],\n    schema_uri=str(schema_uri),\n    name=\"Demo Image Dataset\",\n    description=\"Example dataset demonstrating atmosphere publishing\",\n    tags=[\"demo\", \"images\", \"atdata\"],\n    license=\"MIT\",\n)\nprint(f\"Dataset URI: {dataset_uri}\")\n\n\n\nWith PDS Blob Storage (Recommended)\nThe PDSBlobStore is the fully decentralized option: your dataset shards are stored as ATProto blobs directly in your PDS, alongside your other ATProto records. This means:\n\nNo external dependencies: Data lives in the same infrastructure as your identity\nContent-addressed: Blobs are identified by their CID, ensuring integrity\nFederated replication: Relays can mirror your blobs for availability\n\nFor fully decentralized storage, use PDSBlobStore with the unified Index:\n\n# Create index with blob storage and atmosphere backend\nstore = PDSBlobStore(client)\nindex = atdata.Index(atmosphere=client, data_store=store)\n\n# Define sample type\n@atdata.packable\nclass FeatureSample:\n    features: NDArray\n    label: int\n\n# Create and write samples\nsamples = [FeatureSample(features=np.random.randn(64).astype(np.float32), label=i % 10) for i in range(100)]\n\nds = atdata.write_samples(samples, \"temp.tar\")\n\n# Publish - shards are uploaded as blobs automatically\nschema_uri = index.publish_schema(FeatureSample, version=\"1.0.0\")\nentry = index.insert_dataset(\n    ds,\n    name=\"blob-stored-features\",\n    schema_ref=schema_uri,\n    description=\"Features stored as PDS blobs\",\n)\n\nprint(f\"Dataset URI: {entry.uri}\")\nprint(f\"Blob URLs: {entry.data_urls}\")  # at://did/blob/cid format\n\n\n\n\n\n\n\nReading Blob-Stored Datasets\n\n\n\nUse BlobSource to stream directly from PDS blobs:\n\n# Create source from the blob URLs\nsource = store.create_source(entry.data_urls)\n\n# Or manually from blob references\nsource = BlobSource.from_refs([\n    {\"did\": client.did, \"cid\": \"bafyrei...\"},\n])\n\n# Load and iterate\nds = atdata.Dataset[FeatureSample](source)\nfor batch in ds.ordered(batch_size=32):\n    print(batch.features.shape)\n\n\n\n\n\nWith External URLs\nFor larger datasets that exceed PDS blob limits, or when you already have data in object storage, you can publish a dataset record that references external URLs. The ATProto record serves as the index entry while the actual data lives elsewhere.\nFor larger datasets or when using existing object storage:\n\ndataset_publisher = DatasetPublisher(client)\ndataset_uri = dataset_publisher.publish_with_urls(\n    urls=[\"s3://example-bucket/demo-data-{000000..000009}.tar\"],\n    schema_uri=str(schema_uri),\n    name=\"Demo Image Dataset\",\n    description=\"Example dataset demonstrating atmosphere publishing\",\n    tags=[\"demo\", \"images\", \"atdata\"],\n    license=\"MIT\",\n)\nprint(f\"Dataset URI: {dataset_uri}\")",
    "crumbs": [
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#list-and-load-datasets",
    "href": "tutorials/atmosphere.html#list-and-load-datasets",
    "title": "Atmosphere Publishing",
    "section": "List and Load Datasets",
    "text": "List and Load Datasets\n\ndataset_loader = DatasetLoader(client)\ndatasets = dataset_loader.list_all(limit=10)\nprint(f\"Found {len(datasets)} dataset(s)\")\n\nfor ds in datasets:\n    print(f\"  - {ds.get('name', 'Unknown')}\")\n    print(f\"    Schema: {ds.get('schemaRef', 'N/A')}\")\n    tags = ds.get('tags', [])\n    if tags:\n        print(f\"    Tags: {', '.join(tags)}\")",
    "crumbs": [
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#load-a-dataset",
    "href": "tutorials/atmosphere.html#load-a-dataset",
    "title": "Atmosphere Publishing",
    "section": "Load a Dataset",
    "text": "Load a Dataset\n\n# Check storage type\nstorage_type = dataset_loader.get_storage_type(str(blob_dataset_uri))\nprint(f\"Storage type: {storage_type}\")\n\nif storage_type == \"blobs\":\n    blob_urls = dataset_loader.get_blob_urls(str(blob_dataset_uri))\n    print(f\"Blob URLs: {len(blob_urls)} blob(s)\")\n\n# Load and iterate (works for both storage types)\nds = dataset_loader.to_dataset(str(blob_dataset_uri), DemoSample)\nfor batch in ds.ordered():\n    print(f\"Sample id={batch.id}, text={batch.text}\")",
    "crumbs": [
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#complete-publishing-workflow",
    "href": "tutorials/atmosphere.html#complete-publishing-workflow",
    "title": "Atmosphere Publishing",
    "section": "Complete Publishing Workflow",
    "text": "Complete Publishing Workflow\nHere’s the end-to-end workflow for publishing a dataset to the atmosphere:\n\nDefine your sample type using @packable\nWrite samples with write_samples()\nAuthenticate with your ATProto identity\nCreate index with blob storage (Index + PDSBlobStore)\nPublish schema and insert dataset\n\nNotice how similar this is to the local workflow—the same sample types and patterns, just with a different storage backend.\n\n# 1. Define and create samples\n@atdata.packable\nclass FeatureSample:\n    features: NDArray\n    label: int\n    source: str\n\nsamples = [\n    FeatureSample(\n        features=np.random.randn(128).astype(np.float32),\n        label=i % 10,\n        source=\"synthetic\",\n    )\n    for i in range(1000)\n]\n\n# 2. Write to tar\nds = atdata.write_samples(samples, \"features.tar\", maxcount=500)\n\n# 3. Authenticate and create index with blob storage\nclient = Atmosphere.login(\"myhandle.bsky.social\", \"app-password\")\n\nstore = PDSBlobStore(client)\nindex = atdata.Index(atmosphere=client, data_store=store)\n\n# 4. Publish schema\nschema_uri = index.publish_schema(\n    FeatureSample,\n    version=\"1.0.0\",\n    description=\"Feature vectors with labels\",\n)\n\n# 5. Publish dataset (shards uploaded as blobs automatically)\nentry = index.insert_dataset(\n    ds,\n    name=\"synthetic-features-v1\",\n    schema_ref=schema_uri,\n    tags=[\"features\", \"synthetic\"],\n)\n\nprint(f\"Published: {entry.uri}\")\nprint(f\"Data stored at: {entry.data_urls}\")  # at://did/blob/cid URLs\n\n# 6. Later: load from blobs\nsource = store.create_source(entry.data_urls)\nloaded = atdata.Dataset[FeatureSample](source)\nfor batch in loaded.ordered(batch_size=32):\n    print(f\"Loaded batch with {len(batch.label)} samples\")\n    break",
    "crumbs": [
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#what-youve-learned",
    "href": "tutorials/atmosphere.html#what-youve-learned",
    "title": "Atmosphere Publishing",
    "section": "What You’ve Learned",
    "text": "What You’ve Learned\nYou now understand federated dataset publishing in atdata:\n\n\n\nConcept\nPurpose\n\n\n\n\nAtmosphere\nATProto authentication and record management\n\n\nIndex(atmosphere=client)\nUnified index with atmosphere backend\n\n\nPDSBlobStore\nPDS blob storage implementing AbstractDataStore\n\n\nBlobSource\nStream datasets from PDS blobs\n\n\nAT URIs\nUniversal identifiers for schemas and datasets\n\n\n\nThe protocol abstractions (AbstractIndex, AbstractDataStore, DataSource) ensure your code works across all three layers of atdata—local files, team storage, and federated sharing.",
    "crumbs": [
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#the-full-picture",
    "href": "tutorials/atmosphere.html#the-full-picture",
    "title": "Atmosphere Publishing",
    "section": "The Full Picture",
    "text": "The Full Picture\nYou’ve now seen atdata’s complete architecture:\nLocal Development          Managed Storage           Federation\n─────────────────          ───────────────           ──────────\ntar files                  Index + SQLite/Redis      Index + Atmosphere\nwrite_samples()            LocalDiskStore / S3       PDSBlobStore\nDataset[T]                 load_dataset(\"@local/\")   load_dataset(\"@handle/\")\nThe same @packable sample types, the same Dataset[T] iteration patterns, and the same lens transformations work at every layer. Only the storage backend changes.",
    "crumbs": [
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#next-steps",
    "href": "tutorials/atmosphere.html#next-steps",
    "title": "Atmosphere Publishing",
    "section": "Next Steps",
    "text": "Next Steps\n\n\n\n\n\n\nAlready Have Local Datasets?\n\n\n\nThe Promotion Workflow tutorial shows how to migrate existing datasets from local storage to the atmosphere without re-processing your data.\n\n\n\nPromotion Workflow - Migrate from local storage to atmosphere\nAtmosphere Reference - Complete API reference\nProtocols - Abstract interfaces",
    "crumbs": [
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "api/SchemaLoader.html",
    "href": "api/SchemaLoader.html",
    "title": "SchemaLoader",
    "section": "",
    "text": "atmosphere.SchemaLoader(client)\nLoads PackableSample schemas from ATProto.\nThis class fetches schema records from ATProto and can list available schemas from a repository.\n\n\n&gt;&gt;&gt; atmo = Atmosphere.login(\"handle\", \"password\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; loader = SchemaLoader(atmo)\n&gt;&gt;&gt; schema = loader.get(\"at://did:plc:.../ac.foundation.dataset.schema/...\")\n&gt;&gt;&gt; print(schema[\"name\"])\n'MySample'\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget\nFetch a schema record by AT URI.\n\n\nlist_all\nList schema records from a repository.\n\n\n\n\n\natmosphere.SchemaLoader.get(uri)\nFetch a schema record by AT URI.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr | AtUri\nThe AT URI of the schema record.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict\nThe schema record as a dictionary.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the record is not a schema record.\n\n\n\natproto.exceptions.AtProtocolError\nIf record not found.\n\n\n\n\n\n\n\natmosphere.SchemaLoader.list_all(repo=None, limit=100)\nList schema records from a repository.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrepo\nOptional[str]\nThe DID of the repository. Defaults to authenticated user.\nNone\n\n\nlimit\nint\nMaximum number of records to return.\n100\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[dict]\nList of schema records."
  },
  {
    "objectID": "api/SchemaLoader.html#examples",
    "href": "api/SchemaLoader.html#examples",
    "title": "SchemaLoader",
    "section": "",
    "text": "&gt;&gt;&gt; atmo = Atmosphere.login(\"handle\", \"password\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; loader = SchemaLoader(atmo)\n&gt;&gt;&gt; schema = loader.get(\"at://did:plc:.../ac.foundation.dataset.schema/...\")\n&gt;&gt;&gt; print(schema[\"name\"])\n'MySample'"
  },
  {
    "objectID": "api/SchemaLoader.html#methods",
    "href": "api/SchemaLoader.html#methods",
    "title": "SchemaLoader",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget\nFetch a schema record by AT URI.\n\n\nlist_all\nList schema records from a repository.\n\n\n\n\n\natmosphere.SchemaLoader.get(uri)\nFetch a schema record by AT URI.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr | AtUri\nThe AT URI of the schema record.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict\nThe schema record as a dictionary.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the record is not a schema record.\n\n\n\natproto.exceptions.AtProtocolError\nIf record not found.\n\n\n\n\n\n\n\natmosphere.SchemaLoader.list_all(repo=None, limit=100)\nList schema records from a repository.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrepo\nOptional[str]\nThe DID of the repository. Defaults to authenticated user.\nNone\n\n\nlimit\nint\nMaximum number of records to return.\n100\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[dict]\nList of schema records."
  },
  {
    "objectID": "api/BlobSource.html",
    "href": "api/BlobSource.html",
    "title": "BlobSource",
    "section": "",
    "text": "BlobSource(blob_refs, pds_endpoint=None, _endpoint_cache=dict())\nData source for ATProto PDS blob storage.\nStreams dataset shards stored as blobs on an ATProto Personal Data Server. Each shard is identified by a blob reference containing the DID and CID.\nThis source resolves blob references to HTTP URLs and streams the content directly, supporting efficient iteration over shards without downloading everything upfront.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nblob_refs\nlist[dict[str, str]]\nList of blob reference dicts with ‘did’ and ‘cid’ keys.\n\n\npds_endpoint\nstr | None\nOptional PDS endpoint URL. If not provided, resolved from DID.\n\n\n\n\n\n\n&gt;&gt;&gt; source = BlobSource(\n...     blob_refs=[\n...         {\"did\": \"did:plc:abc123\", \"cid\": \"bafyrei...\"},\n...         {\"did\": \"did:plc:abc123\", \"cid\": \"bafyrei...\"},\n...     ],\n... )\n&gt;&gt;&gt; for shard_id, stream in source.shards:\n...     process(stream)\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfrom_refs\nCreate BlobSource from blob reference dicts.\n\n\nlist_shards\nReturn list of AT URI-style shard identifiers.\n\n\nopen_shard\nOpen a single shard by its AT URI.\n\n\n\n\n\nBlobSource.from_refs(refs, *, pds_endpoint=None)\nCreate BlobSource from blob reference dicts.\nAccepts blob references in the format returned by upload_blob: {\"$type\": \"blob\", \"ref\": {\"$link\": \"cid\"}, ...}\nAlso accepts simplified format: {\"did\": \"...\", \"cid\": \"...\"}\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrefs\nlist[dict]\nList of blob reference dicts.\nrequired\n\n\npds_endpoint\nstr | None\nOptional PDS endpoint to use for all blobs.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n'BlobSource'\nConfigured BlobSource.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf refs is empty or format is invalid.\n\n\n\n\n\n\n\nBlobSource.list_shards()\nReturn list of AT URI-style shard identifiers.\n\n\n\nBlobSource.open_shard(shard_id)\nOpen a single shard by its AT URI.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshard_id\nstr\nAT URI of the shard (at://did/blob/cid).\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nIO[bytes]\nStreaming response body for reading the blob.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf shard_id is not in list_shards().\n\n\n\nValueError\nIf shard_id format is invalid."
  },
  {
    "objectID": "api/BlobSource.html#attributes",
    "href": "api/BlobSource.html#attributes",
    "title": "BlobSource",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nblob_refs\nlist[dict[str, str]]\nList of blob reference dicts with ‘did’ and ‘cid’ keys.\n\n\npds_endpoint\nstr | None\nOptional PDS endpoint URL. If not provided, resolved from DID."
  },
  {
    "objectID": "api/BlobSource.html#examples",
    "href": "api/BlobSource.html#examples",
    "title": "BlobSource",
    "section": "",
    "text": "&gt;&gt;&gt; source = BlobSource(\n...     blob_refs=[\n...         {\"did\": \"did:plc:abc123\", \"cid\": \"bafyrei...\"},\n...         {\"did\": \"did:plc:abc123\", \"cid\": \"bafyrei...\"},\n...     ],\n... )\n&gt;&gt;&gt; for shard_id, stream in source.shards:\n...     process(stream)"
  },
  {
    "objectID": "api/BlobSource.html#methods",
    "href": "api/BlobSource.html#methods",
    "title": "BlobSource",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfrom_refs\nCreate BlobSource from blob reference dicts.\n\n\nlist_shards\nReturn list of AT URI-style shard identifiers.\n\n\nopen_shard\nOpen a single shard by its AT URI.\n\n\n\n\n\nBlobSource.from_refs(refs, *, pds_endpoint=None)\nCreate BlobSource from blob reference dicts.\nAccepts blob references in the format returned by upload_blob: {\"$type\": \"blob\", \"ref\": {\"$link\": \"cid\"}, ...}\nAlso accepts simplified format: {\"did\": \"...\", \"cid\": \"...\"}\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrefs\nlist[dict]\nList of blob reference dicts.\nrequired\n\n\npds_endpoint\nstr | None\nOptional PDS endpoint to use for all blobs.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n'BlobSource'\nConfigured BlobSource.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf refs is empty or format is invalid.\n\n\n\n\n\n\n\nBlobSource.list_shards()\nReturn list of AT URI-style shard identifiers.\n\n\n\nBlobSource.open_shard(shard_id)\nOpen a single shard by its AT URI.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshard_id\nstr\nAT URI of the shard (at://did/blob/cid).\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nIO[bytes]\nStreaming response body for reading the blob.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf shard_id is not in list_shards().\n\n\n\nValueError\nIf shard_id format is invalid."
  },
  {
    "objectID": "api/AtmosphereClient.html",
    "href": "api/AtmosphereClient.html",
    "title": "AtmosphereClient",
    "section": "",
    "text": "AtmosphereClient\natmosphere.AtmosphereClient"
  },
  {
    "objectID": "api/load_dataset.html",
    "href": "api/load_dataset.html",
    "title": "load_dataset",
    "section": "",
    "text": "load_dataset(\n    path,\n    sample_type=None,\n    *,\n    split=None,\n    data_files=None,\n    streaming=False,\n    index=None,\n)\nLoad a dataset from local files, remote URLs, or an index.\nThis function provides a HuggingFace Datasets-style interface for loading atdata typed datasets. It handles path resolution, split detection, and returns either a single Dataset or a DatasetDict depending on the split parameter.\nWhen no sample_type is provided, returns a Dataset[DictSample] that provides dynamic dict-like access to fields. Use .as_type(MyType) to convert to a typed schema.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr\nPath to dataset. Can be: - Index lookup: “@handle/dataset-name” or “@local/dataset-name” - WebDataset brace notation: “path/to/{train,test}-{000..099}.tar” - Local directory: “./data/” (scans for .tar files) - Glob pattern: “path/to/.tar” - Remote URL: ”s3://bucket/path/data-.tar” - Single file: “path/to/data.tar”\nrequired\n\n\nsample_type\nType[ST] | None\nThe PackableSample subclass defining the schema. If None, returns Dataset[DictSample] with dynamic field access. Can also be resolved from an index when using @handle/dataset syntax.\nNone\n\n\nsplit\nstr | None\nWhich split to load. If None, returns a DatasetDict with all detected splits. If specified (e.g., “train”, “test”), returns a single Dataset for that split.\nNone\n\n\ndata_files\nstr | list[str] | dict[str, str | list[str]] | None\nOptional explicit mapping of data files. Can be: - str: Single file pattern - list[str]: List of file patterns (assigned to “train”) - dict[str, str | list[str]]: Explicit split -&gt; files mapping\nNone\n\n\nstreaming\nbool\nIf True, explicitly marks the dataset for streaming mode. Note: atdata Datasets are already lazy/streaming via WebDataset pipelines, so this parameter primarily signals intent.\nFalse\n\n\nindex\nOptional['AbstractIndex']\nOptional AbstractIndex for dataset lookup. Required when using @handle/dataset syntax. When provided with an indexed path, the schema can be auto-resolved from the index.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset[ST] | DatasetDict[ST]\nIf split is None: DatasetDict with all detected splits.\n\n\n\nDataset[ST] | DatasetDict[ST]\nIf split is specified: Dataset for that split.\n\n\n\nDataset[ST] | DatasetDict[ST]\nType is ST if sample_type provided, otherwise DictSample.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the specified split is not found.\n\n\n\nFileNotFoundError\nIf no data files are found at the path.\n\n\n\nKeyError\nIf dataset not found in index.\n\n\n\n\n\n\n&gt;&gt;&gt; # Load without type - get DictSample for exploration\n&gt;&gt;&gt; ds = load_dataset(\"./data/train.tar\", split=\"train\")\n&gt;&gt;&gt; for sample in ds.ordered():\n...     print(sample.keys())  # Explore fields\n...     print(sample[\"text\"]) # Dict-style access\n...     print(sample.label)   # Attribute access\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Convert to typed schema\n&gt;&gt;&gt; typed_ds = ds.as_type(TextData)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Or load with explicit type directly\n&gt;&gt;&gt; train_ds = load_dataset(\"./data/train-*.tar\", TextData, split=\"train\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load from index with auto-type resolution\n&gt;&gt;&gt; index = Index()\n&gt;&gt;&gt; ds = load_dataset(\"@local/my-dataset\", index=index, split=\"train\")"
  },
  {
    "objectID": "api/load_dataset.html#parameters",
    "href": "api/load_dataset.html#parameters",
    "title": "load_dataset",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npath\nstr\nPath to dataset. Can be: - Index lookup: “@handle/dataset-name” or “@local/dataset-name” - WebDataset brace notation: “path/to/{train,test}-{000..099}.tar” - Local directory: “./data/” (scans for .tar files) - Glob pattern: “path/to/.tar” - Remote URL: ”s3://bucket/path/data-.tar” - Single file: “path/to/data.tar”\nrequired\n\n\nsample_type\nType[ST] | None\nThe PackableSample subclass defining the schema. If None, returns Dataset[DictSample] with dynamic field access. Can also be resolved from an index when using @handle/dataset syntax.\nNone\n\n\nsplit\nstr | None\nWhich split to load. If None, returns a DatasetDict with all detected splits. If specified (e.g., “train”, “test”), returns a single Dataset for that split.\nNone\n\n\ndata_files\nstr | list[str] | dict[str, str | list[str]] | None\nOptional explicit mapping of data files. Can be: - str: Single file pattern - list[str]: List of file patterns (assigned to “train”) - dict[str, str | list[str]]: Explicit split -&gt; files mapping\nNone\n\n\nstreaming\nbool\nIf True, explicitly marks the dataset for streaming mode. Note: atdata Datasets are already lazy/streaming via WebDataset pipelines, so this parameter primarily signals intent.\nFalse\n\n\nindex\nOptional['AbstractIndex']\nOptional AbstractIndex for dataset lookup. Required when using @handle/dataset syntax. When provided with an indexed path, the schema can be auto-resolved from the index.\nNone"
  },
  {
    "objectID": "api/load_dataset.html#returns",
    "href": "api/load_dataset.html#returns",
    "title": "load_dataset",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nDataset[ST] | DatasetDict[ST]\nIf split is None: DatasetDict with all detected splits.\n\n\n\nDataset[ST] | DatasetDict[ST]\nIf split is specified: Dataset for that split.\n\n\n\nDataset[ST] | DatasetDict[ST]\nType is ST if sample_type provided, otherwise DictSample."
  },
  {
    "objectID": "api/load_dataset.html#raises",
    "href": "api/load_dataset.html#raises",
    "title": "load_dataset",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf the specified split is not found.\n\n\n\nFileNotFoundError\nIf no data files are found at the path.\n\n\n\nKeyError\nIf dataset not found in index."
  },
  {
    "objectID": "api/load_dataset.html#examples",
    "href": "api/load_dataset.html#examples",
    "title": "load_dataset",
    "section": "",
    "text": "&gt;&gt;&gt; # Load without type - get DictSample for exploration\n&gt;&gt;&gt; ds = load_dataset(\"./data/train.tar\", split=\"train\")\n&gt;&gt;&gt; for sample in ds.ordered():\n...     print(sample.keys())  # Explore fields\n...     print(sample[\"text\"]) # Dict-style access\n...     print(sample.label)   # Attribute access\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Convert to typed schema\n&gt;&gt;&gt; typed_ds = ds.as_type(TextData)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Or load with explicit type directly\n&gt;&gt;&gt; train_ds = load_dataset(\"./data/train-*.tar\", TextData, split=\"train\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load from index with auto-type resolution\n&gt;&gt;&gt; index = Index()\n&gt;&gt;&gt; ds = load_dataset(\"@local/my-dataset\", index=index, split=\"train\")"
  },
  {
    "objectID": "api/promote_to_atmosphere.html",
    "href": "api/promote_to_atmosphere.html",
    "title": "promote_to_atmosphere",
    "section": "",
    "text": "promote.promote_to_atmosphere(\n    local_entry,\n    local_index,\n    atmosphere_client,\n    *,\n    data_store=None,\n    name=None,\n    description=None,\n    tags=None,\n    license=None,\n)\nPromote a local dataset to the atmosphere network.\nThis function takes a locally-indexed dataset and publishes it to ATProto, making it discoverable on the federated atmosphere network.\n.. deprecated:: Prefer Index.promote_entry() or Index.promote_dataset() which provide the same functionality through the unified Index interface without requiring separate client and index arguments.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlocal_entry\nLocalDatasetEntry\nThe LocalDatasetEntry to promote.\nrequired\n\n\nlocal_index\nIndex\nLocal index containing the schema for this entry.\nrequired\n\n\natmosphere_client\nAtmosphere\nAuthenticated Atmosphere.\nrequired\n\n\ndata_store\nAbstractDataStore | None\nOptional data store for copying data to new location. If None, the existing data_urls are used as-is.\nNone\n\n\nname\nstr | None\nOverride name for the atmosphere record. Defaults to local name.\nNone\n\n\ndescription\nstr | None\nOptional description for the dataset.\nNone\n\n\ntags\nlist[str] | None\nOptional tags for discovery.\nNone\n\n\nlicense\nstr | None\nOptional license identifier.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nAT URI of the created atmosphere dataset record.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf schema not found in local index.\n\n\n\nValueError\nIf local entry has no data URLs.\n\n\n\n\n\n\n&gt;&gt;&gt; entry = local_index.get_dataset(\"mnist-train\")\n&gt;&gt;&gt; uri = promote_to_atmosphere(entry, local_index, atmo)\n&gt;&gt;&gt; print(uri)\nat://did:plc:abc123/ac.foundation.dataset.datasetIndex/..."
  },
  {
    "objectID": "api/promote_to_atmosphere.html#parameters",
    "href": "api/promote_to_atmosphere.html#parameters",
    "title": "promote_to_atmosphere",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nlocal_entry\nLocalDatasetEntry\nThe LocalDatasetEntry to promote.\nrequired\n\n\nlocal_index\nIndex\nLocal index containing the schema for this entry.\nrequired\n\n\natmosphere_client\nAtmosphere\nAuthenticated Atmosphere.\nrequired\n\n\ndata_store\nAbstractDataStore | None\nOptional data store for copying data to new location. If None, the existing data_urls are used as-is.\nNone\n\n\nname\nstr | None\nOverride name for the atmosphere record. Defaults to local name.\nNone\n\n\ndescription\nstr | None\nOptional description for the dataset.\nNone\n\n\ntags\nlist[str] | None\nOptional tags for discovery.\nNone\n\n\nlicense\nstr | None\nOptional license identifier.\nNone"
  },
  {
    "objectID": "api/promote_to_atmosphere.html#returns",
    "href": "api/promote_to_atmosphere.html#returns",
    "title": "promote_to_atmosphere",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nstr\nAT URI of the created atmosphere dataset record."
  },
  {
    "objectID": "api/promote_to_atmosphere.html#raises",
    "href": "api/promote_to_atmosphere.html#raises",
    "title": "promote_to_atmosphere",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nKeyError\nIf schema not found in local index.\n\n\n\nValueError\nIf local entry has no data URLs."
  },
  {
    "objectID": "api/promote_to_atmosphere.html#examples",
    "href": "api/promote_to_atmosphere.html#examples",
    "title": "promote_to_atmosphere",
    "section": "",
    "text": "&gt;&gt;&gt; entry = local_index.get_dataset(\"mnist-train\")\n&gt;&gt;&gt; uri = promote_to_atmosphere(entry, local_index, atmo)\n&gt;&gt;&gt; print(uri)\nat://did:plc:abc123/ac.foundation.dataset.datasetIndex/..."
  },
  {
    "objectID": "api/SchemaPublisher.html",
    "href": "api/SchemaPublisher.html",
    "title": "SchemaPublisher",
    "section": "",
    "text": "atmosphere.SchemaPublisher(client)\nPublishes PackableSample schemas to ATProto.\nThis class introspects a PackableSample class to extract its field definitions and publishes them as an ATProto schema record.\n\n\n&gt;&gt;&gt; @atdata.packable\n... class MySample:\n...     image: NDArray\n...     label: str\n...\n&gt;&gt;&gt; atmo = Atmosphere.login(\"handle\", \"password\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; publisher = SchemaPublisher(atmo)\n&gt;&gt;&gt; uri = publisher.publish(MySample, version=\"1.0.0\")\n&gt;&gt;&gt; print(uri)\nat://did:plc:.../ac.foundation.dataset.schema/...\n\n\n\n\n\n\nName\nDescription\n\n\n\n\npublish\nPublish a PackableSample schema to ATProto.\n\n\n\n\n\natmosphere.SchemaPublisher.publish(\n    sample_type,\n    *,\n    name=None,\n    version='1.0.0',\n    description=None,\n    metadata=None,\n    rkey=None,\n)\nPublish a PackableSample schema to ATProto.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsample_type\nType[ST]\nThe PackableSample class to publish.\nrequired\n\n\nname\nOptional[str]\nHuman-readable name. Defaults to the class name.\nNone\n\n\nversion\nstr\nSemantic version string (e.g., ‘1.0.0’).\n'1.0.0'\n\n\ndescription\nOptional[str]\nHuman-readable description.\nNone\n\n\nmetadata\nOptional[dict]\nArbitrary metadata dictionary.\nNone\n\n\nrkey\nOptional[str]\nOptional explicit record key. If not provided, a TID is generated.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAtUri\nThe AT URI of the created schema record.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf sample_type is not a dataclass or client is not authenticated.\n\n\n\nTypeError\nIf a field type is not supported."
  },
  {
    "objectID": "api/SchemaPublisher.html#examples",
    "href": "api/SchemaPublisher.html#examples",
    "title": "SchemaPublisher",
    "section": "",
    "text": "&gt;&gt;&gt; @atdata.packable\n... class MySample:\n...     image: NDArray\n...     label: str\n...\n&gt;&gt;&gt; atmo = Atmosphere.login(\"handle\", \"password\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; publisher = SchemaPublisher(atmo)\n&gt;&gt;&gt; uri = publisher.publish(MySample, version=\"1.0.0\")\n&gt;&gt;&gt; print(uri)\nat://did:plc:.../ac.foundation.dataset.schema/..."
  },
  {
    "objectID": "api/SchemaPublisher.html#methods",
    "href": "api/SchemaPublisher.html#methods",
    "title": "SchemaPublisher",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\npublish\nPublish a PackableSample schema to ATProto.\n\n\n\n\n\natmosphere.SchemaPublisher.publish(\n    sample_type,\n    *,\n    name=None,\n    version='1.0.0',\n    description=None,\n    metadata=None,\n    rkey=None,\n)\nPublish a PackableSample schema to ATProto.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsample_type\nType[ST]\nThe PackableSample class to publish.\nrequired\n\n\nname\nOptional[str]\nHuman-readable name. Defaults to the class name.\nNone\n\n\nversion\nstr\nSemantic version string (e.g., ‘1.0.0’).\n'1.0.0'\n\n\ndescription\nOptional[str]\nHuman-readable description.\nNone\n\n\nmetadata\nOptional[dict]\nArbitrary metadata dictionary.\nNone\n\n\nrkey\nOptional[str]\nOptional explicit record key. If not provided, a TID is generated.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAtUri\nThe AT URI of the created schema record.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf sample_type is not a dataclass or client is not authenticated.\n\n\n\nTypeError\nIf a field type is not supported."
  },
  {
    "objectID": "api/DatasetPublisher.html",
    "href": "api/DatasetPublisher.html",
    "title": "DatasetPublisher",
    "section": "",
    "text": "atmosphere.DatasetPublisher(client)\nPublishes dataset index records to ATProto.\nThis class creates dataset records that reference a schema and point to external storage (WebDataset URLs) or ATProto blobs.\n\n\n&gt;&gt;&gt; dataset = atdata.Dataset[MySample](\"s3://bucket/data-{000000..000009}.tar\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; atmo = Atmosphere.login(\"handle\", \"password\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; publisher = DatasetPublisher(atmo)\n&gt;&gt;&gt; uri = publisher.publish(\n...     dataset,\n...     name=\"My Training Data\",\n...     description=\"Training data for my model\",\n...     tags=[\"computer-vision\", \"training\"],\n... )\n\n\n\n\n\n\nName\nDescription\n\n\n\n\npublish\nPublish a dataset index record to ATProto.\n\n\npublish_with_blobs\nPublish a dataset with data stored as ATProto blobs.\n\n\npublish_with_urls\nPublish a dataset record with explicit URLs.\n\n\n\n\n\natmosphere.DatasetPublisher.publish(\n    dataset,\n    *,\n    name,\n    schema_uri=None,\n    description=None,\n    tags=None,\n    license=None,\n    auto_publish_schema=True,\n    schema_version='1.0.0',\n    rkey=None,\n)\nPublish a dataset index record to ATProto.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset\nDataset[ST]\nThe Dataset to publish.\nrequired\n\n\nname\nstr\nHuman-readable dataset name.\nrequired\n\n\nschema_uri\nOptional[str]\nAT URI of the schema record. If not provided and auto_publish_schema is True, the schema will be published.\nNone\n\n\ndescription\nOptional[str]\nHuman-readable description.\nNone\n\n\ntags\nOptional[list[str]]\nSearchable tags for discovery.\nNone\n\n\nlicense\nOptional[str]\nSPDX license identifier (e.g., ‘MIT’, ‘Apache-2.0’).\nNone\n\n\nauto_publish_schema\nbool\nIf True and schema_uri not provided, automatically publish the schema first.\nTrue\n\n\nschema_version\nstr\nVersion for auto-published schema.\n'1.0.0'\n\n\nrkey\nOptional[str]\nOptional explicit record key.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAtUri\nThe AT URI of the created dataset record.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf schema_uri is not provided and auto_publish_schema is False.\n\n\n\n\n\n\n\natmosphere.DatasetPublisher.publish_with_blobs(\n    blobs,\n    schema_uri,\n    *,\n    name,\n    description=None,\n    tags=None,\n    license=None,\n    metadata=None,\n    mime_type='application/x-tar',\n    rkey=None,\n)\nPublish a dataset with data stored as ATProto blobs.\nThis method uploads the provided data as blobs to the PDS and creates a dataset record referencing them. Suitable for smaller datasets that fit within blob size limits (typically 50MB per blob, configurable).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nblobs\nlist[bytes]\nList of binary data (e.g., tar shards) to upload as blobs.\nrequired\n\n\nschema_uri\nstr\nAT URI of the schema record.\nrequired\n\n\nname\nstr\nHuman-readable dataset name.\nrequired\n\n\ndescription\nOptional[str]\nHuman-readable description.\nNone\n\n\ntags\nOptional[list[str]]\nSearchable tags for discovery.\nNone\n\n\nlicense\nOptional[str]\nSPDX license identifier.\nNone\n\n\nmetadata\nOptional[dict]\nArbitrary metadata dictionary.\nNone\n\n\nmime_type\nstr\nMIME type for the blobs (default: application/x-tar).\n'application/x-tar'\n\n\nrkey\nOptional[str]\nOptional explicit record key.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAtUri\nThe AT URI of the created dataset record.\n\n\n\n\n\n\nBlobs are only retained by the PDS when referenced in a committed record. This method handles that automatically.\n\n\n\n\natmosphere.DatasetPublisher.publish_with_urls(\n    urls,\n    schema_uri,\n    *,\n    name,\n    description=None,\n    tags=None,\n    license=None,\n    metadata=None,\n    rkey=None,\n)\nPublish a dataset record with explicit URLs.\nThis method allows publishing a dataset record without having a Dataset object, useful for registering existing WebDataset files.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nurls\nlist[str]\nList of WebDataset URLs with brace notation.\nrequired\n\n\nschema_uri\nstr\nAT URI of the schema record.\nrequired\n\n\nname\nstr\nHuman-readable dataset name.\nrequired\n\n\ndescription\nOptional[str]\nHuman-readable description.\nNone\n\n\ntags\nOptional[list[str]]\nSearchable tags for discovery.\nNone\n\n\nlicense\nOptional[str]\nSPDX license identifier.\nNone\n\n\nmetadata\nOptional[dict]\nArbitrary metadata dictionary.\nNone\n\n\nrkey\nOptional[str]\nOptional explicit record key.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAtUri\nThe AT URI of the created dataset record."
  },
  {
    "objectID": "api/DatasetPublisher.html#examples",
    "href": "api/DatasetPublisher.html#examples",
    "title": "DatasetPublisher",
    "section": "",
    "text": "&gt;&gt;&gt; dataset = atdata.Dataset[MySample](\"s3://bucket/data-{000000..000009}.tar\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; atmo = Atmosphere.login(\"handle\", \"password\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; publisher = DatasetPublisher(atmo)\n&gt;&gt;&gt; uri = publisher.publish(\n...     dataset,\n...     name=\"My Training Data\",\n...     description=\"Training data for my model\",\n...     tags=[\"computer-vision\", \"training\"],\n... )"
  },
  {
    "objectID": "api/DatasetPublisher.html#methods",
    "href": "api/DatasetPublisher.html#methods",
    "title": "DatasetPublisher",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\npublish\nPublish a dataset index record to ATProto.\n\n\npublish_with_blobs\nPublish a dataset with data stored as ATProto blobs.\n\n\npublish_with_urls\nPublish a dataset record with explicit URLs.\n\n\n\n\n\natmosphere.DatasetPublisher.publish(\n    dataset,\n    *,\n    name,\n    schema_uri=None,\n    description=None,\n    tags=None,\n    license=None,\n    auto_publish_schema=True,\n    schema_version='1.0.0',\n    rkey=None,\n)\nPublish a dataset index record to ATProto.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset\nDataset[ST]\nThe Dataset to publish.\nrequired\n\n\nname\nstr\nHuman-readable dataset name.\nrequired\n\n\nschema_uri\nOptional[str]\nAT URI of the schema record. If not provided and auto_publish_schema is True, the schema will be published.\nNone\n\n\ndescription\nOptional[str]\nHuman-readable description.\nNone\n\n\ntags\nOptional[list[str]]\nSearchable tags for discovery.\nNone\n\n\nlicense\nOptional[str]\nSPDX license identifier (e.g., ‘MIT’, ‘Apache-2.0’).\nNone\n\n\nauto_publish_schema\nbool\nIf True and schema_uri not provided, automatically publish the schema first.\nTrue\n\n\nschema_version\nstr\nVersion for auto-published schema.\n'1.0.0'\n\n\nrkey\nOptional[str]\nOptional explicit record key.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAtUri\nThe AT URI of the created dataset record.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf schema_uri is not provided and auto_publish_schema is False.\n\n\n\n\n\n\n\natmosphere.DatasetPublisher.publish_with_blobs(\n    blobs,\n    schema_uri,\n    *,\n    name,\n    description=None,\n    tags=None,\n    license=None,\n    metadata=None,\n    mime_type='application/x-tar',\n    rkey=None,\n)\nPublish a dataset with data stored as ATProto blobs.\nThis method uploads the provided data as blobs to the PDS and creates a dataset record referencing them. Suitable for smaller datasets that fit within blob size limits (typically 50MB per blob, configurable).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nblobs\nlist[bytes]\nList of binary data (e.g., tar shards) to upload as blobs.\nrequired\n\n\nschema_uri\nstr\nAT URI of the schema record.\nrequired\n\n\nname\nstr\nHuman-readable dataset name.\nrequired\n\n\ndescription\nOptional[str]\nHuman-readable description.\nNone\n\n\ntags\nOptional[list[str]]\nSearchable tags for discovery.\nNone\n\n\nlicense\nOptional[str]\nSPDX license identifier.\nNone\n\n\nmetadata\nOptional[dict]\nArbitrary metadata dictionary.\nNone\n\n\nmime_type\nstr\nMIME type for the blobs (default: application/x-tar).\n'application/x-tar'\n\n\nrkey\nOptional[str]\nOptional explicit record key.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAtUri\nThe AT URI of the created dataset record.\n\n\n\n\n\n\nBlobs are only retained by the PDS when referenced in a committed record. This method handles that automatically.\n\n\n\n\natmosphere.DatasetPublisher.publish_with_urls(\n    urls,\n    schema_uri,\n    *,\n    name,\n    description=None,\n    tags=None,\n    license=None,\n    metadata=None,\n    rkey=None,\n)\nPublish a dataset record with explicit URLs.\nThis method allows publishing a dataset record without having a Dataset object, useful for registering existing WebDataset files.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nurls\nlist[str]\nList of WebDataset URLs with brace notation.\nrequired\n\n\nschema_uri\nstr\nAT URI of the schema record.\nrequired\n\n\nname\nstr\nHuman-readable dataset name.\nrequired\n\n\ndescription\nOptional[str]\nHuman-readable description.\nNone\n\n\ntags\nOptional[list[str]]\nSearchable tags for discovery.\nNone\n\n\nlicense\nOptional[str]\nSPDX license identifier.\nNone\n\n\nmetadata\nOptional[dict]\nArbitrary metadata dictionary.\nNone\n\n\nrkey\nOptional[str]\nOptional explicit record key.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAtUri\nThe AT URI of the created dataset record."
  },
  {
    "objectID": "api/URLSource.html",
    "href": "api/URLSource.html",
    "title": "URLSource",
    "section": "",
    "text": "URLSource(url)\nData source for WebDataset-compatible URLs.\nWraps WebDataset’s gopen to open URLs using built-in handlers for http, https, pipe, gs, hf, sftp, etc. Supports brace expansion for shard patterns like “data-{000..099}.tar”.\nThis is the default source type when a string URL is passed to Dataset.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nurl\nstr\nURL or brace pattern for the shards.\n\n\n\n\n\n\n&gt;&gt;&gt; source = URLSource(\"https://example.com/train-{000..009}.tar\")\n&gt;&gt;&gt; for shard_id, stream in source.shards:\n...     print(f\"Streaming {shard_id}\")\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nlist_shards\nExpand brace pattern and return list of shard URLs.\n\n\nopen_shard\nOpen a single shard by URL.\n\n\n\n\n\nURLSource.list_shards()\nExpand brace pattern and return list of shard URLs.\n\n\n\nURLSource.open_shard(shard_id)\nOpen a single shard by URL.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshard_id\nstr\nURL of the shard to open.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nIO[bytes]\nFile-like stream from gopen.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf shard_id is not in list_shards()."
  },
  {
    "objectID": "api/URLSource.html#attributes",
    "href": "api/URLSource.html#attributes",
    "title": "URLSource",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nurl\nstr\nURL or brace pattern for the shards."
  },
  {
    "objectID": "api/URLSource.html#examples",
    "href": "api/URLSource.html#examples",
    "title": "URLSource",
    "section": "",
    "text": "&gt;&gt;&gt; source = URLSource(\"https://example.com/train-{000..009}.tar\")\n&gt;&gt;&gt; for shard_id, stream in source.shards:\n...     print(f\"Streaming {shard_id}\")"
  },
  {
    "objectID": "api/URLSource.html#methods",
    "href": "api/URLSource.html#methods",
    "title": "URLSource",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nlist_shards\nExpand brace pattern and return list of shard URLs.\n\n\nopen_shard\nOpen a single shard by URL.\n\n\n\n\n\nURLSource.list_shards()\nExpand brace pattern and return list of shard URLs.\n\n\n\nURLSource.open_shard(shard_id)\nOpen a single shard by URL.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshard_id\nstr\nURL of the shard to open.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nIO[bytes]\nFile-like stream from gopen.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf shard_id is not in list_shards()."
  },
  {
    "objectID": "api/index.html",
    "href": "api/index.html",
    "title": "API Reference",
    "section": "",
    "text": "Core types, decorators, and dataset classes\n\n\n\npackable\nConvert a class into a PackableSample dataclass with msgpack serialization.\n\n\nPackableSample\nBase class for samples that can be serialized with msgpack.\n\n\nDictSample\nDynamic sample type providing dict-like access to raw msgpack data.\n\n\nDataset\nA typed dataset built on WebDataset with lens transformations.\n\n\nSampleBatch\nA batch of samples with automatic attribute aggregation.\n\n\nLens\nA bidirectional transformation between two sample types.\n\n\nlens\nLens-based type transformations for datasets.\n\n\nload_dataset\nLoad a dataset from local files, remote URLs, or an index.\n\n\nDatasetDict\nA dictionary of split names to Dataset instances.\n\n\n\n\n\n\nAbstract protocols for storage backends\n\n\n\nPackable\nStructural protocol for packable sample types.\n\n\nIndexEntry\nCommon interface for index entries (local or atmosphere).\n\n\nAbstractIndex\nProtocol for index operations — implemented by Index and AtmosphereIndex.\n\n\nAbstractDataStore\nProtocol for data storage backends (S3, local disk, PDS blobs).\n\n\nDataSource\nProtocol for data sources that stream shard data to Dataset.\n\n\n\n\n\n\nData source implementations for streaming\n\n\n\nURLSource\nData source for WebDataset-compatible URLs.\n\n\nS3Source\nData source for S3-compatible storage with explicit credentials.\n\n\nBlobSource\nData source for ATProto PDS blob storage.\n\n\n\n\n\n\nLocal Redis/S3 storage backend\n\n\n\nlocal.Index\nUnified index for tracking datasets across multiple repositories.\n\n\nlocal.LocalDatasetEntry\nIndex entry for a dataset stored in the local repository.\n\n\nlocal.S3DataStore\nS3-compatible data store implementing AbstractDataStore protocol.\n\n\n\n\n\n\nATProto federation\n\n\n\nAtmosphereClient\n\n\n\nAtmosphereIndex\nATProto index implementing AbstractIndex protocol.\n\n\nAtmosphereIndexEntry\nEntry wrapper for ATProto dataset records implementing IndexEntry protocol.\n\n\nPDSBlobStore\nPDS blob store implementing AbstractDataStore protocol.\n\n\nSchemaPublisher\nPublishes PackableSample schemas to ATProto.\n\n\nSchemaLoader\nLoads PackableSample schemas from ATProto.\n\n\nDatasetPublisher\nPublishes dataset index records to ATProto.\n\n\nDatasetLoader\nLoads dataset records from ATProto.\n\n\nLensPublisher\nPublishes Lens transformation records to ATProto.\n\n\nLensLoader\nLoads lens records from ATProto.\n\n\nAtUri\nParsed AT Protocol URI.\n\n\n\n\n\n\nLocal to atmosphere migration\n\n\n\npromote_to_atmosphere\nPromote a local dataset to the atmosphere network."
  },
  {
    "objectID": "api/index.html#core",
    "href": "api/index.html#core",
    "title": "API Reference",
    "section": "",
    "text": "Core types, decorators, and dataset classes\n\n\n\npackable\nConvert a class into a PackableSample dataclass with msgpack serialization.\n\n\nPackableSample\nBase class for samples that can be serialized with msgpack.\n\n\nDictSample\nDynamic sample type providing dict-like access to raw msgpack data.\n\n\nDataset\nA typed dataset built on WebDataset with lens transformations.\n\n\nSampleBatch\nA batch of samples with automatic attribute aggregation.\n\n\nLens\nA bidirectional transformation between two sample types.\n\n\nlens\nLens-based type transformations for datasets.\n\n\nload_dataset\nLoad a dataset from local files, remote URLs, or an index.\n\n\nDatasetDict\nA dictionary of split names to Dataset instances."
  },
  {
    "objectID": "api/index.html#protocols",
    "href": "api/index.html#protocols",
    "title": "API Reference",
    "section": "",
    "text": "Abstract protocols for storage backends\n\n\n\nPackable\nStructural protocol for packable sample types.\n\n\nIndexEntry\nCommon interface for index entries (local or atmosphere).\n\n\nAbstractIndex\nProtocol for index operations — implemented by Index and AtmosphereIndex.\n\n\nAbstractDataStore\nProtocol for data storage backends (S3, local disk, PDS blobs).\n\n\nDataSource\nProtocol for data sources that stream shard data to Dataset."
  },
  {
    "objectID": "api/index.html#data-sources",
    "href": "api/index.html#data-sources",
    "title": "API Reference",
    "section": "",
    "text": "Data source implementations for streaming\n\n\n\nURLSource\nData source for WebDataset-compatible URLs.\n\n\nS3Source\nData source for S3-compatible storage with explicit credentials.\n\n\nBlobSource\nData source for ATProto PDS blob storage."
  },
  {
    "objectID": "api/index.html#local-storage",
    "href": "api/index.html#local-storage",
    "title": "API Reference",
    "section": "",
    "text": "Local Redis/S3 storage backend\n\n\n\nlocal.Index\nUnified index for tracking datasets across multiple repositories.\n\n\nlocal.LocalDatasetEntry\nIndex entry for a dataset stored in the local repository.\n\n\nlocal.S3DataStore\nS3-compatible data store implementing AbstractDataStore protocol."
  },
  {
    "objectID": "api/index.html#atmosphere",
    "href": "api/index.html#atmosphere",
    "title": "API Reference",
    "section": "",
    "text": "ATProto federation\n\n\n\nAtmosphereClient\n\n\n\nAtmosphereIndex\nATProto index implementing AbstractIndex protocol.\n\n\nAtmosphereIndexEntry\nEntry wrapper for ATProto dataset records implementing IndexEntry protocol.\n\n\nPDSBlobStore\nPDS blob store implementing AbstractDataStore protocol.\n\n\nSchemaPublisher\nPublishes PackableSample schemas to ATProto.\n\n\nSchemaLoader\nLoads PackableSample schemas from ATProto.\n\n\nDatasetPublisher\nPublishes dataset index records to ATProto.\n\n\nDatasetLoader\nLoads dataset records from ATProto.\n\n\nLensPublisher\nPublishes Lens transformation records to ATProto.\n\n\nLensLoader\nLoads lens records from ATProto.\n\n\nAtUri\nParsed AT Protocol URI."
  },
  {
    "objectID": "api/index.html#promotion",
    "href": "api/index.html#promotion",
    "title": "API Reference",
    "section": "",
    "text": "Local to atmosphere migration\n\n\n\npromote_to_atmosphere\nPromote a local dataset to the atmosphere network."
  },
  {
    "objectID": "api/IndexEntry.html",
    "href": "api/IndexEntry.html",
    "title": "IndexEntry",
    "section": "",
    "text": "IndexEntry()\nCommon interface for index entries (local or atmosphere).\nBoth LocalDatasetEntry and atmosphere DatasetRecord-based entries should satisfy this protocol, enabling code that works with either.\n\n\nname: Human-readable dataset name schema_ref: Reference to schema (local:// path or AT URI) data_urls: WebDataset URLs for the data metadata: Arbitrary metadata dict, or None\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ndata_urls\nWebDataset URLs for the data.\n\n\nschema_ref\nSchema reference string."
  },
  {
    "objectID": "api/IndexEntry.html#properties",
    "href": "api/IndexEntry.html#properties",
    "title": "IndexEntry",
    "section": "",
    "text": "name: Human-readable dataset name schema_ref: Reference to schema (local:// path or AT URI) data_urls: WebDataset URLs for the data metadata: Arbitrary metadata dict, or None"
  },
  {
    "objectID": "api/IndexEntry.html#attributes",
    "href": "api/IndexEntry.html#attributes",
    "title": "IndexEntry",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndata_urls\nWebDataset URLs for the data.\n\n\nschema_ref\nSchema reference string."
  },
  {
    "objectID": "api/S3Source.html",
    "href": "api/S3Source.html",
    "title": "S3Source",
    "section": "",
    "text": "S3Source(\n    bucket,\n    keys,\n    endpoint=None,\n    access_key=None,\n    secret_key=None,\n    region=None,\n    _client=None,\n)\nData source for S3-compatible storage with explicit credentials.\nUses boto3 to stream directly from S3, supporting: - Standard AWS S3 - S3-compatible endpoints (Cloudflare R2, MinIO, etc.) - Private buckets with credentials - IAM role authentication (when keys not provided)\nUnlike URL-based approaches, this doesn’t require URL transformation or global gopen_schemes registration. Credentials are scoped to the source instance.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nbucket\nstr\nS3 bucket name.\n\n\nkeys\nlist[str]\nList of object keys (paths within bucket).\n\n\nendpoint\nstr | None\nOptional custom endpoint URL for S3-compatible services.\n\n\naccess_key\nstr | None\nOptional AWS access key ID.\n\n\nsecret_key\nstr | None\nOptional AWS secret access key.\n\n\nregion\nstr | None\nOptional AWS region (defaults to us-east-1).\n\n\n\n\n\n\n&gt;&gt;&gt; source = S3Source(\n...     bucket=\"my-datasets\",\n...     keys=[\"train/shard-000.tar\", \"train/shard-001.tar\"],\n...     endpoint=\"https://abc123.r2.cloudflarestorage.com\",\n...     access_key=\"AKIAIOSFODNN7EXAMPLE\",\n...     secret_key=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n... )\n&gt;&gt;&gt; for shard_id, stream in source.shards:\n...     process(stream)\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfrom_credentials\nCreate S3Source from a credentials dictionary.\n\n\nfrom_urls\nCreate S3Source from s3:// URLs.\n\n\nlist_shards\nReturn list of S3 URIs for the shards.\n\n\nopen_shard\nOpen a single shard by S3 URI.\n\n\n\n\n\nS3Source.from_credentials(credentials, bucket, keys)\nCreate S3Source from a credentials dictionary.\nAccepts the same credential format used by S3DataStore.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncredentials\ndict[str, str]\nDict with AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and optionally AWS_ENDPOINT.\nrequired\n\n\nbucket\nstr\nS3 bucket name.\nrequired\n\n\nkeys\nlist[str]\nList of object keys.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n'S3Source'\nConfigured S3Source.\n\n\n\n\n\n\n&gt;&gt;&gt; creds = {\n...     \"AWS_ACCESS_KEY_ID\": \"...\",\n...     \"AWS_SECRET_ACCESS_KEY\": \"...\",\n...     \"AWS_ENDPOINT\": \"https://r2.example.com\",\n... }\n&gt;&gt;&gt; source = S3Source.from_credentials(creds, \"my-bucket\", [\"data.tar\"])\n\n\n\n\nS3Source.from_urls(\n    urls,\n    *,\n    endpoint=None,\n    access_key=None,\n    secret_key=None,\n    region=None,\n)\nCreate S3Source from s3:// URLs.\nParses s3://bucket/key URLs and extracts bucket and keys. All URLs must be in the same bucket.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nurls\nlist[str]\nList of s3:// URLs.\nrequired\n\n\nendpoint\nstr | None\nOptional custom endpoint.\nNone\n\n\naccess_key\nstr | None\nOptional access key.\nNone\n\n\nsecret_key\nstr | None\nOptional secret key.\nNone\n\n\nregion\nstr | None\nOptional region.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n'S3Source'\nS3Source configured for the given URLs.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf URLs are not valid s3:// URLs or span multiple buckets.\n\n\n\n\n\n\n&gt;&gt;&gt; source = S3Source.from_urls(\n...     [\"s3://my-bucket/train-000.tar\", \"s3://my-bucket/train-001.tar\"],\n...     endpoint=\"https://r2.example.com\",\n... )\n\n\n\n\nS3Source.list_shards()\nReturn list of S3 URIs for the shards.\n\n\n\nS3Source.open_shard(shard_id)\nOpen a single shard by S3 URI.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshard_id\nstr\nS3 URI of the shard (s3://bucket/key).\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nIO[bytes]\nStreamingBody for reading the object.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf shard_id is not in list_shards()."
  },
  {
    "objectID": "api/S3Source.html#attributes",
    "href": "api/S3Source.html#attributes",
    "title": "S3Source",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nbucket\nstr\nS3 bucket name.\n\n\nkeys\nlist[str]\nList of object keys (paths within bucket).\n\n\nendpoint\nstr | None\nOptional custom endpoint URL for S3-compatible services.\n\n\naccess_key\nstr | None\nOptional AWS access key ID.\n\n\nsecret_key\nstr | None\nOptional AWS secret access key.\n\n\nregion\nstr | None\nOptional AWS region (defaults to us-east-1)."
  },
  {
    "objectID": "api/S3Source.html#examples",
    "href": "api/S3Source.html#examples",
    "title": "S3Source",
    "section": "",
    "text": "&gt;&gt;&gt; source = S3Source(\n...     bucket=\"my-datasets\",\n...     keys=[\"train/shard-000.tar\", \"train/shard-001.tar\"],\n...     endpoint=\"https://abc123.r2.cloudflarestorage.com\",\n...     access_key=\"AKIAIOSFODNN7EXAMPLE\",\n...     secret_key=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n... )\n&gt;&gt;&gt; for shard_id, stream in source.shards:\n...     process(stream)"
  },
  {
    "objectID": "api/S3Source.html#methods",
    "href": "api/S3Source.html#methods",
    "title": "S3Source",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfrom_credentials\nCreate S3Source from a credentials dictionary.\n\n\nfrom_urls\nCreate S3Source from s3:// URLs.\n\n\nlist_shards\nReturn list of S3 URIs for the shards.\n\n\nopen_shard\nOpen a single shard by S3 URI.\n\n\n\n\n\nS3Source.from_credentials(credentials, bucket, keys)\nCreate S3Source from a credentials dictionary.\nAccepts the same credential format used by S3DataStore.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncredentials\ndict[str, str]\nDict with AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and optionally AWS_ENDPOINT.\nrequired\n\n\nbucket\nstr\nS3 bucket name.\nrequired\n\n\nkeys\nlist[str]\nList of object keys.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n'S3Source'\nConfigured S3Source.\n\n\n\n\n\n\n&gt;&gt;&gt; creds = {\n...     \"AWS_ACCESS_KEY_ID\": \"...\",\n...     \"AWS_SECRET_ACCESS_KEY\": \"...\",\n...     \"AWS_ENDPOINT\": \"https://r2.example.com\",\n... }\n&gt;&gt;&gt; source = S3Source.from_credentials(creds, \"my-bucket\", [\"data.tar\"])\n\n\n\n\nS3Source.from_urls(\n    urls,\n    *,\n    endpoint=None,\n    access_key=None,\n    secret_key=None,\n    region=None,\n)\nCreate S3Source from s3:// URLs.\nParses s3://bucket/key URLs and extracts bucket and keys. All URLs must be in the same bucket.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nurls\nlist[str]\nList of s3:// URLs.\nrequired\n\n\nendpoint\nstr | None\nOptional custom endpoint.\nNone\n\n\naccess_key\nstr | None\nOptional access key.\nNone\n\n\nsecret_key\nstr | None\nOptional secret key.\nNone\n\n\nregion\nstr | None\nOptional region.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n'S3Source'\nS3Source configured for the given URLs.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf URLs are not valid s3:// URLs or span multiple buckets.\n\n\n\n\n\n\n&gt;&gt;&gt; source = S3Source.from_urls(\n...     [\"s3://my-bucket/train-000.tar\", \"s3://my-bucket/train-001.tar\"],\n...     endpoint=\"https://r2.example.com\",\n... )\n\n\n\n\nS3Source.list_shards()\nReturn list of S3 URIs for the shards.\n\n\n\nS3Source.open_shard(shard_id)\nOpen a single shard by S3 URI.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshard_id\nstr\nS3 URI of the shard (s3://bucket/key).\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nIO[bytes]\nStreamingBody for reading the object.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf shard_id is not in list_shards()."
  },
  {
    "objectID": "api/local.LocalDatasetEntry.html",
    "href": "api/local.LocalDatasetEntry.html",
    "title": "local.LocalDatasetEntry",
    "section": "",
    "text": "local.LocalDatasetEntry(\n    name,\n    schema_ref,\n    data_urls,\n    metadata=None,\n    _cid=None,\n    _legacy_uuid=None,\n)\nIndex entry for a dataset stored in the local repository.\nImplements the IndexEntry protocol for compatibility with AbstractIndex. Uses dual identity: a content-addressable CID (ATProto-compatible) and a human-readable name.\nThe CID is generated from the entry’s content (schema_ref + data_urls), ensuring the same data produces the same CID whether stored locally or in the atmosphere. This enables seamless promotion from local to ATProto.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nname\nstr\nHuman-readable name for this dataset.\n\n\nschema_ref\nstr\nReference to the schema for this dataset.\n\n\ndata_urls\nlist[str]\nWebDataset URLs for the data.\n\n\nmetadata\ndict | None\nArbitrary metadata dictionary, or None if not set.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfrom_redis\nLoad an entry from Redis by CID.\n\n\nwrite_to\nPersist this index entry to Redis.\n\n\n\n\n\nlocal.LocalDatasetEntry.from_redis(redis, cid)\nLoad an entry from Redis by CID.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nredis\nRedis\nRedis connection to read from.\nrequired\n\n\ncid\nstr\nContent identifier of the entry to load.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nLocalDatasetEntry\nLocalDatasetEntry loaded from Redis.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf entry not found.\n\n\n\n\n\n\n\nlocal.LocalDatasetEntry.write_to(redis)\nPersist this index entry to Redis.\nStores the entry as a Redis hash with key ‘{REDIS_KEY_DATASET_ENTRY}:{cid}’.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nredis\nRedis\nRedis connection to write to.\nrequired"
  },
  {
    "objectID": "api/local.LocalDatasetEntry.html#attributes",
    "href": "api/local.LocalDatasetEntry.html#attributes",
    "title": "local.LocalDatasetEntry",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nname\nstr\nHuman-readable name for this dataset.\n\n\nschema_ref\nstr\nReference to the schema for this dataset.\n\n\ndata_urls\nlist[str]\nWebDataset URLs for the data.\n\n\nmetadata\ndict | None\nArbitrary metadata dictionary, or None if not set."
  },
  {
    "objectID": "api/local.LocalDatasetEntry.html#methods",
    "href": "api/local.LocalDatasetEntry.html#methods",
    "title": "local.LocalDatasetEntry",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfrom_redis\nLoad an entry from Redis by CID.\n\n\nwrite_to\nPersist this index entry to Redis.\n\n\n\n\n\nlocal.LocalDatasetEntry.from_redis(redis, cid)\nLoad an entry from Redis by CID.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nredis\nRedis\nRedis connection to read from.\nrequired\n\n\ncid\nstr\nContent identifier of the entry to load.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nLocalDatasetEntry\nLocalDatasetEntry loaded from Redis.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf entry not found.\n\n\n\n\n\n\n\nlocal.LocalDatasetEntry.write_to(redis)\nPersist this index entry to Redis.\nStores the entry as a Redis hash with key ‘{REDIS_KEY_DATASET_ENTRY}:{cid}’.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nredis\nRedis\nRedis connection to write to.\nrequired"
  },
  {
    "objectID": "api/AbstractIndex.html",
    "href": "api/AbstractIndex.html",
    "title": "AbstractIndex",
    "section": "",
    "text": "AbstractIndex()\nProtocol for index operations — implemented by Index and AtmosphereIndex.\nManages dataset metadata: publishing/retrieving schemas, inserting/listing datasets. A single index holds datasets of many sample types, tracked via schema references.\n\n\n&gt;&gt;&gt; def publish_and_list(index: AbstractIndex) -&gt; None:\n...     index.publish_schema(ImageSample, version=\"1.0.0\")\n...     index.insert_dataset(image_ds, name=\"images\")\n...     for entry in index.list_datasets():\n...         print(f\"{entry.name} -&gt; {entry.schema_ref}\")\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ndata_store\nOptional data store for reading/writing shards.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ndecode_schema\nReconstruct a Packable type from a stored schema.\n\n\nget_dataset\nGet a dataset entry by name or reference.\n\n\nget_schema\nGet a schema record by reference.\n\n\ninsert_dataset\nRegister an existing dataset in the index.\n\n\npublish_schema\nPublish a schema for a sample type.\n\n\nwrite\nWrite samples and create an index entry in one step.\n\n\n\n\n\nAbstractIndex.decode_schema(ref)\nReconstruct a Packable type from a stored schema.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf schema not found.\n\n\n\nValueError\nIf schema has unsupported field types.\n\n\n\n\n\n\n&gt;&gt;&gt; SampleType = index.decode_schema(entry.schema_ref)\n&gt;&gt;&gt; ds = Dataset[SampleType](entry.data_urls[0])\n\n\n\n\nAbstractIndex.get_dataset(ref)\nGet a dataset entry by name or reference.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf dataset not found.\n\n\n\n\n\n\n\nAbstractIndex.get_schema(ref)\nGet a schema record by reference.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf schema not found.\n\n\n\n\n\n\n\nAbstractIndex.insert_dataset(ds, *, name, schema_ref=None, **kwargs)\nRegister an existing dataset in the index.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nds\nDataset\nThe Dataset to register.\nrequired\n\n\nname\nstr\nHuman-readable name.\nrequired\n\n\nschema_ref\nOptional[str]\nExplicit schema ref; auto-published if None.\nNone\n\n\n**kwargs\n\nBackend-specific options.\n{}\n\n\n\n\n\n\n\nAbstractIndex.publish_schema(sample_type, *, version='1.0.0', **kwargs)\nPublish a schema for a sample type.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsample_type\ntype\nA Packable type (@packable-decorated or subclass).\nrequired\n\n\nversion\nstr\nSemantic version string.\n'1.0.0'\n\n\n**kwargs\n\nBackend-specific options.\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nSchema reference string (local://... or at://...).\n\n\n\n\n\n\n\nAbstractIndex.write(samples, *, name, schema_ref=None, **kwargs)\nWrite samples and create an index entry in one step.\nSerializes samples to WebDataset tar files, stores them via the appropriate backend, and creates an index entry.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsamples\nIterable\nIterable of Packable samples. Must be non-empty.\nrequired\n\n\nname\nstr\nDataset name, optionally prefixed with target backend.\nrequired\n\n\nschema_ref\nOptional[str]\nOptional schema reference.\nNone\n\n\n**kwargs\n\nBackend-specific options (maxcount, description, etc.).\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nIndexEntry\nIndexEntry for the created dataset."
  },
  {
    "objectID": "api/AbstractIndex.html#examples",
    "href": "api/AbstractIndex.html#examples",
    "title": "AbstractIndex",
    "section": "",
    "text": "&gt;&gt;&gt; def publish_and_list(index: AbstractIndex) -&gt; None:\n...     index.publish_schema(ImageSample, version=\"1.0.0\")\n...     index.insert_dataset(image_ds, name=\"images\")\n...     for entry in index.list_datasets():\n...         print(f\"{entry.name} -&gt; {entry.schema_ref}\")"
  },
  {
    "objectID": "api/AbstractIndex.html#attributes",
    "href": "api/AbstractIndex.html#attributes",
    "title": "AbstractIndex",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndata_store\nOptional data store for reading/writing shards."
  },
  {
    "objectID": "api/AbstractIndex.html#methods",
    "href": "api/AbstractIndex.html#methods",
    "title": "AbstractIndex",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndecode_schema\nReconstruct a Packable type from a stored schema.\n\n\nget_dataset\nGet a dataset entry by name or reference.\n\n\nget_schema\nGet a schema record by reference.\n\n\ninsert_dataset\nRegister an existing dataset in the index.\n\n\npublish_schema\nPublish a schema for a sample type.\n\n\nwrite\nWrite samples and create an index entry in one step.\n\n\n\n\n\nAbstractIndex.decode_schema(ref)\nReconstruct a Packable type from a stored schema.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf schema not found.\n\n\n\nValueError\nIf schema has unsupported field types.\n\n\n\n\n\n\n&gt;&gt;&gt; SampleType = index.decode_schema(entry.schema_ref)\n&gt;&gt;&gt; ds = Dataset[SampleType](entry.data_urls[0])\n\n\n\n\nAbstractIndex.get_dataset(ref)\nGet a dataset entry by name or reference.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf dataset not found.\n\n\n\n\n\n\n\nAbstractIndex.get_schema(ref)\nGet a schema record by reference.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf schema not found.\n\n\n\n\n\n\n\nAbstractIndex.insert_dataset(ds, *, name, schema_ref=None, **kwargs)\nRegister an existing dataset in the index.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nds\nDataset\nThe Dataset to register.\nrequired\n\n\nname\nstr\nHuman-readable name.\nrequired\n\n\nschema_ref\nOptional[str]\nExplicit schema ref; auto-published if None.\nNone\n\n\n**kwargs\n\nBackend-specific options.\n{}\n\n\n\n\n\n\n\nAbstractIndex.publish_schema(sample_type, *, version='1.0.0', **kwargs)\nPublish a schema for a sample type.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsample_type\ntype\nA Packable type (@packable-decorated or subclass).\nrequired\n\n\nversion\nstr\nSemantic version string.\n'1.0.0'\n\n\n**kwargs\n\nBackend-specific options.\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nSchema reference string (local://... or at://...).\n\n\n\n\n\n\n\nAbstractIndex.write(samples, *, name, schema_ref=None, **kwargs)\nWrite samples and create an index entry in one step.\nSerializes samples to WebDataset tar files, stores them via the appropriate backend, and creates an index entry.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsamples\nIterable\nIterable of Packable samples. Must be non-empty.\nrequired\n\n\nname\nstr\nDataset name, optionally prefixed with target backend.\nrequired\n\n\nschema_ref\nOptional[str]\nOptional schema reference.\nNone\n\n\n**kwargs\n\nBackend-specific options (maxcount, description, etc.).\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nIndexEntry\nIndexEntry for the created dataset."
  },
  {
    "objectID": "api/AtmosphereIndexEntry.html",
    "href": "api/AtmosphereIndexEntry.html",
    "title": "AtmosphereIndexEntry",
    "section": "",
    "text": "atmosphere.AtmosphereIndexEntry(uri, record)\nEntry wrapper for ATProto dataset records implementing IndexEntry protocol.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n_uri\n\nAT URI of the record.\n\n\n_record\n\nRaw record dictionary."
  },
  {
    "objectID": "api/AtmosphereIndexEntry.html#attributes",
    "href": "api/AtmosphereIndexEntry.html#attributes",
    "title": "AtmosphereIndexEntry",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n_uri\n\nAT URI of the record.\n\n\n_record\n\nRaw record dictionary."
  },
  {
    "objectID": "api/LensPublisher.html",
    "href": "api/LensPublisher.html",
    "title": "LensPublisher",
    "section": "",
    "text": "atmosphere.LensPublisher(client)\nPublishes Lens transformation records to ATProto.\nThis class creates lens records that reference source and target schemas and point to the transformation code in a git repository.\n\n\n&gt;&gt;&gt; @atdata.lens\n... def my_lens(source: SourceType) -&gt; TargetType:\n...     return TargetType(field=source.other_field)\n&gt;&gt;&gt;\n&gt;&gt;&gt; atmo = Atmosphere.login(\"handle\", \"password\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; publisher = LensPublisher(atmo)\n&gt;&gt;&gt; uri = publisher.publish(\n...     name=\"my_lens\",\n...     source_schema_uri=\"at://did:plc:abc/ac.foundation.dataset.schema/source\",\n...     target_schema_uri=\"at://did:plc:abc/ac.foundation.dataset.schema/target\",\n...     code_repository=\"https://github.com/user/repo\",\n...     code_commit=\"abc123def456\",\n...     getter_path=\"mymodule.lenses:my_lens\",\n...     putter_path=\"mymodule.lenses:my_lens_putter\",\n... )\n\n\n\nLens code is stored as references to git repositories rather than inline code. This prevents arbitrary code execution from ATProto records. Users must manually install and trust lens implementations.\n\n\n\n\n\n\nName\nDescription\n\n\n\n\npublish\nPublish a lens transformation record to ATProto.\n\n\npublish_from_lens\nPublish a lens record from an existing Lens object.\n\n\n\n\n\natmosphere.LensPublisher.publish(\n    name,\n    source_schema_uri,\n    target_schema_uri,\n    description=None,\n    code_repository=None,\n    code_commit=None,\n    getter_path=None,\n    putter_path=None,\n    rkey=None,\n)\nPublish a lens transformation record to ATProto.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nHuman-readable lens name.\nrequired\n\n\nsource_schema_uri\nstr\nAT URI of the source schema.\nrequired\n\n\ntarget_schema_uri\nstr\nAT URI of the target schema.\nrequired\n\n\ndescription\nOptional[str]\nWhat this transformation does.\nNone\n\n\ncode_repository\nOptional[str]\nGit repository URL containing the lens code.\nNone\n\n\ncode_commit\nOptional[str]\nGit commit hash for reproducibility.\nNone\n\n\ngetter_path\nOptional[str]\nModule path to the getter function (e.g., ‘mymodule.lenses:my_getter’).\nNone\n\n\nputter_path\nOptional[str]\nModule path to the putter function (e.g., ‘mymodule.lenses:my_putter’).\nNone\n\n\nrkey\nOptional[str]\nOptional explicit record key.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAtUri\nThe AT URI of the created lens record.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf code references are incomplete.\n\n\n\n\n\n\n\natmosphere.LensPublisher.publish_from_lens(\n    lens_obj,\n    *,\n    name,\n    source_schema_uri,\n    target_schema_uri,\n    code_repository,\n    code_commit,\n    description=None,\n    rkey=None,\n)\nPublish a lens record from an existing Lens object.\nThis method extracts the getter and putter function names from the Lens object and publishes a record referencing them.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlens_obj\nLens\nThe Lens object to publish.\nrequired\n\n\nname\nstr\nHuman-readable lens name.\nrequired\n\n\nsource_schema_uri\nstr\nAT URI of the source schema.\nrequired\n\n\ntarget_schema_uri\nstr\nAT URI of the target schema.\nrequired\n\n\ncode_repository\nstr\nGit repository URL.\nrequired\n\n\ncode_commit\nstr\nGit commit hash.\nrequired\n\n\ndescription\nOptional[str]\nWhat this transformation does.\nNone\n\n\nrkey\nOptional[str]\nOptional explicit record key.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAtUri\nThe AT URI of the created lens record."
  },
  {
    "objectID": "api/LensPublisher.html#examples",
    "href": "api/LensPublisher.html#examples",
    "title": "LensPublisher",
    "section": "",
    "text": "&gt;&gt;&gt; @atdata.lens\n... def my_lens(source: SourceType) -&gt; TargetType:\n...     return TargetType(field=source.other_field)\n&gt;&gt;&gt;\n&gt;&gt;&gt; atmo = Atmosphere.login(\"handle\", \"password\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; publisher = LensPublisher(atmo)\n&gt;&gt;&gt; uri = publisher.publish(\n...     name=\"my_lens\",\n...     source_schema_uri=\"at://did:plc:abc/ac.foundation.dataset.schema/source\",\n...     target_schema_uri=\"at://did:plc:abc/ac.foundation.dataset.schema/target\",\n...     code_repository=\"https://github.com/user/repo\",\n...     code_commit=\"abc123def456\",\n...     getter_path=\"mymodule.lenses:my_lens\",\n...     putter_path=\"mymodule.lenses:my_lens_putter\",\n... )"
  },
  {
    "objectID": "api/LensPublisher.html#security-note",
    "href": "api/LensPublisher.html#security-note",
    "title": "LensPublisher",
    "section": "",
    "text": "Lens code is stored as references to git repositories rather than inline code. This prevents arbitrary code execution from ATProto records. Users must manually install and trust lens implementations."
  },
  {
    "objectID": "api/LensPublisher.html#methods",
    "href": "api/LensPublisher.html#methods",
    "title": "LensPublisher",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\npublish\nPublish a lens transformation record to ATProto.\n\n\npublish_from_lens\nPublish a lens record from an existing Lens object.\n\n\n\n\n\natmosphere.LensPublisher.publish(\n    name,\n    source_schema_uri,\n    target_schema_uri,\n    description=None,\n    code_repository=None,\n    code_commit=None,\n    getter_path=None,\n    putter_path=None,\n    rkey=None,\n)\nPublish a lens transformation record to ATProto.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nHuman-readable lens name.\nrequired\n\n\nsource_schema_uri\nstr\nAT URI of the source schema.\nrequired\n\n\ntarget_schema_uri\nstr\nAT URI of the target schema.\nrequired\n\n\ndescription\nOptional[str]\nWhat this transformation does.\nNone\n\n\ncode_repository\nOptional[str]\nGit repository URL containing the lens code.\nNone\n\n\ncode_commit\nOptional[str]\nGit commit hash for reproducibility.\nNone\n\n\ngetter_path\nOptional[str]\nModule path to the getter function (e.g., ‘mymodule.lenses:my_getter’).\nNone\n\n\nputter_path\nOptional[str]\nModule path to the putter function (e.g., ‘mymodule.lenses:my_putter’).\nNone\n\n\nrkey\nOptional[str]\nOptional explicit record key.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAtUri\nThe AT URI of the created lens record.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf code references are incomplete.\n\n\n\n\n\n\n\natmosphere.LensPublisher.publish_from_lens(\n    lens_obj,\n    *,\n    name,\n    source_schema_uri,\n    target_schema_uri,\n    code_repository,\n    code_commit,\n    description=None,\n    rkey=None,\n)\nPublish a lens record from an existing Lens object.\nThis method extracts the getter and putter function names from the Lens object and publishes a record referencing them.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlens_obj\nLens\nThe Lens object to publish.\nrequired\n\n\nname\nstr\nHuman-readable lens name.\nrequired\n\n\nsource_schema_uri\nstr\nAT URI of the source schema.\nrequired\n\n\ntarget_schema_uri\nstr\nAT URI of the target schema.\nrequired\n\n\ncode_repository\nstr\nGit repository URL.\nrequired\n\n\ncode_commit\nstr\nGit commit hash.\nrequired\n\n\ndescription\nOptional[str]\nWhat this transformation does.\nNone\n\n\nrkey\nOptional[str]\nOptional explicit record key.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAtUri\nThe AT URI of the created lens record."
  },
  {
    "objectID": "api/SampleBatch.html",
    "href": "api/SampleBatch.html",
    "title": "SampleBatch",
    "section": "",
    "text": "SampleBatch(samples)\nA batch of samples with automatic attribute aggregation.\nAccessing an attribute aggregates that field across all samples: NDArray fields are stacked into a numpy array with a batch dimension; other fields are collected into a list. Results are cached.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nDT\n\nThe sample type, must derive from PackableSample.\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; batch = SampleBatch[MyData]([sample1, sample2, sample3])\n&gt;&gt;&gt; batch.embeddings  # Stacked numpy array of shape (3, ...)\n&gt;&gt;&gt; batch.names  # List of names\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nsample_type\nThe type parameter DT used when creating this batch."
  },
  {
    "objectID": "api/SampleBatch.html#parameters",
    "href": "api/SampleBatch.html#parameters",
    "title": "SampleBatch",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nDT\n\nThe sample type, must derive from PackableSample.\nrequired"
  },
  {
    "objectID": "api/SampleBatch.html#examples",
    "href": "api/SampleBatch.html#examples",
    "title": "SampleBatch",
    "section": "",
    "text": "&gt;&gt;&gt; batch = SampleBatch[MyData]([sample1, sample2, sample3])\n&gt;&gt;&gt; batch.embeddings  # Stacked numpy array of shape (3, ...)\n&gt;&gt;&gt; batch.names  # List of names"
  },
  {
    "objectID": "api/SampleBatch.html#attributes",
    "href": "api/SampleBatch.html#attributes",
    "title": "SampleBatch",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nsample_type\nThe type parameter DT used when creating this batch."
  },
  {
    "objectID": "examples/index-workflow.html",
    "href": "examples/index-workflow.html",
    "title": "Index-Managed Datasets",
    "section": "",
    "text": "Writing raw tar files is fine for quick experiments, but as your dataset collection grows you need discovery, versioning, and metadata. The Index class provides this layer—backed by SQLite by default—and pairs with LocalDiskStore for persistent file storage.\nThis example creates an index, writes two datasets into it, lists and retrieves them, and shows how load_dataset resolves from the index.",
    "crumbs": [
      "Examples",
      "Index-Managed Datasets"
    ]
  },
  {
    "objectID": "examples/index-workflow.html#define-sample-types",
    "href": "examples/index-workflow.html#define-sample-types",
    "title": "Index-Managed Datasets",
    "section": "1 — Define sample types",
    "text": "1 — Define sample types\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport atdata\n\n\n@atdata.packable\nclass TextSample:\n    \"\"\"Simple text document.\"\"\"\n\n    text: str\n    language: str\n    word_count: int\n\n\n@atdata.packable\nclass EmbeddingSample:\n    \"\"\"Dense embedding with label.\"\"\"\n\n    vector: NDArray\n    label: str\n    source: str",
    "crumbs": [
      "Examples",
      "Index-Managed Datasets"
    ]
  },
  {
    "objectID": "examples/index-workflow.html#create-an-index-with-local-disk-storage",
    "href": "examples/index-workflow.html#create-an-index-with-local-disk-storage",
    "title": "Index-Managed Datasets",
    "section": "2 — Create an Index with local disk storage",
    "text": "2 — Create an Index with local disk storage\nBy default Index() uses an in-memory SQLite database. We pass a custom path to persist across the example, and a LocalDiskStore rooted in a temp directory.\n\nimport tempfile\nfrom pathlib import Path\n\ntmpdir = Path(tempfile.mkdtemp(prefix=\"atdata_index_\"))\n\nindex = atdata.Index(\n    path=tmpdir / \"index.db\",\n    data_store=atdata.LocalDiskStore(root=tmpdir / \"data\"),\n)\n\nprint(f\"Index DB   : {tmpdir / 'index.db'}\")\nprint(f\"Data root  : {tmpdir / 'data'}\")\n\nIndex DB   : /var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_index_mgzuaw0j/index.db\nData root  : /var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_index_mgzuaw0j/data",
    "crumbs": [
      "Examples",
      "Index-Managed Datasets"
    ]
  },
  {
    "objectID": "examples/index-workflow.html#write-datasets-through-the-index",
    "href": "examples/index-workflow.html#write-datasets-through-the-index",
    "title": "Index-Managed Datasets",
    "section": "3 — Write datasets through the index",
    "text": "3 — Write datasets through the index\nindex.write() serializes samples to sharded tars via the data store and creates a tracked entry with a content-addressed CID.\n\nrng = np.random.default_rng(99)\n\n# --- Dataset 1: text documents ---\ntext_samples = [\n    TextSample(\n        text=f\"Document number {i} about topic {i % 5}.\",\n        language=\"en\" if i % 3 != 0 else \"es\",\n        word_count=len(f\"Document number {i} about topic {i % 5}.\".split()),\n    )\n    for i in range(500)\n]\n\ntext_entry = index.write(\n    text_samples,\n    name=\"docs-v1\",\n    description=\"500 synthetic text documents\",\n    tags=[\"text\", \"multilingual\"],\n    maxcount=250,\n)\n\nprint(f\"Text dataset CID  : {text_entry.cid[:16]}...\")\nprint(f\"Text dataset shards: {len(text_entry.data_urls)}\")\n\n# writing /var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/tmp7jfd4umc/data-000000.tar 0 0.0 GB 0\n# writing /var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/tmp7jfd4umc/data-000001.tar 250 0.0 GB 250\n# writing /private/var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_index_mgzuaw0j/data/docs-v1/data--a33cae87--000000.tar 0 0.0 GB 0\nText dataset CID  : bafyreihvtxet3k6...\nText dataset shards: 1\n\n\n\n# --- Dataset 2: embeddings ---\nembedding_samples = [\n    EmbeddingSample(\n        vector=rng.standard_normal(128).astype(np.float32),\n        label=f\"class_{i % 10}\",\n        source=\"synthetic\",\n    )\n    for i in range(300)\n]\n\nemb_entry = index.write(\n    embedding_samples,\n    name=\"embeddings-v1\",\n    description=\"300 synthetic 128-d embeddings\",\n    tags=[\"embeddings\", \"synthetic\"],\n    maxcount=150,\n)\n\nprint(f\"Embedding CID     : {emb_entry.cid[:16]}...\")\nprint(f\"Embedding shards  : {len(emb_entry.data_urls)}\")\n\n# writing /var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/tmpz3he7ve8/data-000000.tar 0 0.0 GB 0\n# writing /var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/tmpz3he7ve8/data-000001.tar 150 0.0 GB 150\n# writing /private/var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_index_mgzuaw0j/data/embeddings-v1/data--04a64610--000000.tar 0 0.0 GB 0\nEmbedding CID     : bafyreic2r24xfjd...\nEmbedding shards  : 1",
    "crumbs": [
      "Examples",
      "Index-Managed Datasets"
    ]
  },
  {
    "objectID": "examples/index-workflow.html#list-and-discover-datasets",
    "href": "examples/index-workflow.html#list-and-discover-datasets",
    "title": "Index-Managed Datasets",
    "section": "4 — List and discover datasets",
    "text": "4 — List and discover datasets\n\nprint(\"Datasets in index:\")\nfor entry in index.list_datasets():\n    print(f\"  {entry.name:20s}  shards={len(entry.data_urls)}  cid={entry.cid[:12]}...\")\n\nDatasets in index:\n  test-ly               shards=1  cid=bafyreiawvxv...\n  proto-ly              shards=1  cid=bafyreidtr6v...\n  proto-ly              shards=1  cid=bafyreihelhd...\n  proto-ly              shards=1  cid=bafyreihly5j...\n  docs-v1               shards=1  cid=bafyreicek7z...\n  embeddings-v1         shards=1  cid=bafyreidc3by...\n  docs-v1               shards=1  cid=bafyreia4iqz...\n  embeddings-v1         shards=1  cid=bafyreihsz4n...\n  docs-v1               shards=1  cid=bafyreiejgub...\n  embeddings-v1         shards=1  cid=bafyreifwtav...\n  docs-v1               shards=1  cid=bafyreiaor62...\n  embeddings-v1         shards=1  cid=bafyreidjt2i...\n  docs-v1               shards=1  cid=bafyreidphej...\n  embeddings-v1         shards=1  cid=bafyreid5c7s...\n  docs-v1               shards=1  cid=bafyreia6rt3...\n  embeddings-v1         shards=1  cid=bafyreifwj4u...\n  docs-v1               shards=1  cid=bafyreig5yro...\n  embeddings-v1         shards=1  cid=bafyreiey4ua...\n  docs-v1               shards=1  cid=bafyreihvtxe...\n  embeddings-v1         shards=1  cid=bafyreic2r24...",
    "crumbs": [
      "Examples",
      "Index-Managed Datasets"
    ]
  },
  {
    "objectID": "examples/index-workflow.html#load-a-dataset-from-the-index",
    "href": "examples/index-workflow.html#load-a-dataset-from-the-index",
    "title": "Index-Managed Datasets",
    "section": "5 — Load a dataset from the index",
    "text": "5 — Load a dataset from the index\nload_dataset can resolve dataset names through the default index. Because the index stores the schema for each dataset, you don’t need to pass the sample type explicitly—it’s reconstructed automatically.\n\natdata.set_default_index(index)\n\n# No sample_type argument needed -- schema is resolved from the index\nds = atdata.load_dataset(\"@local/docs-v1\", split=\"train\")\nfor batch in ds.ordered(batch_size=10):\n    print(f\"Texts     : {batch.text[:2]}...\")\n    print(f\"Languages : {batch.language[:5]}...\")\n    print(f\"Counts    : {batch.word_count[:5]}...\")\n    break\n\nTexts     : ['Document number 0 about topic 0.', 'Document number 1 about topic 1.']...\nLanguages : ['es', 'en', 'en', 'es', 'en']...\nCounts    : [6, 6, 6, 6, 6]...\n\n\n\n# Same for embeddings -- type is auto-resolved\nemb_ds = atdata.load_dataset(\"@local/embeddings-v1\", split=\"train\")\nfor batch in emb_ds.ordered(batch_size=8):\n    print(f\"Vector shape : {batch.vector.shape}\")\n    print(f\"Labels       : {batch.label}\")\n    break\n\nVector shape : (8, 128)\nLabels       : ['class_0', 'class_1', 'class_2', 'class_3', 'class_4', 'class_5', 'class_6', 'class_7']",
    "crumbs": [
      "Examples",
      "Index-Managed Datasets"
    ]
  },
  {
    "objectID": "examples/index-workflow.html#schema-tracking",
    "href": "examples/index-workflow.html#schema-tracking",
    "title": "Index-Managed Datasets",
    "section": "6 — Schema tracking",
    "text": "6 — Schema tracking\nThe index records the schema for each dataset, enabling type reconstruction at load time. Schemas are persisted automatically by index.write().\n\nfor schema in index.schemas:\n    print(f\"  {schema.name:20s}  fields={[f.name for f in schema.fields]}\")\n\n  TextSample            fields=['text', 'language', 'word_count']\n  EmbeddingSample       fields=['vector', 'label', 'source']",
    "crumbs": [
      "Examples",
      "Index-Managed Datasets"
    ]
  },
  {
    "objectID": "examples/index-workflow.html#inspect-the-repository",
    "href": "examples/index-workflow.html#inspect-the-repository",
    "title": "Index-Managed Datasets",
    "section": "7 — Inspect the repository",
    "text": "7 — Inspect the repository\nThe index stores all backends in a uniform repos dict. The \"local\" repository is always present.\n\nfor name, repo in index.repos.items():\n    has_store = \"yes\" if repo.data_store else \"no\"\n    print(f\"  {name:10s}  provider={type(repo.provider).__name__}  data_store={has_store}\")\n\n  local       provider=SqliteProvider  data_store=yes",
    "crumbs": [
      "Examples",
      "Index-Managed Datasets"
    ]
  },
  {
    "objectID": "examples/index-workflow.html#clean-up",
    "href": "examples/index-workflow.html#clean-up",
    "title": "Index-Managed Datasets",
    "section": "8 — Clean up",
    "text": "8 — Clean up\n\nimport shutil\n\natdata.set_default_index(None)\nshutil.rmtree(tmpdir, ignore_errors=True)",
    "crumbs": [
      "Examples",
      "Index-Managed Datasets"
    ]
  },
  {
    "objectID": "examples/index-workflow.html#key-takeaways",
    "href": "examples/index-workflow.html#key-takeaways",
    "title": "Index-Managed Datasets",
    "section": "Key takeaways",
    "text": "Key takeaways\n\n\n\n\n\n\n\nConcept\nAPI\n\n\n\n\nCreate an index\natdata.Index(path=..., data_store=...)\n\n\nWrite with tracking\nindex.write(samples, name=..., tags=...)\n\n\nList datasets\nindex.list_datasets()\n\n\nSchema discovery\nindex.list_schemas()\n\n\nInspect repositories\nindex.repos (dict including \"local\")\n\n\nSet global index\natdata.set_default_index(index)\n\n\nLoad by name (auto-typed)\natdata.load_dataset(\"@local/name\", split=...)",
    "crumbs": [
      "Examples",
      "Index-Managed Datasets"
    ]
  },
  {
    "objectID": "examples/typed-pipeline.html",
    "href": "examples/typed-pipeline.html",
    "title": "Typed Dataset Pipeline",
    "section": "",
    "text": "This example walks through the full lifecycle of a typed dataset. You will define a sample type with the @packable decorator, generate synthetic data, write it across multiple shards, and iterate over it with automatic batch aggregation.",
    "crumbs": [
      "Examples",
      "Typed Dataset Pipeline"
    ]
  },
  {
    "objectID": "examples/typed-pipeline.html#define-a-sample-type",
    "href": "examples/typed-pipeline.html#define-a-sample-type",
    "title": "Typed Dataset Pipeline",
    "section": "1 — Define a sample type",
    "text": "1 — Define a sample type\nThe @packable decorator turns a plain class into a serializable dataclass with automatic msgpack encoding and transparent NDArray conversion.\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport atdata\n\n\n@atdata.packable\nclass SensorReading:\n    \"\"\"A single sensor observation with metadata.\"\"\"\n\n    waveform: NDArray  # 1-D time-series array\n    sensor_id: str\n    temperature: float\n    anomaly: bool\n\nEvery field is strongly typed. NDArray fields are automatically compressed to bytes during serialization and restored on read—no manual conversion needed.",
    "crumbs": [
      "Examples",
      "Typed Dataset Pipeline"
    ]
  },
  {
    "objectID": "examples/typed-pipeline.html#generate-synthetic-samples",
    "href": "examples/typed-pipeline.html#generate-synthetic-samples",
    "title": "Typed Dataset Pipeline",
    "section": "2 — Generate synthetic samples",
    "text": "2 — Generate synthetic samples\n\nrng = np.random.default_rng(42)\n\nsamples = [\n    SensorReading(\n        waveform=rng.standard_normal(512).astype(np.float32),\n        sensor_id=f\"sensor_{i % 8:02d}\",\n        temperature=20.0 + rng.normal(0, 3),\n        anomaly=rng.random() &lt; 0.05,\n    )\n    for i in range(2_000)\n]\n\nprint(f\"Created {len(samples)} samples\")\nprint(f\"Waveform shape: {samples[0].waveform.shape}\")\n\nCreated 2000 samples\nWaveform shape: (512,)",
    "crumbs": [
      "Examples",
      "Typed Dataset Pipeline"
    ]
  },
  {
    "objectID": "examples/typed-pipeline.html#verify-round-trip-serialization",
    "href": "examples/typed-pipeline.html#verify-round-trip-serialization",
    "title": "Typed Dataset Pipeline",
    "section": "3 — Verify round-trip serialization",
    "text": "3 — Verify round-trip serialization\nBefore writing to disk, confirm that packing and unpacking preserves data.\n\noriginal = samples[0]\npacked = original.packed                         # -&gt; bytes (msgpack)\nrestored = SensorReading.from_bytes(packed)      # -&gt; SensorReading\n\nassert original.sensor_id == restored.sensor_id\nassert original.temperature == restored.temperature\nassert original.anomaly == restored.anomaly\nassert np.allclose(original.waveform, restored.waveform)\n\nprint(f\"Packed size: {len(packed):,} bytes\")\nprint(\"Round-trip: OK\")\n\nPacked size: 2,124 bytes\nRound-trip: OK\n\n\nNumpy scalars (like np.float64 from rng.normal()) are automatically coerced to Python natives during serialization, so temperature comes back as a plain float:\n\nprint(f\"Temperature type (original)  : {type(original.temperature).__name__}\")\nprint(f\"Temperature type (restored)  : {type(restored.temperature).__name__}\")\nassert isinstance(restored.temperature, float)\n\nTemperature type (original)  : float\nTemperature type (restored)  : float",
    "crumbs": [
      "Examples",
      "Typed Dataset Pipeline"
    ]
  },
  {
    "objectID": "examples/typed-pipeline.html#write-sharded-tar-files",
    "href": "examples/typed-pipeline.html#write-sharded-tar-files",
    "title": "Typed Dataset Pipeline",
    "section": "4 — Write sharded tar files",
    "text": "4 — Write sharded tar files\nwrite_samples serializes samples to WebDataset tar archives. Setting maxcount splits the output across multiple shards—essential for parallel I/O at scale.\n\nimport tempfile\nfrom pathlib import Path\n\ntmpdir = Path(tempfile.mkdtemp(prefix=\"atdata_example_\"))\nds = atdata.write_samples(\n    samples,\n    tmpdir / \"readings.tar\",\n    maxcount=500,       # 4 shards of 500 samples each\n    manifest=True,      # generate per-shard metadata manifests\n)\n\nprint(f\"Dataset URL : {ds.url}\")\nprint(f\"Sample type : {ds.sample_type.__name__}\")\nprint(f\"Shard count : {len(ds.list_shards())}\")\n\n# writing /var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_example_ao1wuq0r/readings-000000.tar 0 0.0 GB 0\n# writing /var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_example_ao1wuq0r/readings-000001.tar 500 0.0 GB 500\n# writing /var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_example_ao1wuq0r/readings-000002.tar 500 0.0 GB 1000\n# writing /var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_example_ao1wuq0r/readings-000003.tar 500 0.0 GB 1500\nDataset URL : /private/var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_example_ao1wuq0r/readings-00000{0,1,2,3}.tar\nSample type : SensorReading\nShard count : 4\n\n\nThe returned Dataset[SensorReading] is immediately usable—no separate loading step required.",
    "crumbs": [
      "Examples",
      "Typed Dataset Pipeline"
    ]
  },
  {
    "objectID": "examples/typed-pipeline.html#iterate-with-automatic-batching",
    "href": "examples/typed-pipeline.html#iterate-with-automatic-batching",
    "title": "Typed Dataset Pipeline",
    "section": "5 — Iterate with automatic batching",
    "text": "5 — Iterate with automatic batching\nDataset.ordered() streams samples in shard order. When you pass a batch_size, atdata wraps consecutive samples in a SampleBatch that aggregates fields automatically:\n\nNDArray fields are stacked into a single array with a leading batch dimension.\nScalar/string fields become Python lists.\n\n\nfor batch in ds.ordered(batch_size=64):\n    # NDArray field -&gt; (64, 512) float32 array\n    print(f\"waveform    : {batch.waveform.shape}, {batch.waveform.dtype}\")\n\n    # Scalar fields -&gt; plain lists\n    print(f\"sensor_id   : {batch.sensor_id[:3]}...\")\n    print(f\"temperature : {[round(t, 1) for t in batch.temperature[:3]]}...\")\n    print(f\"anomaly     : {batch.anomaly[:3]}...\")\n    break\n\nwaveform    : (64, 512), float32\nsensor_id   : ['sensor_00', 'sensor_01', 'sensor_02']...\ntemperature : [20.3, 13.5, 19.2]...\nanomaly     : [True, False, False]...",
    "crumbs": [
      "Examples",
      "Typed Dataset Pipeline"
    ]
  },
  {
    "objectID": "examples/typed-pipeline.html#shuffled-iteration-for-training",
    "href": "examples/typed-pipeline.html#shuffled-iteration-for-training",
    "title": "Typed Dataset Pipeline",
    "section": "6 — Shuffled iteration for training",
    "text": "6 — Shuffled iteration for training\nFor model training you want randomized order. shuffled() applies two-level shuffling (shard order + in-buffer sample shuffling) to give well-mixed batches while staying streaming-friendly.\n\nsensor_ids_seen: set[str] = set()\n\nfor batch in ds.shuffled(batch_size=128):\n    sensor_ids_seen.update(batch.sensor_id)\n    if len(sensor_ids_seen) == 8:\n        break\n\nprint(f\"Unique sensors in early batches: {sorted(sensor_ids_seen)}\")\n\nUnique sensors in early batches: ['sensor_00', 'sensor_01', 'sensor_02', 'sensor_03', 'sensor_04', 'sensor_05', 'sensor_06', 'sensor_07']",
    "crumbs": [
      "Examples",
      "Typed Dataset Pipeline"
    ]
  },
  {
    "objectID": "examples/typed-pipeline.html#clean-up",
    "href": "examples/typed-pipeline.html#clean-up",
    "title": "Typed Dataset Pipeline",
    "section": "7 — Clean up",
    "text": "7 — Clean up\n\nimport shutil\n\nshutil.rmtree(tmpdir, ignore_errors=True)",
    "crumbs": [
      "Examples",
      "Typed Dataset Pipeline"
    ]
  },
  {
    "objectID": "examples/typed-pipeline.html#key-takeaways",
    "href": "examples/typed-pipeline.html#key-takeaways",
    "title": "Typed Dataset Pipeline",
    "section": "Key takeaways",
    "text": "Key takeaways\n\n\n\n\n\n\n\nConcept\nAPI\n\n\n\n\nType-safe samples\n@atdata.packable\n\n\nAutomatic serialization\n.packed / .from_bytes()\n\n\nSharded writes with manifests\natdata.write_samples(..., maxcount=N, manifest=True)\n\n\nNumpy scalar coercion\nnp.float64 → float automatically\n\n\nOrdered iteration\nds.ordered(batch_size=N)\n\n\nShuffled iteration\nds.shuffled(batch_size=N)\n\n\nBatch aggregation\nbatch.field (NDArray stacking or list)",
    "crumbs": [
      "Examples",
      "Typed Dataset Pipeline"
    ]
  },
  {
    "objectID": "examples/manifest-queries.html",
    "href": "examples/manifest-queries.html",
    "title": "Manifest-Powered Queries",
    "section": "",
    "text": "Large datasets become unwieldy when you need to find specific samples. atdata’s manifest system records per-shard metadata at write time, then lets you query across shards using pandas predicates—without ever opening the tar files themselves.\nThis example builds a labeled image dataset with manifests, writes them to JSON + Parquet sidecar files, and queries for specific samples.",
    "crumbs": [
      "Examples",
      "Manifest-Powered Queries"
    ]
  },
  {
    "objectID": "examples/manifest-queries.html#define-a-sample-type-with-manifest-aware-fields",
    "href": "examples/manifest-queries.html#define-a-sample-type-with-manifest-aware-fields",
    "title": "Manifest-Powered Queries",
    "section": "1 — Define a sample type with manifest-aware fields",
    "text": "1 — Define a sample type with manifest-aware fields\nPrimitive fields (str, int, float, bool, list) are automatically included in manifests with inferred aggregate types. You can also use Annotated[..., ManifestField(...)] for explicit control.\n\nimport numpy as np\nfrom numpy.typing import NDArray\nfrom typing import Annotated\nimport atdata\nfrom atdata import ManifestField\n\n\n@atdata.packable\nclass LabeledImage:\n    \"\"\"Image sample with queryable metadata.\"\"\"\n\n    pixels: NDArray\n    label: Annotated[str, ManifestField(\"categorical\")]\n    confidence: Annotated[float, ManifestField(\"numeric\")]\n    tags: Annotated[list[str], ManifestField(\"set\")]\n\nThe ManifestField annotations tell atdata which aggregate statistics to collect:\n\ncategorical — tracks the set of distinct values\nnumeric — tracks min, max, mean, count\nset — tracks the union of all values across list fields",
    "crumbs": [
      "Examples",
      "Manifest-Powered Queries"
    ]
  },
  {
    "objectID": "examples/manifest-queries.html#see-which-fields-are-manifest-included",
    "href": "examples/manifest-queries.html#see-which-fields-are-manifest-included",
    "title": "Manifest-Powered Queries",
    "section": "2 — See which fields are manifest-included",
    "text": "2 — See which fields are manifest-included\n\nfrom atdata.manifest import resolve_manifest_fields\n\nfields = resolve_manifest_fields(LabeledImage)\nfor name, mf in fields.items():\n    print(f\"  {name:15s} -&gt; {mf.aggregate}\")\n\n  label           -&gt; categorical\n  confidence      -&gt; numeric\n  tags            -&gt; set",
    "crumbs": [
      "Examples",
      "Manifest-Powered Queries"
    ]
  },
  {
    "objectID": "examples/manifest-queries.html#generate-data-and-write-shards-with-manifests",
    "href": "examples/manifest-queries.html#generate-data-and-write-shards-with-manifests",
    "title": "Manifest-Powered Queries",
    "section": "3 — Generate data and write shards with manifests",
    "text": "3 — Generate data and write shards with manifests\nPass manifest=True to write_samples and atdata builds the sidecar manifest files automatically—no manual ManifestBuilder plumbing needed.\n\nimport tempfile\nfrom pathlib import Path\n\ntmpdir = Path(tempfile.mkdtemp(prefix=\"atdata_manifest_\"))\nrng = np.random.default_rng(7)\n\ncategories = [\"cat\", \"dog\", \"bird\", \"fish\", \"horse\"]\nall_tags = [\"outdoor\", \"indoor\", \"closeup\", \"wide\", \"blurry\", \"sharp\"]\nnum_samples = 300\n\nsamples = [\n    LabeledImage(\n        pixels=rng.integers(0, 255, (32, 32, 3), dtype=np.uint8),\n        label=categories[rng.integers(0, len(categories))],\n        confidence=round(float(rng.uniform(0.5, 1.0)), 3),\n        tags=list(rng.choice(all_tags, size=rng.integers(1, 4), replace=False)),\n    )\n    for _ in range(num_samples)\n]\n\nds = atdata.write_samples(\n    samples,\n    tmpdir / \"images.tar\",\n    maxcount=100,       # 3 shards of 100 samples each\n    manifest=True,      # auto-build JSON + Parquet sidecar manifests\n)\n\nprint(f\"Wrote {num_samples} samples across {len(ds.list_shards())} shards\")\nprint(f\"Manifest files: {sorted(p.name for p in tmpdir.glob('*.manifest.*'))}\")\n\n# writing /var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_manifest_vbxke_zq/images-000000.tar 0 0.0 GB 0\n# writing /var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_manifest_vbxke_zq/images-000001.tar 100 0.0 GB 100\n# writing /var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_manifest_vbxke_zq/images-000002.tar 100 0.0 GB 200\nWrote 300 samples across 3 shards\nManifest files: ['images-000000.manifest.json', 'images-000000.manifest.parquet', 'images-000001.manifest.json', 'images-000001.manifest.parquet', 'images-000002.manifest.json', 'images-000002.manifest.parquet']",
    "crumbs": [
      "Examples",
      "Manifest-Powered Queries"
    ]
  },
  {
    "objectID": "examples/manifest-queries.html#inspect-a-manifest",
    "href": "examples/manifest-queries.html#inspect-a-manifest",
    "title": "Manifest-Powered Queries",
    "section": "4 — Inspect a manifest",
    "text": "4 — Inspect a manifest\nEach shard gets a JSON header (aggregates + metadata) and a Parquet file (per-sample columns).\n\nimport json\n\nmanifest_json = sorted(tmpdir.glob(\"*.manifest.json\"))[0]\nwith open(manifest_json) as f:\n    header = json.load(f)\n\nprint(\"Shard:\", header[\"shard_id\"])\nprint(\"Samples:\", header[\"num_samples\"])\nprint()\nprint(\"Aggregates:\")\nfor field_name, agg in header[\"aggregates\"].items():\n    print(f\"  {field_name}: {agg}\")\n\nShard: images\nSamples: 100\n\nAggregates:\n  label: {'type': 'categorical', 'cardinality': 5, 'value_counts': {'horse': 22, 'bird': 26, 'fish': 25, 'dog': 12, 'cat': 15}}\n  confidence: {'type': 'numeric', 'min': 0.509, 'max': 0.996, 'mean': 0.7610199999999999, 'count': 100}\n  tags: {'type': 'set', 'all_values': ['blurry', 'closeup', 'indoor', 'outdoor', 'sharp', 'wide']}",
    "crumbs": [
      "Examples",
      "Manifest-Powered Queries"
    ]
  },
  {
    "objectID": "examples/manifest-queries.html#query-across-shards",
    "href": "examples/manifest-queries.html#query-across-shards",
    "title": "Manifest-Powered Queries",
    "section": "5 — Query across shards",
    "text": "5 — Query across shards\nQueryExecutor loads all manifests from a directory and runs a pandas predicate over the per-sample Parquet data.\n\nfrom atdata import QueryExecutor\n\nexecutor = QueryExecutor.from_directory(tmpdir)\n\n# Find high-confidence dog samples\nresults = executor.query(\n    where=lambda df: (df[\"confidence\"] &gt; 0.9) & (df[\"label\"] == \"dog\")\n)\n\nprint(f\"Found {len(results)} high-confidence dog samples\")\nfor loc in results[:5]:\n    print(f\"  shard={loc.shard}  key={loc.key}\")\n\nFound 6 high-confidence dog samples\n  shard=images  key=f8548f8e-0173-11f1-8000-000000000000\n  shard=images-000000  key=f8550748-0173-11f1-8000-000000000000\n  shard=images-000000  key=f8551698-0173-11f1-8000-000000000000\n  shard=images-000000  key=f8551af8-0173-11f1-8000-000000000000\n  shard=images-000000  key=f85551f8-0173-11f1-8000-000000000000",
    "crumbs": [
      "Examples",
      "Manifest-Powered Queries"
    ]
  },
  {
    "objectID": "examples/manifest-queries.html#compound-queries",
    "href": "examples/manifest-queries.html#compound-queries",
    "title": "Manifest-Powered Queries",
    "section": "6 — Compound queries",
    "text": "6 — Compound queries\nThe predicate receives a full DataFrame, so you can compose arbitrarily complex filters.\n\n# Samples tagged \"outdoor\" with confidence below 0.7\nresults = executor.query(\n    where=lambda df: (df[\"confidence\"] &lt; 0.7)\n    & (df[\"tags\"].apply(lambda t: \"outdoor\" in t if isinstance(t, list) else False))\n)\n\nprint(f\"Found {len(results)} low-confidence outdoor samples\")\n\nFound 22 low-confidence outdoor samples\n\n\n\n# Count samples per label across all shards\nimport pandas as pd\n\nall_dfs = [m.samples for m in executor._manifests if not m.samples.empty]\ncombined = pd.concat(all_dfs, ignore_index=True)\nprint(combined[\"label\"].value_counts().to_string())\n\nlabel\nfish     50\nbird     45\nhorse    38\ndog      36\ncat      31",
    "crumbs": [
      "Examples",
      "Manifest-Powered Queries"
    ]
  },
  {
    "objectID": "examples/manifest-queries.html#clean-up",
    "href": "examples/manifest-queries.html#clean-up",
    "title": "Manifest-Powered Queries",
    "section": "7 — Clean up",
    "text": "7 — Clean up\n\nimport shutil\n\nshutil.rmtree(tmpdir, ignore_errors=True)",
    "crumbs": [
      "Examples",
      "Manifest-Powered Queries"
    ]
  },
  {
    "objectID": "examples/manifest-queries.html#key-takeaways",
    "href": "examples/manifest-queries.html#key-takeaways",
    "title": "Manifest-Powered Queries",
    "section": "Key takeaways",
    "text": "Key takeaways\n\n\n\n\n\n\n\nConcept\nAPI\n\n\n\n\nMark fields for manifests\nAnnotated[T, ManifestField(\"categorical\")]\n\n\nResolve manifest fields\nresolve_manifest_fields(SampleType)\n\n\nWrite with manifests\nwrite_samples(samples, path, manifest=True)\n\n\nQuery across shards\nQueryExecutor.from_directory(path).query(where=...)\n\n\nLocate matching samples\nReturns list[SampleLocation] with shard + key",
    "crumbs": [
      "Examples",
      "Manifest-Powered Queries"
    ]
  },
  {
    "objectID": "examples/multi-split.html",
    "href": "examples/multi-split.html",
    "title": "Multi-Split Datasets",
    "section": "",
    "text": "Real-world ML workflows almost always have multiple splits—train, validation, test. atdata’s DatasetDict and load_dataset support this natively, following the HuggingFace convention of naming shards by split.\nThis example generates a toy classification dataset with train/test splits, writes them as separate shard sets, and loads everything back through the unified load_dataset API.",
    "crumbs": [
      "Examples",
      "Multi-Split Datasets"
    ]
  },
  {
    "objectID": "examples/multi-split.html#define-a-sample-type",
    "href": "examples/multi-split.html#define-a-sample-type",
    "title": "Multi-Split Datasets",
    "section": "1 — Define a sample type",
    "text": "1 — Define a sample type\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport atdata\n\n\n@atdata.packable\nclass DigitSample:\n    \"\"\"A small grayscale digit image with label.\"\"\"\n\n    image: NDArray  # (28, 28) float32\n    label: int\n    split: str",
    "crumbs": [
      "Examples",
      "Multi-Split Datasets"
    ]
  },
  {
    "objectID": "examples/multi-split.html#generate-train-and-test-data",
    "href": "examples/multi-split.html#generate-train-and-test-data",
    "title": "Multi-Split Datasets",
    "section": "2 — Generate train and test data",
    "text": "2 — Generate train and test data\n\nrng = np.random.default_rng(12)\n\ndef make_samples(n: int, split: str) -&gt; list[DigitSample]:\n    return [\n        DigitSample(\n            image=rng.random((28, 28), dtype=np.float32),\n            label=int(rng.integers(0, 10)),\n            split=split,\n        )\n        for _ in range(n)\n    ]\n\ntrain_samples = make_samples(1_000, \"train\")\ntest_samples = make_samples(200, \"test\")\n\nprint(f\"Train: {len(train_samples)} samples\")\nprint(f\"Test : {len(test_samples)} samples\")\n\nTrain: 1000 samples\nTest : 200 samples",
    "crumbs": [
      "Examples",
      "Multi-Split Datasets"
    ]
  },
  {
    "objectID": "examples/multi-split.html#write-each-split-to-its-own-shard-set",
    "href": "examples/multi-split.html#write-each-split-to-its-own-shard-set",
    "title": "Multi-Split Datasets",
    "section": "3 — Write each split to its own shard set",
    "text": "3 — Write each split to its own shard set\nFollowing the {split}-{shard} naming convention lets load_dataset detect splits automatically.\n\nimport tempfile\nfrom pathlib import Path\n\ntmpdir = Path(tempfile.mkdtemp(prefix=\"atdata_splits_\"))\n\ntrain_ds = atdata.write_samples(\n    train_samples,\n    tmpdir / \"train.tar\",\n    maxcount=500,\n)\nprint(f\"Train shards: {train_ds.list_shards()}\")\n\ntest_ds = atdata.write_samples(\n    test_samples,\n    tmpdir / \"test.tar\",\n    maxcount=200,\n)\nprint(f\"Test shards : {test_ds.list_shards()}\")\n\n# writing /var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_splits_nelcv7no/train-000000.tar 0 0.0 GB 0\n# writing /var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_splits_nelcv7no/train-000001.tar 500 0.0 GB 500\nTrain shards: ['/private/var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_splits_nelcv7no/train-000000.tar', '/private/var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_splits_nelcv7no/train-000001.tar']\n# writing /var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_splits_nelcv7no/test-000000.tar 0 0.0 GB 0\nTest shards : ['/private/var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_splits_nelcv7no/test-000000.tar']",
    "crumbs": [
      "Examples",
      "Multi-Split Datasets"
    ]
  },
  {
    "objectID": "examples/multi-split.html#load-a-single-split",
    "href": "examples/multi-split.html#load-a-single-split",
    "title": "Multi-Split Datasets",
    "section": "4 — Load a single split",
    "text": "4 — Load a single split\nPass a specific shard URL and split to load_dataset to get a single Dataset.\n\nds_train = atdata.load_dataset(\n    str(tmpdir / \"train-{000000..000001}.tar\"),\n    DigitSample,\n    split=\"train\",\n)\n\nfor batch in ds_train.ordered(batch_size=32):\n    print(f\"Image batch shape : {batch.image.shape}\")\n    print(f\"Labels            : {batch.label[:8]}...\")\n    break\n\nImage batch shape : (32, 28, 28)\nLabels            : [2, 2, 3, 4, 7, 9, 7, 1]...",
    "crumbs": [
      "Examples",
      "Multi-Split Datasets"
    ]
  },
  {
    "objectID": "examples/multi-split.html#build-a-datasetdict-from-multiple-splits",
    "href": "examples/multi-split.html#build-a-datasetdict-from-multiple-splits",
    "title": "Multi-Split Datasets",
    "section": "5 — Build a DatasetDict from multiple splits",
    "text": "5 — Build a DatasetDict from multiple splits\nWrap per-split Dataset objects into a DatasetDict for convenient multi-split access.\n\nds_dict = atdata.DatasetDict({\n    \"train\": atdata.Dataset[DigitSample](str(tmpdir / \"train-{000000..000001}.tar\")),\n    \"test\": atdata.Dataset[DigitSample](str(tmpdir / \"test-000000.tar\")),\n})\n\nprint(f\"Splits: {list(ds_dict.keys())}\")\nprint(f\"Train type: {type(ds_dict['train']).__name__}\")\nprint(f\"Test type : {type(ds_dict['test']).__name__}\")\n\nSplits: ['train', 'test']\nTrain type: Dataset\nTest type : Dataset",
    "crumbs": [
      "Examples",
      "Multi-Split Datasets"
    ]
  },
  {
    "objectID": "examples/multi-split.html#single-split-proxy",
    "href": "examples/multi-split.html#single-split-proxy",
    "title": "Multi-Split Datasets",
    "section": "6 — Single-split proxy",
    "text": "6 — Single-split proxy\nWhen a DatasetDict has exactly one split, you can call Dataset methods directly on it—no need to index by split name first.\n\nsingle = atdata.DatasetDict({\n    \"train\": atdata.Dataset[DigitSample](str(tmpdir / \"train-{000000..000001}.tar\")),\n})\n\n# .ordered() is proxied to the sole split automatically\nfor batch in single.ordered(batch_size=32):\n    print(f\"Proxy batch shape: {batch.image.shape}\")\n    break\n\nProxy batch shape: (32, 28, 28)",
    "crumbs": [
      "Examples",
      "Multi-Split Datasets"
    ]
  },
  {
    "objectID": "examples/multi-split.html#iterate-over-each-split",
    "href": "examples/multi-split.html#iterate-over-each-split",
    "title": "Multi-Split Datasets",
    "section": "7 — Iterate over each split",
    "text": "7 — Iterate over each split\n\nfor split_name, split_ds in ds_dict.items():\n    count = 0\n    for batch in split_ds.ordered(batch_size=64):\n        count += len(batch.label)\n    print(f\"  {split_name:6s}: {count} samples iterated\")\n\n  train : 1000 samples iterated\n  test  : 200 samples iterated",
    "crumbs": [
      "Examples",
      "Multi-Split Datasets"
    ]
  },
  {
    "objectID": "examples/multi-split.html#cross-split-statistics",
    "href": "examples/multi-split.html#cross-split-statistics",
    "title": "Multi-Split Datasets",
    "section": "8 — Cross-split statistics",
    "text": "8 — Cross-split statistics\nBecause both splits share the same DigitSample type, you can compute comparable statistics directly.\n\nfor split_name, split_ds in ds_dict.items():\n    all_labels: list[int] = []\n    for batch in split_ds.ordered(batch_size=256):\n        all_labels.extend(batch.label)\n\n    label_arr = np.array(all_labels)\n    counts = np.bincount(label_arr, minlength=10)\n    print(f\"  {split_name:6s}: {len(all_labels)} samples, label distribution: {counts.tolist()}\")\n\n  train : 1000 samples, label distribution: [113, 85, 95, 100, 109, 91, 110, 100, 103, 94]\n  test  : 200 samples, label distribution: [23, 20, 19, 23, 21, 20, 19, 19, 19, 17]",
    "crumbs": [
      "Examples",
      "Multi-Split Datasets"
    ]
  },
  {
    "objectID": "examples/multi-split.html#clean-up",
    "href": "examples/multi-split.html#clean-up",
    "title": "Multi-Split Datasets",
    "section": "9 — Clean up",
    "text": "9 — Clean up\n\nimport shutil\n\nshutil.rmtree(tmpdir, ignore_errors=True)",
    "crumbs": [
      "Examples",
      "Multi-Split Datasets"
    ]
  },
  {
    "objectID": "examples/multi-split.html#key-takeaways",
    "href": "examples/multi-split.html#key-takeaways",
    "title": "Multi-Split Datasets",
    "section": "Key takeaways",
    "text": "Key takeaways\n\n\n\n\n\n\n\nConcept\nAPI\n\n\n\n\nWrite split shards\nwrite_samples(samples, \"split.tar\", maxcount=N)\n\n\nLoad single split\nload_dataset(\"path/train-{000..N}.tar\")\n\n\nBuild a DatasetDict\nDatasetDict({\"train\": ds1, \"test\": ds2})\n\n\nSingle-split proxy\nds_dict.ordered() when only one split\n\n\nIterate splits\nfor name, ds in ds_dict.items()\n\n\nTyped across splits\nSame @packable type works everywhere",
    "crumbs": [
      "Examples",
      "Multi-Split Datasets"
    ]
  },
  {
    "objectID": "examples/lens-transforms.html",
    "href": "examples/lens-transforms.html",
    "title": "Lens Transformations",
    "section": "",
    "text": "Lenses let you reinterpret existing data under a different type schema—without duplicating storage. This example defines a rich sample type, creates two alternative views, and shows how the global LensNetwork routes as_type() calls automatically.",
    "crumbs": [
      "Examples",
      "Lens Transformations"
    ]
  },
  {
    "objectID": "examples/lens-transforms.html#define-the-source-schema",
    "href": "examples/lens-transforms.html#define-the-source-schema",
    "title": "Lens Transformations",
    "section": "1 — Define the source schema",
    "text": "1 — Define the source schema\nImagine a multi-modal dataset that stores images, text captions, and classification metadata together.\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport atdata\n\n\n@atdata.packable\nclass RichSample:\n    \"\"\"Multi-modal sample with image, caption, and classification.\"\"\"\n\n    image: NDArray\n    caption: str\n    label: str\n    split: str\n    confidence: float",
    "crumbs": [
      "Examples",
      "Lens Transformations"
    ]
  },
  {
    "objectID": "examples/lens-transforms.html#define-view-schemas",
    "href": "examples/lens-transforms.html#define-view-schemas",
    "title": "Lens Transformations",
    "section": "2 — Define view schemas",
    "text": "2 — Define view schemas\nDifferent consumers need different slices of this data. An image classifier only needs the image and label; a text model only needs the caption.\n\n@atdata.packable\nclass ClassificationView:\n    \"\"\"Minimal view for image classification tasks.\"\"\"\n\n    image: NDArray\n    label: str\n\n\n@atdata.packable\nclass CaptionView:\n    \"\"\"Text-only view for captioning or NLP tasks.\"\"\"\n\n    caption: str\n    label: str\n    confidence: float",
    "crumbs": [
      "Examples",
      "Lens Transformations"
    ]
  },
  {
    "objectID": "examples/lens-transforms.html#register-lenses",
    "href": "examples/lens-transforms.html#register-lenses",
    "title": "Lens Transformations",
    "section": "3 — Register lenses",
    "text": "3 — Register lenses\nA lens is a getter (source -&gt; view) and an optional putter (view, source -&gt; source). The @atdata.lens decorator registers the transformation globally.\n\n@atdata.lens\ndef to_classification(src: RichSample) -&gt; ClassificationView:\n    return ClassificationView(image=src.image, label=src.label)\n\n\n@to_classification.putter\ndef to_classification_put(\n    view: ClassificationView, src: RichSample\n) -&gt; RichSample:\n    return RichSample(\n        image=view.image,\n        caption=src.caption,\n        label=view.label,\n        split=src.split,\n        confidence=src.confidence,\n    )\n\n\n@atdata.lens\ndef to_caption(src: RichSample) -&gt; CaptionView:\n    return CaptionView(\n        caption=src.caption,\n        label=src.label,\n        confidence=src.confidence,\n    )",
    "crumbs": [
      "Examples",
      "Lens Transformations"
    ]
  },
  {
    "objectID": "examples/lens-transforms.html#write-a-small-dataset",
    "href": "examples/lens-transforms.html#write-a-small-dataset",
    "title": "Lens Transformations",
    "section": "4 — Write a small dataset",
    "text": "4 — Write a small dataset\n\nimport tempfile\nfrom pathlib import Path\n\nrng = np.random.default_rng(0)\nlabels = [\"cat\", \"dog\", \"bird\", \"fish\"]\n\nsamples = [\n    RichSample(\n        image=rng.integers(0, 255, (64, 64, 3), dtype=np.uint8),\n        caption=f\"A photo of a {labels[i % 4]}\",\n        label=labels[i % 4],\n        split=\"train\",\n        confidence=round(float(rng.uniform(0.7, 1.0)), 3),\n    )\n    for i in range(200)\n]\n\ntmpdir = Path(tempfile.mkdtemp(prefix=\"atdata_lens_\"))\nds = atdata.write_samples(samples, tmpdir / \"data.tar\", maxcount=100)\nprint(f\"Wrote {len(samples)} samples across {len(ds.list_shards())} shards\")\n\n# writing /var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_lens_qdg5pnot/data-000000.tar 0 0.0 GB 0\n# writing /var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_lens_qdg5pnot/data-000001.tar 100 0.0 GB 100\nWrote 200 samples across 2 shards",
    "crumbs": [
      "Examples",
      "Lens Transformations"
    ]
  },
  {
    "objectID": "examples/lens-transforms.html#view-through-lenses",
    "href": "examples/lens-transforms.html#view-through-lenses",
    "title": "Lens Transformations",
    "section": "5 — View through lenses",
    "text": "5 — View through lenses\nDataset.as_type() looks up the registered lens automatically via the LensNetwork singleton.\n\n# Classification view -- only image + label\ncls_ds = ds.as_type(ClassificationView)\n\nfor batch in cls_ds.ordered(batch_size=16):\n    print(f\"image shape : {batch.image.shape}\")   # (16, 64, 64, 3)\n    print(f\"labels      : {batch.label[:4]}...\")\n    break\n\nimage shape : (16, 64, 64, 3)\nlabels      : ['cat', 'dog', 'bird', 'fish']...\n\n\n\n# Caption view -- only text fields\ncap_ds = ds.as_type(CaptionView)\n\nfor batch in cap_ds.ordered(batch_size=16):\n    print(f\"captions    : {batch.caption[:2]}...\")\n    print(f\"confidence  : {batch.confidence[:4]}...\")\n    break\n\ncaptions    : ['A photo of a cat', 'A photo of a dog']...\nconfidence  : [0.714, 0.82, 0.896, 0.725]...\n\n\nThe underlying tar files are read once; the lens getter runs per-sample during iteration. No extra storage, no ETL step.",
    "crumbs": [
      "Examples",
      "Lens Transformations"
    ]
  },
  {
    "objectID": "examples/lens-transforms.html#round-trip-with-putter",
    "href": "examples/lens-transforms.html#round-trip-with-putter",
    "title": "Lens Transformations",
    "section": "6 — Round-trip with putter",
    "text": "6 — Round-trip with putter\nA lens with a putter supports the put direction: update the view, then propagate changes back to the source while preserving untouched fields.\n\noriginal = samples[0]\nprint(f\"Original label : {original.label}\")\n\n# Get the classification view\nview = to_classification.get(original)\nprint(f\"View label     : {view.label}\")\n\n# Modify the view\ncorrected = ClassificationView(image=view.image, label=\"kitten\")\n\n# Put it back -- caption, split, confidence are preserved\nupdated = to_classification.put(corrected, original)\nprint(f\"Updated label  : {updated.label}\")\nprint(f\"Caption kept   : {updated.caption}\")\n\nOriginal label : cat\nView label     : cat\nUpdated label  : kitten\nCaption kept   : A photo of a cat",
    "crumbs": [
      "Examples",
      "Lens Transformations"
    ]
  },
  {
    "objectID": "examples/lens-transforms.html#inspect-the-lensnetwork",
    "href": "examples/lens-transforms.html#inspect-the-lensnetwork",
    "title": "Lens Transformations",
    "section": "7 — Inspect the LensNetwork",
    "text": "7 — Inspect the LensNetwork\nThe global registry lets you see all registered transformations at a glance.\n\nnetwork = atdata.LensNetwork()\n\nfor (src, view), lens_obj in network._registry.items():\n    if src is RichSample:\n        print(f\"  {src.__name__} -&gt; {view.__name__}  via {lens_obj.__name__}\")\n\n  RichSample -&gt; ClassificationView  via to_classification\n  RichSample -&gt; CaptionView  via to_caption",
    "crumbs": [
      "Examples",
      "Lens Transformations"
    ]
  },
  {
    "objectID": "examples/lens-transforms.html#clean-up",
    "href": "examples/lens-transforms.html#clean-up",
    "title": "Lens Transformations",
    "section": "8 — Clean up",
    "text": "8 — Clean up\n\nimport shutil\n\nshutil.rmtree(tmpdir, ignore_errors=True)",
    "crumbs": [
      "Examples",
      "Lens Transformations"
    ]
  },
  {
    "objectID": "examples/lens-transforms.html#key-takeaways",
    "href": "examples/lens-transforms.html#key-takeaways",
    "title": "Lens Transformations",
    "section": "Key takeaways",
    "text": "Key takeaways\n\n\n\nConcept\nAPI\n\n\n\n\nDefine a lens\n@atdata.lens decorator\n\n\nAdd a putter\n@my_lens.putter decorator\n\n\nView a dataset\nds.as_type(ViewType)\n\n\nForward transform\nlens.get(source)\n\n\nReverse transform\nlens.put(view, source)\n\n\nInspect registry\natdata.LensNetwork()._registry\n\n\n\nLenses compose naturally with batching and shuffling—as_type() returns a full Dataset that supports every iteration mode.",
    "crumbs": [
      "Examples",
      "Lens Transformations"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "atdata",
    "section": "",
    "text": "Get Started Examples Benchmarks GitHub",
    "crumbs": [
      "atdata"
    ]
  },
  {
    "objectID": "index.html#what-is-atdata",
    "href": "index.html#what-is-atdata",
    "title": "atdata",
    "section": "What is atdata?",
    "text": "What is atdata?\natdata is a Python library for typed, serializable datasets that flow seamlessly from local development to managed storage to federated sharing—all with the same sample types and consistent APIs.\n\n\nTyped Samples\nDefine dataclass-based sample types with @packable. Automatic msgpack serialization and NDArray handling.\n\n\nOne-Call Writes\nwrite_samples() handles sharding, key generation, and optional manifest sidecar files.\n\n\nBatch Aggregation\nNDArray fields are stacked automatically. Scalar fields become lists. No manual collation code.\n\n\nLens Transformations\nView datasets through different schemas without duplicating data. Bidirectional with putters.\n\n\nManaged Storage\nIndex with pluggable providers (SQLite, Redis, PostgreSQL) and data stores (local disk, S3). Schema auto-resolution on load.\n\n\nATProto Federation\nPublish datasets to the decentralized AT Protocol network. Cross-organization discovery via your Bluesky identity.",
    "crumbs": [
      "atdata"
    ]
  },
  {
    "objectID": "index.html#quick-example",
    "href": "index.html#quick-example",
    "title": "atdata",
    "section": "Quick Example",
    "text": "Quick Example\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport atdata\n\n@atdata.packable\nclass ImageSample:\n    image: NDArray\n    label: str\n    confidence: float\n\n# Write sharded tar files (returns a typed Dataset)\nsamples = [ImageSample(image=np.random.rand(224, 224, 3).astype(np.float32),\n                        label=\"cat\", confidence=0.95) for _ in range(100)]\nds = atdata.write_samples(samples, \"data.tar\", maxcount=50)\n\n# Iterate with automatic batching\nfor batch in ds.shuffled(batch_size=32):\n    images = batch.image          # (32, 224, 224, 3) numpy array\n    labels = batch.label          # list of 32 strings\n    break",
    "crumbs": [
      "atdata"
    ]
  },
  {
    "objectID": "index.html#scaling-up",
    "href": "index.html#scaling-up",
    "title": "atdata",
    "section": "Scaling Up",
    "text": "Scaling Up\n\nManaged storage with Index\n\nindex = atdata.Index(data_store=atdata.LocalDiskStore())\nentry = index.write(samples, name=\"training-v1\")\n\n# Load by name --- schema auto-resolved, no sample_type needed\nds = atdata.load_dataset(\"@local/training-v1\", split=\"train\")\n\n\n\nFederation with ATProto\n\nfrom atdata.atmosphere import Atmosphere\n\nclient = Atmosphere.login(\"handle.bsky.social\", \"app-password\")\nindex = atdata.Index(atmosphere=client)\nuri = index.promote_entry(\"training-v1\")",
    "crumbs": [
      "atdata"
    ]
  },
  {
    "objectID": "index.html#the-architecture",
    "href": "index.html#the-architecture",
    "title": "atdata",
    "section": "The Architecture",
    "text": "The Architecture\n┌─────────────────────────────────────────────────────────────┐\n│  Federation: ATProto Atmosphere                             │\n│  Decentralized discovery, cross-org sharing                 │\n└─────────────────────────────────────────────────────────────┘\n                              ↑ promote\n┌─────────────────────────────────────────────────────────────┐\n│  Managed Storage: Index + SQLite/Redis + Disk/S3            │\n│  Schema registry, versioned datasets, persistent data       │\n└─────────────────────────────────────────────────────────────┘\n                              ↑ index.write\n┌─────────────────────────────────────────────────────────────┐\n│  Local Development                                          │\n│  write_samples(), typed iteration, lenses                   │\n└─────────────────────────────────────────────────────────────┘",
    "crumbs": [
      "atdata"
    ]
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "atdata",
    "section": "Installation",
    "text": "Installation\n\npip install atdata\n\n# With ATProto support\npip install atdata[atmosphere]",
    "crumbs": [
      "atdata"
    ]
  },
  {
    "objectID": "index.html#next-steps",
    "href": "index.html#next-steps",
    "title": "atdata",
    "section": "Next Steps",
    "text": "Next Steps\n\nQuick Start — Define samples, write datasets, iterate with batching\nExamples — Five end-to-end worked examples\nLocal Workflow — Index-managed storage for teams\nAtmosphere Publishing — Publish to the ATProto network\nAPI Reference — Architecture overview and API docs\nBenchmarks — Performance benchmark results",
    "crumbs": [
      "atdata"
    ]
  },
  {
    "objectID": "examples/dataset-profiler.html",
    "href": "examples/dataset-profiler.html",
    "title": "Dataset Profiler",
    "section": "",
    "text": "Manifests record per-shard statistics and per-sample metadata at write time. That means you can profile an entire dataset — class distributions, numeric ranges, tag frequencies — without ever cracking open a tar file.\nThis example generates a synthetic NLP sentiment corpus, writes it with manifests, then builds a visual profile using only the sidecar files.",
    "crumbs": [
      "Examples",
      "Dataset Profiler"
    ]
  },
  {
    "objectID": "examples/dataset-profiler.html#define-a-sentiment-sample-type",
    "href": "examples/dataset-profiler.html#define-a-sentiment-sample-type",
    "title": "Dataset Profiler",
    "section": "1 — Define a sentiment sample type",
    "text": "1 — Define a sentiment sample type\n\nimport numpy as np\nfrom numpy.typing import NDArray\nfrom typing import Annotated\nimport atdata\nfrom atdata import ManifestField\n\n\n@atdata.packable\nclass SentimentSample:\n    \"\"\"A text-embedding sample with rich metadata for profiling.\"\"\"\n\n    embedding: NDArray\n    sentiment: Annotated[str, ManifestField(\"categorical\")]\n    confidence: Annotated[float, ManifestField(\"numeric\")]\n    language: Annotated[str, ManifestField(\"categorical\")]\n    tags: Annotated[list[str], ManifestField(\"set\")]\n\nThe embedding field (NDArray) is automatically excluded from the manifest. The other four fields each get an appropriate aggregate collector.",
    "crumbs": [
      "Examples",
      "Dataset Profiler"
    ]
  },
  {
    "objectID": "examples/dataset-profiler.html#generate-1000-synthetic-samples",
    "href": "examples/dataset-profiler.html#generate-1000-synthetic-samples",
    "title": "Dataset Profiler",
    "section": "2 — Generate 1,000 synthetic samples",
    "text": "2 — Generate 1,000 synthetic samples\nWe use weighted distributions to make the profile interesting: sentiment is skewed positive, confidence follows a beta distribution, and language has realistic frequency skew.\n\nrng = np.random.default_rng(2024)\n\nsentiments = rng.choice(\n    [\"positive\", \"negative\", \"neutral\"],\n    size=1_000,\n    p=[0.50, 0.20, 0.30],\n)\n\nconfidences = rng.beta(5, 2, size=1_000)  # right-skewed toward 1.0\n\nlanguages = rng.choice(\n    [\"en\", \"es\", \"fr\", \"de\", \"ja\"],\n    size=1_000,\n    p=[0.55, 0.15, 0.12, 0.10, 0.08],\n)\n\ntag_pool = [\"formal\", \"informal\", \"slang\", \"technical\", \"conversational\",\n            \"academic\", \"casual\", \"literary\"]\n\nsamples = [\n    SentimentSample(\n        embedding=rng.standard_normal(64).astype(np.float32),\n        sentiment=str(sentiments[i]),\n        confidence=round(float(confidences[i]), 4),\n        language=str(languages[i]),\n        tags=list(rng.choice(tag_pool, size=rng.integers(1, 4), replace=False)),\n    )\n    for i in range(1_000)\n]\n\nprint(f\"Generated {len(samples)} samples\")\nprint(f\"Embedding shape: {samples[0].embedding.shape}\")\nprint(f\"Example: sentiment={samples[0].sentiment!r}, confidence={samples[0].confidence}\")\n\nGenerated 1000 samples\nEmbedding shape: (64,)\nExample: sentiment='negative', confidence=0.7578",
    "crumbs": [
      "Examples",
      "Dataset Profiler"
    ]
  },
  {
    "objectID": "examples/dataset-profiler.html#write-shards-with-manifests",
    "href": "examples/dataset-profiler.html#write-shards-with-manifests",
    "title": "Dataset Profiler",
    "section": "3 — Write shards with manifests",
    "text": "3 — Write shards with manifests\n\nimport tempfile\nfrom pathlib import Path\n\ntmpdir = Path(tempfile.mkdtemp(prefix=\"atdata_profiler_\"))\n\nds = atdata.write_samples(\n    samples,\n    tmpdir / \"sentiment.tar\",\n    maxcount=250,\n    manifest=True,\n)\n\nshards = ds.list_shards()\nmanifests = sorted(tmpdir.glob(\"*.manifest.*\"))\nprint(f\"Wrote {len(shards)} shards with {len(manifests)} manifest files\")\nfor m in manifests:\n    print(f\"  {m.name}\")\n\n# writing /var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_profiler_qx6hkqfm/sentiment-000000.tar 0 0.0 GB 0\n# writing /var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_profiler_qx6hkqfm/sentiment-000001.tar 250 0.0 GB 250\n# writing /var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_profiler_qx6hkqfm/sentiment-000002.tar 250 0.0 GB 500\n# writing /var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_profiler_qx6hkqfm/sentiment-000003.tar 250 0.0 GB 750\nWrote 4 shards with 8 manifest files\n  sentiment-000000.manifest.json\n  sentiment-000000.manifest.parquet\n  sentiment-000001.manifest.json\n  sentiment-000001.manifest.parquet\n  sentiment-000002.manifest.json\n  sentiment-000002.manifest.parquet\n  sentiment-000003.manifest.json\n  sentiment-000003.manifest.parquet",
    "crumbs": [
      "Examples",
      "Dataset Profiler"
    ]
  },
  {
    "objectID": "examples/dataset-profiler.html#load-manifests-and-preview",
    "href": "examples/dataset-profiler.html#load-manifests-and-preview",
    "title": "Dataset Profiler",
    "section": "4 — Load manifests and preview",
    "text": "4 — Load manifests and preview\n\n\n\n\n\n\nNote\n\n\n\nEverything from here on uses only the manifest sidecar files — the .manifest.json headers and .manifest.parquet per-sample tables. The raw tar data is never read.\n\n\n\nimport pandas as pd\nfrom atdata import QueryExecutor\n\nexecutor = QueryExecutor.from_directory(tmpdir)\ncombined = pd.concat(\n    [m.samples for m in executor._manifests if not m.samples.empty],\n    ignore_index=True,\n)\n\nprint(f\"Total samples in manifests: {len(combined)}\")\nprint(f\"Columns: {list(combined.columns)}\")\nprint()\nprint(combined[[\"sentiment\", \"confidence\", \"language\"]].head(8).to_string(index=False))\n\nTotal samples in manifests: 750\nColumns: ['__key__', '__offset__', '__size__', 'sentiment', 'confidence', 'language', 'tags']\n\nsentiment  confidence language\n negative      0.7578       en\n positive      0.8524       en\n positive      0.7038       en\n  neutral      0.9045       de\n  neutral      0.8305       es\n positive      0.5754       en\n positive      0.8700       en\n positive      0.7343       de",
    "crumbs": [
      "Examples",
      "Dataset Profiler"
    ]
  },
  {
    "objectID": "examples/dataset-profiler.html#class-distribution-bar-chart",
    "href": "examples/dataset-profiler.html#class-distribution-bar-chart",
    "title": "Dataset Profiler",
    "section": "5 — Class distribution (bar chart)",
    "text": "5 — Class distribution (bar chart)\n\nimport matplotlib\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\n\nsentiment_counts = combined[\"sentiment\"].value_counts()\ncolors = {\"positive\": \"#4CAF50\", \"neutral\": \"#9E9E9E\", \"negative\": \"#F44336\"}\n\nfig, ax = plt.subplots(figsize=(6, 3.5))\nbars = ax.bar(\n    sentiment_counts.index,\n    sentiment_counts.values,\n    color=[colors.get(s, \"#2196F3\") for s in sentiment_counts.index],\n    edgecolor=\"white\",\n    linewidth=0.8,\n)\nfor bar, count in zip(bars, sentiment_counts.values):\n    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 8,\n            str(count), ha=\"center\", va=\"bottom\", fontweight=\"bold\", fontsize=11)\n\nax.set_ylabel(\"Count\")\nax.set_title(\"Sentiment Distribution\", fontweight=\"bold\")\nax.spines[[\"top\", \"right\"]].set_visible(False)\nplt.tight_layout()\nplt.show()\n\n/var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/ipykernel_23293/3396340794.py:24: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  plt.show()",
    "crumbs": [
      "Examples",
      "Dataset Profiler"
    ]
  },
  {
    "objectID": "examples/dataset-profiler.html#confidence-score-distribution-histogram",
    "href": "examples/dataset-profiler.html#confidence-score-distribution-histogram",
    "title": "Dataset Profiler",
    "section": "6 — Confidence score distribution (histogram)",
    "text": "6 — Confidence score distribution (histogram)\nThe numeric aggregate in the manifest JSON header gives us min, max, and mean without touching parquet. Here we overlay those on the full histogram.\n\nimport json\n\nmanifest_json = sorted(tmpdir.glob(\"*.manifest.json\"))[0]\nwith open(manifest_json) as f:\n    header = json.load(f)\n\nconf_agg = header[\"aggregates\"][\"confidence\"]\nprint(f\"Aggregate from shard 0: {conf_agg}\")\n\nAggregate from shard 0: {'type': 'numeric', 'min': 0.201, 'max': 0.9863, 'mean': 0.7155908000000004, 'count': 250}\n\n\n\nfig, ax = plt.subplots(figsize=(6, 3.5))\nax.hist(combined[\"confidence\"], bins=30, color=\"#42A5F5\", edgecolor=\"white\",\n        linewidth=0.5, alpha=0.85)\n\noverall_mean = combined[\"confidence\"].mean()\noverall_min = combined[\"confidence\"].min()\noverall_max = combined[\"confidence\"].max()\n\nax.axvline(overall_mean, color=\"#E65100\", linewidth=2, linestyle=\"--\",\n           label=f\"mean = {overall_mean:.3f}\")\nax.axvline(overall_min, color=\"#78909C\", linewidth=1, linestyle=\":\",\n           label=f\"min = {overall_min:.3f}\")\nax.axvline(overall_max, color=\"#78909C\", linewidth=1, linestyle=\":\",\n           label=f\"max = {overall_max:.3f}\")\n\nax.set_xlabel(\"Confidence\")\nax.set_ylabel(\"Count\")\nax.set_title(\"Confidence Score Distribution\", fontweight=\"bold\")\nax.legend(fontsize=9)\nax.spines[[\"top\", \"right\"]].set_visible(False)\nplt.tight_layout()\nplt.show()\n\n/var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/ipykernel_23293/4059321866.py:22: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  plt.show()",
    "crumbs": [
      "Examples",
      "Dataset Profiler"
    ]
  },
  {
    "objectID": "examples/dataset-profiler.html#language-breakdown-horizontal-bar",
    "href": "examples/dataset-profiler.html#language-breakdown-horizontal-bar",
    "title": "Dataset Profiler",
    "section": "7 — Language breakdown (horizontal bar)",
    "text": "7 — Language breakdown (horizontal bar)\n\nlang_counts = combined[\"language\"].value_counts().sort_values()\ntotal = lang_counts.sum()\n\nfig, ax = plt.subplots(figsize=(6, 3))\nbars = ax.barh(lang_counts.index, lang_counts.values, color=\"#7E57C2\",\n               edgecolor=\"white\", linewidth=0.5)\n\nfor bar, count in zip(bars, lang_counts.values):\n    pct = count / total * 100\n    ax.text(bar.get_width() + 5, bar.get_y() + bar.get_height() / 2,\n            f\"{count}  ({pct:.0f}%)\", va=\"center\", fontsize=9)\n\nax.set_xlabel(\"Count\")\nax.set_title(\"Language Breakdown\", fontweight=\"bold\")\nax.spines[[\"top\", \"right\"]].set_visible(False)\nplt.tight_layout()\nplt.show()\n\n/var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/ipykernel_23293/3086307890.py:17: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  plt.show()",
    "crumbs": [
      "Examples",
      "Dataset Profiler"
    ]
  },
  {
    "objectID": "examples/dataset-profiler.html#tag-frequency-lollipop-chart",
    "href": "examples/dataset-profiler.html#tag-frequency-lollipop-chart",
    "title": "Dataset Profiler",
    "section": "8 — Tag frequency (lollipop chart)",
    "text": "8 — Tag frequency (lollipop chart)\nTags are stored as lists per sample. Exploding and counting gives us a frequency profile.\n\ntag_series = combined[\"tags\"].explode()\ntag_counts = tag_series.value_counts().sort_values()\n\nfig, ax = plt.subplots(figsize=(6, 3.5))\nax.hlines(y=tag_counts.index, xmin=0, xmax=tag_counts.values,\n          color=\"#26A69A\", linewidth=2)\nax.plot(tag_counts.values, tag_counts.index, \"o\", color=\"#00796B\", markersize=7)\n\nax.set_xlabel(\"Occurrences\")\nax.set_title(\"Tag Frequency\", fontweight=\"bold\")\nax.spines[[\"top\", \"right\"]].set_visible(False)\nplt.tight_layout()\nplt.show()\n\n/var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/ipykernel_23293/3143888597.py:13: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  plt.show()",
    "crumbs": [
      "Examples",
      "Dataset Profiler"
    ]
  },
  {
    "objectID": "examples/dataset-profiler.html#cross-tabulation-sentiment-x-language",
    "href": "examples/dataset-profiler.html#cross-tabulation-sentiment-x-language",
    "title": "Dataset Profiler",
    "section": "9 — Cross-tabulation: sentiment x language",
    "text": "9 — Cross-tabulation: sentiment x language\nA styled heatmap reveals how sentiment distributes across languages.\n\nct = pd.crosstab(combined[\"sentiment\"], combined[\"language\"])\nprint(ct.to_string())\n\nlanguage   de   en  es  fr  ja\nsentiment                     \nnegative   19   87  22  16  12\nneutral    16  111  42  32  22\npositive   38  208  66  35  24\n\n\n\nfig, ax = plt.subplots(figsize=(6, 3.5))\nim = ax.imshow(ct.values, cmap=\"YlOrRd\", aspect=\"auto\")\n\nax.set_xticks(range(len(ct.columns)))\nax.set_xticklabels(ct.columns)\nax.set_yticks(range(len(ct.index)))\nax.set_yticklabels(ct.index)\n\nfor i in range(len(ct.index)):\n    for j in range(len(ct.columns)):\n        ax.text(j, i, str(ct.values[i, j]), ha=\"center\", va=\"center\",\n                fontweight=\"bold\", fontsize=10,\n                color=\"white\" if ct.values[i, j] &gt; ct.values.max() * 0.6 else \"black\")\n\nax.set_title(\"Sentiment x Language\", fontweight=\"bold\")\nfig.colorbar(im, ax=ax, shrink=0.8)\nplt.tight_layout()\nplt.show()\n\n/var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/ipykernel_23293/2093014448.py:18: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  plt.show()",
    "crumbs": [
      "Examples",
      "Dataset Profiler"
    ]
  },
  {
    "objectID": "examples/dataset-profiler.html#query-driven-profiling",
    "href": "examples/dataset-profiler.html#query-driven-profiling",
    "title": "Dataset Profiler",
    "section": "10 — Query-driven profiling",
    "text": "10 — Query-driven profiling\nLet’s profile just the low-confidence samples to see if certain sentiments or languages are harder to classify.\n\nlow_conf = executor.query(where=lambda df: df[\"confidence\"] &lt; 0.5)\nprint(f\"Low-confidence samples (&lt;0.5): {len(low_conf)}\")\n\nlow_keys = {loc.key for loc in low_conf}\nlow_df = combined[combined[\"__key__\"].isin(low_keys)]\n\nprint(\"\\nSentiment breakdown of low-confidence samples:\")\nprint(low_df[\"sentiment\"].value_counts().to_string())\n\nprint(\"\\nLanguage breakdown of low-confidence samples:\")\nprint(low_df[\"language\"].value_counts().to_string())\n\nLow-confidence samples (&lt;0.5): 94\n\nSentiment breakdown of low-confidence samples:\nsentiment\npositive    50\nneutral     23\nnegative    21\n\nLanguage breakdown of low-confidence samples:\nlanguage\nen    39\nes    17\nde    16\nfr    11\nja    11",
    "crumbs": [
      "Examples",
      "Dataset Profiler"
    ]
  },
  {
    "objectID": "examples/dataset-profiler.html#clean-up",
    "href": "examples/dataset-profiler.html#clean-up",
    "title": "Dataset Profiler",
    "section": "11 — Clean up",
    "text": "11 — Clean up\n\nimport shutil\n\nshutil.rmtree(tmpdir, ignore_errors=True)",
    "crumbs": [
      "Examples",
      "Dataset Profiler"
    ]
  },
  {
    "objectID": "examples/dataset-profiler.html#key-takeaways",
    "href": "examples/dataset-profiler.html#key-takeaways",
    "title": "Dataset Profiler",
    "section": "Key takeaways",
    "text": "Key takeaways\n\n\n\n\n\n\n\nConcept\nAPI\n\n\n\n\nAnnotate manifest fields\nAnnotated[T, ManifestField(\"categorical\")]\n\n\nWrite with manifests\nwrite_samples(samples, path, manifest=True)\n\n\nLoad sidecar manifests\nQueryExecutor.from_directory(path)\n\n\nCombine per-sample metadata\npd.concat([m.samples for m in executor._manifests])\n\n\nProfile without tar access\nAggregates + parquet give full statistical profile\n\n\nQuery-driven subsets\nexecutor.query(where=...) returns SampleLocation list",
    "crumbs": [
      "Examples",
      "Dataset Profiler"
    ]
  },
  {
    "objectID": "examples/index.html",
    "href": "examples/index.html",
    "title": "Examples",
    "section": "",
    "text": "These examples are self-contained demonstrations of atdata’s key features. Each one builds a working pipeline from scratch so you can see real code paths end-to-end.\n\n\n\nExample\nWhat It Shows\n\n\n\n\nTyped Dataset Pipeline\nDefine a sample type, write shards, iterate with batching\n\n\nLens Transformations\nView datasets through different schemas without copying data\n\n\nManifest-Powered Queries\nBuild per-shard manifests and query samples by metadata\n\n\nIndex-Managed Datasets\nUse Index with LocalDiskStore for managed dataset storage\n\n\nMulti-Split Datasets\nWork with train/test splits via DatasetDict and load_dataset\n\n\nDataset Profiler\nProfile datasets visually using manifest aggregates and matplotlib\n\n\nLens Graph Explorer\nVisualize lens transformation networks with Mermaid diagrams\n\n\nQuery Cookbook\nPractical query recipes with the typed proxy DSL and scatter plots\n\n\n\n\n\n\n\n\n\nNew to atdata?\n\n\n\nStart with the Quick Start tutorial for foundational concepts, then come back here for deeper worked examples.",
    "crumbs": [
      "Examples",
      "Examples"
    ]
  },
  {
    "objectID": "examples/query-cookbook.html",
    "href": "examples/query-cookbook.html",
    "title": "Query Cookbook",
    "section": "",
    "text": "This cookbook collects practical query patterns for manifest-powered datasets. Each recipe shows both the lambda syntax (pandas-native) and the typed proxy DSL (F expressions) side by side, so you can pick whichever reads best for your use case.",
    "crumbs": [
      "Examples",
      "Query Cookbook"
    ]
  },
  {
    "objectID": "examples/query-cookbook.html#build-the-example-dataset",
    "href": "examples/query-cookbook.html#build-the-example-dataset",
    "title": "Query Cookbook",
    "section": "1 — Build the example dataset",
    "text": "1 — Build the example dataset\nWe model a wildlife observation log with species, weight, maturity, and habitat tags. Weight distributions vary by species to make queries interesting.\n\nimport numpy as np\nfrom numpy.typing import NDArray\nfrom typing import Annotated\nimport atdata\nfrom atdata import ManifestField\n\n\n@atdata.packable\nclass Observation:\n    \"\"\"A wildlife observation with queryable metadata.\"\"\"\n\n    embedding: NDArray\n    species: Annotated[str, ManifestField(\"categorical\")]\n    weight_kg: Annotated[float, ManifestField(\"numeric\")]\n    is_adult: Annotated[bool, ManifestField(\"categorical\")]\n    habitats: Annotated[list[str], ManifestField(\"set\")]\n\n\nimport tempfile\nfrom pathlib import Path\n\ntmpdir = Path(tempfile.mkdtemp(prefix=\"atdata_cookbook_\"))\nrng = np.random.default_rng(2024)\n\nspecies_weights = {\n    \"elephant\": (3500, 800),\n    \"bear\": (250, 80),\n    \"wolf\": (40, 12),\n    \"eagle\": (5, 1.5),\n    \"salmon\": (8, 3),\n    \"mouse\": (0.03, 0.01),\n}\nhabitat_pool = [\"forest\", \"desert\", \"ocean\", \"mountain\", \"tundra\", \"grassland\"]\n\nsamples = []\nfor _ in range(800):\n    sp = rng.choice(list(species_weights.keys()))\n    mean, std = species_weights[sp]\n    samples.append(Observation(\n        embedding=rng.standard_normal(32).astype(np.float32),\n        species=sp,\n        weight_kg=round(max(0.01, float(rng.normal(mean, std))), 2),\n        is_adult=bool(rng.random() &gt; 0.3),\n        habitats=list(rng.choice(habitat_pool, size=rng.integers(1, 4), replace=False)),\n    ))\n\nds = atdata.write_samples(samples, tmpdir / \"wildlife.tar\", maxcount=200, manifest=True)\nprint(f\"Wrote {len(samples)} observations across {len(ds.list_shards())} shards\")\n\n# writing /var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_cookbook_yfgbs7x7/wildlife-000000.tar 0 0.0 GB 0\n# writing /var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_cookbook_yfgbs7x7/wildlife-000001.tar 200 0.0 GB 200\n# writing /var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_cookbook_yfgbs7x7/wildlife-000002.tar 200 0.0 GB 400\n# writing /var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_cookbook_yfgbs7x7/wildlife-000003.tar 200 0.0 GB 600\nWrote 800 observations across 4 shards",
    "crumbs": [
      "Examples",
      "Query Cookbook"
    ]
  },
  {
    "objectID": "examples/query-cookbook.html#load-manifests-and-preview",
    "href": "examples/query-cookbook.html#load-manifests-and-preview",
    "title": "Query Cookbook",
    "section": "2 — Load manifests and preview",
    "text": "2 — Load manifests and preview\n\nimport pandas as pd\nfrom atdata import QueryExecutor\nfrom atdata.manifest import F\n\nexecutor = QueryExecutor.from_directory(tmpdir)\ncombined = pd.concat(\n    [m.samples for m in executor._manifests if not m.samples.empty],\n    ignore_index=True,\n)\n\nprint(combined[[\"species\", \"weight_kg\", \"is_adult\"]].describe().to_string())\nprint()\nprint(f\"Species: {sorted(combined['species'].unique())}\")\n\n         weight_kg\ncount   600.000000\nmean    616.406983\nstd    1322.182118\nmin       0.010000\n25%       4.195000\n50%      11.790000\n75%     257.692500\nmax    5921.580000\n\nSpecies: ['bear', 'eagle', 'elephant', 'mouse', 'salmon', 'wolf']",
    "crumbs": [
      "Examples",
      "Query Cookbook"
    ]
  },
  {
    "objectID": "examples/query-cookbook.html#recipe-threshold-filter",
    "href": "examples/query-cookbook.html#recipe-threshold-filter",
    "title": "Query Cookbook",
    "section": "3 — Recipe: threshold filter",
    "text": "3 — Recipe: threshold filter\nFind heavy animals above a weight threshold.\n\nLambdaTyped DSL\n\n\n\nresults_lambda = executor.query(\n    where=lambda df: df[\"weight_kg\"] &gt; 500\n)\nprint(f\"Matches (lambda): {len(results_lambda)}\")\n\nMatches (lambda): 95\n\n\n\n\n\nresults_dsl = executor.query(where=F.weight_kg &gt; 500)\nprint(f\"Matches (DSL):    {len(results_dsl)}\")\n\nMatches (DSL):    95\n\n\n\n\n\n\nheavy_keys = {loc.key for loc in results_dsl}\nheavy_df = combined[combined[\"__key__\"].isin(heavy_keys)]\nprint(\"\\nSpecies breakdown of heavy animals (&gt;500 kg):\")\nprint(heavy_df[\"species\"].value_counts().to_string())\n\n\nSpecies breakdown of heavy animals (&gt;500 kg):\nspecies\nelephant    95",
    "crumbs": [
      "Examples",
      "Query Cookbook"
    ]
  },
  {
    "objectID": "examples/query-cookbook.html#recipe-categorical-selection",
    "href": "examples/query-cookbook.html#recipe-categorical-selection",
    "title": "Query Cookbook",
    "section": "4 — Recipe: categorical selection",
    "text": "4 — Recipe: categorical selection\n\nSingle valueMultiple values (.isin)Exclusion (~)\n\n\n\nbears = executor.query(where=F.species == \"bear\")\nprint(f\"Bears: {len(bears)}\")\n\nBears: 112\n\n\n\n\n\nlarge_animals = executor.query(where=F.species.isin([\"elephant\", \"bear\"]))\nprint(f\"Elephants + bears: {len(large_animals)}\")\n\nElephants + bears: 207\n\n\n\n\n\nnot_small = executor.query(where=~F.species.isin([\"mouse\", \"salmon\"]))\nprint(f\"Excluding mouse & salmon: {len(not_small)}\")\n\nExcluding mouse & salmon: 398",
    "crumbs": [
      "Examples",
      "Query Cookbook"
    ]
  },
  {
    "objectID": "examples/query-cookbook.html#recipe-range-filter",
    "href": "examples/query-cookbook.html#recipe-range-filter",
    "title": "Query Cookbook",
    "section": "5 — Recipe: range filter",
    "text": "5 — Recipe: range filter\nThe .between() method creates a closed-interval filter.\n\nLambdaTyped DSL\n\n\n\nmid_range = executor.query(\n    where=lambda df: df[\"weight_kg\"].between(10, 100)\n)\nprint(f\"Weight 10-100 kg (lambda): {len(mid_range)}\")\n\nWeight 10-100 kg (lambda): 112\n\n\n\n\n\nmid_range_dsl = executor.query(where=F.weight_kg.between(10, 100))\nprint(f\"Weight 10-100 kg (DSL):    {len(mid_range_dsl)}\")\n\nWeight 10-100 kg (DSL):    112",
    "crumbs": [
      "Examples",
      "Query Cookbook"
    ]
  },
  {
    "objectID": "examples/query-cookbook.html#recipe-tagset-membership",
    "href": "examples/query-cookbook.html#recipe-tagset-membership",
    "title": "Query Cookbook",
    "section": "6 — Recipe: tag/set membership",
    "text": "6 — Recipe: tag/set membership\nTags are stored as lists. With the lambda syntax you can use .apply() for arbitrary list predicates.\n\n# Observations in mountain habitats\nmountain = executor.query(\n    where=lambda df: df[\"habitats\"].apply(\n        lambda h: \"mountain\" in h if isinstance(h, list) else False\n    )\n)\nprint(f\"Mountain habitat: {len(mountain)} observations\")\n\nMountain habitat: 204 observations\n\n\n\n# Observations in BOTH forest AND mountain\ndual_habitat = executor.query(\n    where=lambda df: df[\"habitats\"].apply(\n        lambda h: {\"forest\", \"mountain\"}.issubset(h) if isinstance(h, list) else False\n    )\n)\nprint(f\"Forest + mountain: {len(dual_habitat)} observations\")\n\nForest + mountain: 56 observations",
    "crumbs": [
      "Examples",
      "Query Cookbook"
    ]
  },
  {
    "objectID": "examples/query-cookbook.html#recipe-compound-queries",
    "href": "examples/query-cookbook.html#recipe-compound-queries",
    "title": "Query Cookbook",
    "section": "7 — Recipe: compound queries",
    "text": "7 — Recipe: compound queries\nCombine predicates with & (AND), | (OR), and ~ (NOT).\n\nANDORNOT\n\n\n\nadult_bears = executor.query(\n    where=(F.species == \"bear\") & (F.is_adult == True)\n)\nprint(f\"Adult bears: {len(adult_bears)}\")\n\nAdult bears: 73\n\n\n\n\n\nextreme = executor.query(\n    where=(F.species == \"elephant\") | (F.weight_kg &lt; 0.1)\n)\nprint(f\"Elephants or very light (&lt;0.1 kg): {len(extreme)}\")\n\nElephants or very light (&lt;0.1 kg): 202\n\n\n\n\n\njuveniles = executor.query(where=~(F.is_adult == True))\nprint(f\"Juveniles: {len(juveniles)}\")\n\nJuveniles: 181",
    "crumbs": [
      "Examples",
      "Query Cookbook"
    ]
  },
  {
    "objectID": "examples/query-cookbook.html#scatter-plot-query-results-highlighted",
    "href": "examples/query-cookbook.html#scatter-plot-query-results-highlighted",
    "title": "Query Cookbook",
    "section": "8 — Scatter plot: query results highlighted",
    "text": "8 — Scatter plot: query results highlighted\n\nimport matplotlib\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\n\nquery_pred = (F.species.isin([\"elephant\", \"bear\"])) & (F.is_adult == True)\nmatch_locs = executor.query(where=query_pred)\nmatch_keys = {loc.key for loc in match_locs}\n\nspecies_list = sorted(combined[\"species\"].unique())\ncmap = plt.cm.Set2\nspecies_colors = {sp: cmap(i / len(species_list)) for i, sp in enumerate(species_list)}\n\nfig, ax = plt.subplots(figsize=(8, 4))\n\nfor sp in species_list:\n    mask = combined[\"species\"] == sp\n    subset = combined[mask]\n    ax.scatter(\n        subset.index, subset[\"weight_kg\"],\n        c=[species_colors[sp]], label=sp,\n        s=15, alpha=0.6, linewidths=0,\n    )\n\n# Highlight matches\nmatch_mask = combined[\"__key__\"].isin(match_keys)\nax.scatter(\n    combined[match_mask].index,\n    combined[match_mask][\"weight_kg\"],\n    facecolors=\"none\", edgecolors=\"#D32F2F\", s=50, linewidths=1.5,\n    label=\"query match\", zorder=5,\n)\n\nax.set_xlabel(\"Sample index\")\nax.set_ylabel(\"Weight (kg)\")\nax.set_yscale(\"log\")\nax.set_title(\"Adult elephants & bears highlighted\", fontweight=\"bold\")\nax.legend(fontsize=8, ncol=4, loc=\"upper center\", bbox_to_anchor=(0.5, -0.15))\nax.spines[[\"top\", \"right\"]].set_visible(False)\nplt.tight_layout()\nplt.show()\n\n/var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/ipykernel_23348/4089515773.py:40: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  plt.show()",
    "crumbs": [
      "Examples",
      "Query Cookbook"
    ]
  },
  {
    "objectID": "examples/query-cookbook.html#query-hit-rate-per-shard",
    "href": "examples/query-cookbook.html#query-hit-rate-per-shard",
    "title": "Query Cookbook",
    "section": "9 — Query hit rate per shard",
    "text": "9 — Query hit rate per shard\nThis chart shows how matches distribute across shards. When aggregate pruning is active, shards with zero hits can be skipped entirely.\n\nfrom collections import Counter\n\nshard_hits = Counter(loc.shard for loc in match_locs)\n\nshards_all = sorted({m.shard_id for m in executor._manifests})\nhit_counts = [shard_hits.get(s, 0) for s in shards_all]\nshard_labels = [Path(s).stem for s in shards_all]\n\nfig, ax = plt.subplots(figsize=(6, 3))\ncolors = [\"#43A047\" if c &gt; 0 else \"#BDBDBD\" for c in hit_counts]\nax.bar(shard_labels, hit_counts, color=colors, edgecolor=\"white\")\n\nfor i, count in enumerate(hit_counts):\n    if count &gt; 0:\n        ax.text(i, count + 0.5, str(count), ha=\"center\", fontweight=\"bold\", fontsize=10)\n\nax.set_ylabel(\"Matching samples\")\nax.set_title(\"Query Hits per Shard\", fontweight=\"bold\")\nax.spines[[\"top\", \"right\"]].set_visible(False)\nplt.tight_layout()\nplt.show()\n\n/var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/ipykernel_23348/1217293230.py:21: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  plt.show()\n\n\n\n\n\n\n\n\nShard pruning\n\n\n\nIn production, QueryExecutor checks each shard’s aggregate statistics before loading its parquet file. If the aggregates prove no samples can match (e.g., a shard’s max weight is below your threshold), the parquet is never read—saving I/O on large datasets.",
    "crumbs": [
      "Examples",
      "Query Cookbook"
    ]
  },
  {
    "objectID": "examples/query-cookbook.html#building-a-filtered-subset",
    "href": "examples/query-cookbook.html#building-a-filtered-subset",
    "title": "Query Cookbook",
    "section": "10 — Building a filtered subset",
    "text": "10 — Building a filtered subset\nQuery results are SampleLocation objects with shard path, key, and byte offset. Group them by shard for downstream processing.\n\nfrom itertools import groupby\nfrom operator import attrgetter\n\nsorted_locs = sorted(match_locs, key=attrgetter(\"shard\"))\nfor shard, group in groupby(sorted_locs, key=attrgetter(\"shard\")):\n    keys = [loc.key for loc in group]\n    print(f\"  {Path(shard).stem}: {len(keys)} matches (first 3: {keys[:3]})\")\n\n  wildlife: 49 matches (first 3: ['fa3119bc-0173-11f1-8000-000000000000', 'fa312560-0173-11f1-8000-000000000000', 'fa312af6-0173-11f1-8000-000000000000'])\n  wildlife-000000: 39 matches (first 3: ['fa328b4e-0173-11f1-8000-000000000000', 'fa328ca2-0173-11f1-8000-000000000000', 'fa328df6-0173-11f1-8000-000000000000'])\n  wildlife-000001: 52 matches (first 3: ['fa33c22a-0173-11f1-8000-000000000000', 'fa33cfc2-0173-11f1-8000-000000000000', 'fa33d814-0173-11f1-8000-000000000000'])",
    "crumbs": [
      "Examples",
      "Query Cookbook"
    ]
  },
  {
    "objectID": "examples/query-cookbook.html#clean-up",
    "href": "examples/query-cookbook.html#clean-up",
    "title": "Query Cookbook",
    "section": "11 — Clean up",
    "text": "11 — Clean up\n\nimport shutil\n\nshutil.rmtree(tmpdir, ignore_errors=True)",
    "crumbs": [
      "Examples",
      "Query Cookbook"
    ]
  },
  {
    "objectID": "examples/query-cookbook.html#key-takeaways",
    "href": "examples/query-cookbook.html#key-takeaways",
    "title": "Query Cookbook",
    "section": "Key takeaways",
    "text": "Key takeaways\n\n\n\n\n\n\n\n\nPattern\nLambda\nTyped DSL\n\n\n\n\nThreshold\ndf[\"weight\"] &gt; 50\nF.weight &gt; 50\n\n\nEquality\ndf[\"species\"] == \"bear\"\nF.species == \"bear\"\n\n\nSet membership\ndf[\"species\"].isin([...])\nF.species.isin([...])\n\n\nRange\ndf[\"x\"].between(a, b)\nF.x.between(a, b)\n\n\nAND\n(cond1) & (cond2)\n(cond1) & (cond2)\n\n\nOR\n(cond1) \\| (cond2)\n(cond1) \\| (cond2)\n\n\nNOT\n~(condition)\n~(condition)\n\n\nTag contains\ndf[\"tags\"].apply(...)\n(use lambda)",
    "crumbs": [
      "Examples",
      "Query Cookbook"
    ]
  },
  {
    "objectID": "examples/lens-graph.html",
    "href": "examples/lens-graph.html",
    "title": "Lens Graph Explorer",
    "section": "",
    "text": "A lens maps one sample type to another while preserving a round-trip guarantee. When you chain several lenses together, you get a lens graph — a network of interconvertible types.\nThis example models a sensor data pipeline with four types and three lenses, then visualizes the transformation network and traces data through it.",
    "crumbs": [
      "Examples",
      "Lens Graph Explorer"
    ]
  },
  {
    "objectID": "examples/lens-graph.html#the-type-hierarchy",
    "href": "examples/lens-graph.html#the-type-hierarchy",
    "title": "Lens Graph Explorer",
    "section": "1 — The type hierarchy",
    "text": "1 — The type hierarchy\n\n\n\n\n\nflowchart LR\n    A[\"&lt;b&gt;RawSensor&lt;/b&gt;&lt;br/&gt;&lt;small&gt;voltage &middot; timestamp&lt;br/&gt;sensor_id &middot; location&lt;/small&gt;\"]\n    B[\"&lt;b&gt;Calibrated&lt;/b&gt;&lt;br/&gt;&lt;small&gt;celsius &middot; timestamp&lt;br/&gt;sensor_id &middot; location&lt;br/&gt;cal_factor&lt;/small&gt;\"]\n    C[\"&lt;b&gt;Features&lt;/b&gt;&lt;br/&gt;&lt;small&gt;celsius &middot; hour&lt;br/&gt;is_daytime &middot; sensor_id&lt;/small&gt;\"]\n    D[\"&lt;b&gt;Prediction&lt;/b&gt;&lt;br/&gt;&lt;small&gt;classification&lt;br/&gt;sensor_id&lt;/small&gt;\"]\n    A --&gt;|calibrate| B\n    B --&gt;|featurize| C\n    C --&gt;|classify| D\n    style A fill:#e1f5fe,stroke:#0288d1\n    style B fill:#fff9c4,stroke:#f9a825\n    style C fill:#f3e5f5,stroke:#7b1fa2\n    style D fill:#c8e6c9,stroke:#388e3c\n\n\n\n\n\n\nRaw voltage readings get calibrated into Celsius, then distilled into time-of-day features, and finally classified. Each arrow is a lens with a well-behaved round-trip.",
    "crumbs": [
      "Examples",
      "Lens Graph Explorer"
    ]
  },
  {
    "objectID": "examples/lens-graph.html#define-the-four-sample-types",
    "href": "examples/lens-graph.html#define-the-four-sample-types",
    "title": "Lens Graph Explorer",
    "section": "2 — Define the four sample types",
    "text": "2 — Define the four sample types\n\nimport numpy as np\nimport atdata\n\n\n@atdata.packable\nclass RawSensor:\n    \"\"\"Raw sensor observation.\"\"\"\n    voltage: float\n    timestamp: str\n    sensor_id: str\n    location: str\n\n\n@atdata.packable\nclass Calibrated:\n    \"\"\"Sensor reading after calibration to Celsius.\"\"\"\n    celsius: float\n    timestamp: str\n    sensor_id: str\n    location: str\n    cal_factor: float\n\n\n@atdata.packable\nclass Features:\n    \"\"\"Derived features for ML.\"\"\"\n    celsius: float\n    hour: int\n    is_daytime: bool\n    sensor_id: str\n\n\n@atdata.packable\nclass Prediction:\n    \"\"\"Classification output.\"\"\"\n    classification: str\n    sensor_id: str",
    "crumbs": [
      "Examples",
      "Lens Graph Explorer"
    ]
  },
  {
    "objectID": "examples/lens-graph.html#register-the-lens-chain",
    "href": "examples/lens-graph.html#register-the-lens-chain",
    "title": "Lens Graph Explorer",
    "section": "3 — Register the lens chain",
    "text": "3 — Register the lens chain\nEach lens defines a getter (forward transform) and a putter (reverse transform that preserves untouched fields).\n\nCAL_FACTOR = 0.489  # voltage-to-celsius constant\n\n\n@atdata.lens\ndef calibrate(raw: RawSensor) -&gt; Calibrated:\n    return Calibrated(\n        celsius=raw.voltage * CAL_FACTOR,\n        timestamp=raw.timestamp,\n        sensor_id=raw.sensor_id,\n        location=raw.location,\n        cal_factor=CAL_FACTOR,\n    )\n\n\n@calibrate.putter\ndef calibrate_put(view: Calibrated, source: RawSensor) -&gt; RawSensor:\n    return RawSensor(\n        voltage=view.celsius / view.cal_factor,\n        timestamp=view.timestamp,\n        sensor_id=view.sensor_id,\n        location=view.location,\n    )\n\n\n@atdata.lens\ndef featurize(cal: Calibrated) -&gt; Features:\n    hour = int(cal.timestamp.split(\"T\")[1].split(\":\")[0])\n    return Features(\n        celsius=cal.celsius,\n        hour=hour,\n        is_daytime=(6 &lt;= hour &lt; 20),\n        sensor_id=cal.sensor_id,\n    )\n\n\n@featurize.putter\ndef featurize_put(view: Features, source: Calibrated) -&gt; Calibrated:\n    return Calibrated(\n        celsius=view.celsius,\n        timestamp=source.timestamp,\n        sensor_id=view.sensor_id,\n        location=source.location,\n        cal_factor=source.cal_factor,\n    )\n\n\n@atdata.lens\ndef classify(feat: Features) -&gt; Prediction:\n    if feat.celsius &gt;= 35:\n        label = \"hot\"\n    elif feat.celsius &gt;= 15:\n        label = \"warm\"\n    else:\n        label = \"cold\"\n    return Prediction(classification=label, sensor_id=feat.sensor_id)\n\n\n@classify.putter\ndef classify_put(view: Prediction, source: Features) -&gt; Features:\n    return Features(\n        celsius=source.celsius,\n        hour=source.hour,\n        is_daytime=source.is_daytime,\n        sensor_id=view.sensor_id,\n    )\n\n\nprint(\"Registered 3 lenses: calibrate, featurize, classify\")\n\nRegistered 3 lenses: calibrate, featurize, classify",
    "crumbs": [
      "Examples",
      "Lens Graph Explorer"
    ]
  },
  {
    "objectID": "examples/lens-graph.html#trace-a-single-sample-through-the-chain",
    "href": "examples/lens-graph.html#trace-a-single-sample-through-the-chain",
    "title": "Lens Graph Explorer",
    "section": "4 — Trace a single sample through the chain",
    "text": "4 — Trace a single sample through the chain\n\nRaw -&gt; CalibratedCalibrated -&gt; FeaturesFeatures -&gt; Prediction\n\n\n\nraw = RawSensor(\n    voltage=72.5,\n    timestamp=\"2024-07-15T14:30:00Z\",\n    sensor_id=\"sensor_03\",\n    location=\"rooftop-north\",\n)\n\ncal = calibrate.get(raw)\n\nprint(\"INPUT  (RawSensor):\")\nprint(f\"  voltage    = {raw.voltage}\")\nprint(f\"  timestamp  = {raw.timestamp}\")\nprint(f\"  sensor_id  = {raw.sensor_id}\")\nprint(f\"  location   = {raw.location}\")\nprint()\nprint(\"OUTPUT (Calibrated):\")\nprint(f\"  celsius    = {cal.celsius:.2f}\")\nprint(f\"  timestamp  = {cal.timestamp}\")\nprint(f\"  sensor_id  = {cal.sensor_id}\")\nprint(f\"  location   = {cal.location}\")\nprint(f\"  cal_factor = {cal.cal_factor}\")\n\nINPUT  (RawSensor):\n  voltage    = 72.5\n  timestamp  = 2024-07-15T14:30:00Z\n  sensor_id  = sensor_03\n  location   = rooftop-north\n\nOUTPUT (Calibrated):\n  celsius    = 35.45\n  timestamp  = 2024-07-15T14:30:00Z\n  sensor_id  = sensor_03\n  location   = rooftop-north\n  cal_factor = 0.489\n\n\n\n\n\nfeat = featurize.get(cal)\n\nprint(\"INPUT  (Calibrated):\")\nprint(f\"  celsius    = {cal.celsius:.2f}\")\nprint(f\"  timestamp  = {cal.timestamp}\")\nprint()\nprint(\"OUTPUT (Features):\")\nprint(f\"  celsius    = {feat.celsius:.2f}\")\nprint(f\"  hour       = {feat.hour}\")\nprint(f\"  is_daytime = {feat.is_daytime}\")\nprint(f\"  sensor_id  = {feat.sensor_id}\")\n\nINPUT  (Calibrated):\n  celsius    = 35.45\n  timestamp  = 2024-07-15T14:30:00Z\n\nOUTPUT (Features):\n  celsius    = 35.45\n  hour       = 14\n  is_daytime = True\n  sensor_id  = sensor_03\n\n\n\n\n\npred = classify.get(feat)\n\nprint(\"INPUT  (Features):\")\nprint(f\"  celsius    = {feat.celsius:.2f}\")\nprint(f\"  hour       = {feat.hour}\")\nprint(f\"  is_daytime = {feat.is_daytime}\")\nprint()\nprint(\"OUTPUT (Prediction):\")\nprint(f\"  classification = {pred.classification}\")\nprint(f\"  sensor_id      = {pred.sensor_id}\")\n\nINPUT  (Features):\n  celsius    = 35.45\n  hour       = 14\n  is_daytime = True\n\nOUTPUT (Prediction):\n  classification = hot\n  sensor_id      = sensor_03",
    "crumbs": [
      "Examples",
      "Lens Graph Explorer"
    ]
  },
  {
    "objectID": "examples/lens-graph.html#batch-transformation-on-a-dataset",
    "href": "examples/lens-graph.html#batch-transformation-on-a-dataset",
    "title": "Lens Graph Explorer",
    "section": "5 — Batch transformation on a dataset",
    "text": "5 — Batch transformation on a dataset\nWrite a dataset of RawSensor readings and iterate as Calibrated using ds.as_type().\n\nimport tempfile\nfrom pathlib import Path\n\ntmpdir = Path(tempfile.mkdtemp(prefix=\"atdata_lens_graph_\"))\nrng = np.random.default_rng(42)\n\nraw_samples = [\n    RawSensor(\n        voltage=round(float(rng.uniform(20, 90)), 2),\n        timestamp=f\"2024-08-{10 + i % 20:02d}T{rng.integers(0, 24):02d}:{rng.integers(0, 60):02d}:00Z\",\n        sensor_id=f\"sensor_{i % 5:02d}\",\n        location=rng.choice([\"rooftop\", \"basement\", \"outdoor\", \"lab\"]),\n    )\n    for i in range(500)\n]\n\nds = atdata.write_samples(raw_samples, tmpdir / \"sensors.tar\", maxcount=250)\nprint(f\"Wrote {len(raw_samples)} RawSensor samples across {len(ds.list_shards())} shards\")\n\n# writing /var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_lens_graph__bva6ddu/sensors-000000.tar 0 0.0 GB 0\n# writing /var/folders/hx/9l078dds5z945qcv8j1hsnr00000gn/T/atdata_lens_graph__bva6ddu/sensors-000001.tar 250 0.0 GB 250\nWrote 500 RawSensor samples across 2 shards\n\n\n\nds_cal = ds.as_type(Calibrated)\n\nbatch = next(iter(ds_cal.ordered(batch_size=8)))\nprint(f\"Batch sample type: Calibrated\")\nprint(f\"  celsius values:    {[round(c, 2) for c in batch.celsius[:4]]} ...\")\nprint(f\"  cal_factor values: {batch.cal_factor[:4]} ...\")\nprint(f\"  sensor_ids:        {batch.sensor_id[:4]} ...\")\n\nBatch sample type: Calibrated\n  celsius values:    [36.27, 33.65, 43.17, 14.17] ...\n  cal_factor values: [0.489, 0.489, 0.489, 0.489] ...\n  sensor_ids:        ['sensor_00', 'sensor_01', 'sensor_02', 'sensor_03'] ...",
    "crumbs": [
      "Examples",
      "Lens Graph Explorer"
    ]
  },
  {
    "objectID": "examples/lens-graph.html#verify-lens-laws",
    "href": "examples/lens-graph.html#verify-lens-laws",
    "title": "Lens Graph Explorer",
    "section": "6 — Verify lens laws",
    "text": "6 — Verify lens laws\nWell-behaved lenses satisfy two laws:\n\n\n\n\n\nflowchart TB\n    subgraph gp [\"GetPut: put(get(s), s) == s\"]\n        direction LR\n        S1[\"source\"] --&gt;|get| V1[\"view\"]\n        V1 --&gt;|\"put(view, source)\"| S2[\"source'\"]\n    end\n    subgraph pg [\"PutGet: get(put(v, s)) == v\"]\n        direction LR\n        V2[\"view\"] --&gt;|\"put(view, source)\"| S3[\"source'\"]\n        S3 --&gt;|get| V3[\"view'\"]\n    end\n    style gp fill:#e8f5e9,stroke:#388e3c\n    style pg fill:#e3f2fd,stroke:#1565c0\n\n\n\n\n\n\n\n# GetPut: put(get(source), source) == source\nraw_roundtrip = calibrate.put(calibrate.get(raw), raw)\nassert raw_roundtrip.voltage == raw.voltage\nassert raw_roundtrip.timestamp == raw.timestamp\nassert raw_roundtrip.sensor_id == raw.sensor_id\nassert raw_roundtrip.location == raw.location\nprint(\"GetPut (calibrate): PASS\")\n\ncal_roundtrip = featurize.put(featurize.get(cal), cal)\nassert cal_roundtrip.celsius == cal.celsius\nassert cal_roundtrip.sensor_id == cal.sensor_id\nassert cal_roundtrip.location == cal.location\nprint(\"GetPut (featurize): PASS\")\n\nfeat_roundtrip = classify.put(classify.get(feat), feat)\nassert feat_roundtrip.celsius == feat.celsius\nassert feat_roundtrip.hour == feat.hour\nassert feat_roundtrip.is_daytime == feat.is_daytime\nprint(\"GetPut (classify):  PASS\")\n\nGetPut (calibrate): PASS\nGetPut (featurize): PASS\nGetPut (classify):  PASS\n\n\n\n# PutGet: get(put(view, source)) == view\nmodified_cal = Calibrated(\n    celsius=25.0, timestamp=raw.timestamp,\n    sensor_id=raw.sensor_id, location=raw.location, cal_factor=CAL_FACTOR,\n)\nassert calibrate.get(calibrate.put(modified_cal, raw)).celsius == modified_cal.celsius\nprint(\"PutGet (calibrate): PASS\")\n\nmodified_feat = Features(celsius=25.0, hour=14, is_daytime=True, sensor_id=\"sensor_03\")\nresult = featurize.get(featurize.put(modified_feat, cal))\nassert result.celsius == modified_feat.celsius\nassert result.sensor_id == modified_feat.sensor_id\nprint(\"PutGet (featurize): PASS\")\n\nmodified_pred = Prediction(classification=\"hot\", sensor_id=\"sensor_03\")\nassert classify.get(classify.put(modified_pred, feat)).classification == modified_pred.classification\nprint(\"PutGet (classify):  PASS\")\n\nPutGet (calibrate): PASS\nPutGet (featurize): PASS\nPutGet (classify):  PASS\n\n\n\n\n\n\n\n\nWhat are lens laws?\n\n\n\nGetPut guarantees that if you read a view and write it back unchanged, the source is unchanged. PutGet guarantees that if you write a view and read it back, you get the same view. Together they ensure lenses are information-preserving round-trips.\nNote that classify is a lossy lens: the putter preserves the source’s celsius value, so PutGet only holds when the classification is consistent with the source temperature. This is a common pattern for summary/derived lenses.",
    "crumbs": [
      "Examples",
      "Lens Graph Explorer"
    ]
  },
  {
    "objectID": "examples/lens-graph.html#inspect-the-lensnetwork",
    "href": "examples/lens-graph.html#inspect-the-lensnetwork",
    "title": "Lens Graph Explorer",
    "section": "7 — Inspect the LensNetwork",
    "text": "7 — Inspect the LensNetwork\nAll lenses registered with @atdata.lens are tracked in a global registry.\n\nnetwork = atdata.LensNetwork()\n\nprint(\"Registered lenses:\")\nfor (src, tgt), lens_obj in network._registry.items():\n    print(f\"  {src.__name__:15s} -&gt; {tgt.__name__}\")\n\nRegistered lenses:\n  DictSample      -&gt; RawSensor\n  DictSample      -&gt; Calibrated\n  DictSample      -&gt; Features\n  DictSample      -&gt; Prediction\n  RawSensor       -&gt; Calibrated\n  Calibrated      -&gt; Features\n  Features        -&gt; Prediction",
    "crumbs": [
      "Examples",
      "Lens Graph Explorer"
    ]
  },
  {
    "objectID": "examples/lens-graph.html#clean-up",
    "href": "examples/lens-graph.html#clean-up",
    "title": "Lens Graph Explorer",
    "section": "8 — Clean up",
    "text": "8 — Clean up\n\nimport shutil\n\nshutil.rmtree(tmpdir, ignore_errors=True)",
    "crumbs": [
      "Examples",
      "Lens Graph Explorer"
    ]
  },
  {
    "objectID": "examples/lens-graph.html#key-takeaways",
    "href": "examples/lens-graph.html#key-takeaways",
    "title": "Lens Graph Explorer",
    "section": "Key takeaways",
    "text": "Key takeaways\n\n\n\n\n\n\n\nConcept\nAPI\n\n\n\n\nDefine a forward transform\n@atdata.lens decorator on getter function\n\n\nDefine a reverse transform\n@my_lens.putter decorator\n\n\nApply lens to a single sample\nmy_lens.get(source) / my_lens.put(view, source)\n\n\nTransform an entire dataset\nds.as_type(TargetType)\n\n\nInspect registered lenses\nLensNetwork()._registry\n\n\nVerify round-trip correctness\nAssert GetPut and PutGet laws",
    "crumbs": [
      "Examples",
      "Lens Graph Explorer"
    ]
  },
  {
    "objectID": "api/packable.html",
    "href": "api/packable.html",
    "title": "packable",
    "section": "",
    "text": "packable(cls)\nConvert a class into a PackableSample dataclass with msgpack serialization.\nThe resulting class gains packed, as_wds, from_bytes, and from_data methods, and satisfies the Packable protocol. NDArray fields are automatically handled during serialization.\n\n\n&gt;&gt;&gt; @packable\n... class MyData:\n...     name: str\n...     values: NDArray\n...\n&gt;&gt;&gt; sample = MyData(name=\"test\", values=np.array([1, 2, 3]))\n&gt;&gt;&gt; restored = MyData.from_bytes(sample.packed)"
  },
  {
    "objectID": "api/packable.html#examples",
    "href": "api/packable.html#examples",
    "title": "packable",
    "section": "",
    "text": "&gt;&gt;&gt; @packable\n... class MyData:\n...     name: str\n...     values: NDArray\n...\n&gt;&gt;&gt; sample = MyData(name=\"test\", values=np.array([1, 2, 3]))\n&gt;&gt;&gt; restored = MyData.from_bytes(sample.packed)"
  },
  {
    "objectID": "api/Packable-protocol.html",
    "href": "api/Packable-protocol.html",
    "title": "Packable",
    "section": "",
    "text": "Packable()\nStructural protocol for packable sample types.\nThis protocol allows classes decorated with @packable to be recognized as valid types for lens transformations and schema operations, even though the decorator doesn’t change the class’s nominal type at static analysis time.\nBoth PackableSample subclasses and @packable-decorated classes satisfy this protocol structurally.\nThe protocol captures the full interface needed for: - Lens type transformations (as_wds, from_data) - Schema publishing (class introspection via dataclass fields) - Serialization/deserialization (packed, from_bytes)\n\n\n&gt;&gt;&gt; @packable\n... class MySample:\n...     name: str\n...     value: int\n...\n&gt;&gt;&gt; def process(sample_type: Type[Packable]) -&gt; None:\n...     # Type checker knows sample_type has from_bytes, packed, etc.\n...     instance = sample_type.from_bytes(data)\n...     print(instance.packed)"
  },
  {
    "objectID": "api/Packable-protocol.html#examples",
    "href": "api/Packable-protocol.html#examples",
    "title": "Packable",
    "section": "",
    "text": "&gt;&gt;&gt; @packable\n... class MySample:\n...     name: str\n...     value: int\n...\n&gt;&gt;&gt; def process(sample_type: Type[Packable]) -&gt; None:\n...     # Type checker knows sample_type has from_bytes, packed, etc.\n...     instance = sample_type.from_bytes(data)\n...     print(instance.packed)"
  },
  {
    "objectID": "api/AtUri.html",
    "href": "api/AtUri.html",
    "title": "AtUri",
    "section": "",
    "text": "atmosphere.AtUri(authority, collection, rkey)\nParsed AT Protocol URI.\nAT URIs follow the format: at:////\n\n\n&gt;&gt;&gt; uri = AtUri.parse(\"at://did:plc:abc123/ac.foundation.dataset.schema/xyz\")\n&gt;&gt;&gt; uri.authority\n'did:plc:abc123'\n&gt;&gt;&gt; uri.collection\n'ac.foundation.dataset.schema'\n&gt;&gt;&gt; uri.rkey\n'xyz'\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nauthority\nThe DID or handle of the repository owner.\n\n\ncollection\nThe NSID of the record collection.\n\n\nrkey\nThe record key within the collection.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nparse\nParse an AT URI string into components.\n\n\n\n\n\natmosphere.AtUri.parse(uri)\nParse an AT URI string into components.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nAT URI string in format at://&lt;authority&gt;/&lt;collection&gt;/&lt;rkey&gt;\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAtUri\nParsed AtUri instance.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the URI format is invalid."
  },
  {
    "objectID": "api/AtUri.html#examples",
    "href": "api/AtUri.html#examples",
    "title": "AtUri",
    "section": "",
    "text": "&gt;&gt;&gt; uri = AtUri.parse(\"at://did:plc:abc123/ac.foundation.dataset.schema/xyz\")\n&gt;&gt;&gt; uri.authority\n'did:plc:abc123'\n&gt;&gt;&gt; uri.collection\n'ac.foundation.dataset.schema'\n&gt;&gt;&gt; uri.rkey\n'xyz'"
  },
  {
    "objectID": "api/AtUri.html#attributes",
    "href": "api/AtUri.html#attributes",
    "title": "AtUri",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nauthority\nThe DID or handle of the repository owner.\n\n\ncollection\nThe NSID of the record collection.\n\n\nrkey\nThe record key within the collection."
  },
  {
    "objectID": "api/AtUri.html#methods",
    "href": "api/AtUri.html#methods",
    "title": "AtUri",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nparse\nParse an AT URI string into components.\n\n\n\n\n\natmosphere.AtUri.parse(uri)\nParse an AT URI string into components.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nAT URI string in format at://&lt;authority&gt;/&lt;collection&gt;/&lt;rkey&gt;\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAtUri\nParsed AtUri instance.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the URI format is invalid."
  },
  {
    "objectID": "api/local.S3DataStore.html",
    "href": "api/local.S3DataStore.html",
    "title": "local.S3DataStore",
    "section": "",
    "text": "local.S3DataStore(credentials, *, bucket)\nS3-compatible data store implementing AbstractDataStore protocol.\nHandles writing dataset shards to S3-compatible object storage and resolving URLs for reading.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ncredentials\n\nS3 credentials dictionary.\n\n\nbucket\n\nTarget bucket name.\n\n\n_fs\n\nS3FileSystem instance.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nread_url\nResolve an S3 URL for reading/streaming.\n\n\nsupports_streaming\nS3 supports streaming reads.\n\n\nwrite_shards\nWrite dataset shards to S3.\n\n\n\n\n\nlocal.S3DataStore.read_url(url)\nResolve an S3 URL for reading/streaming.\nFor S3-compatible stores with custom endpoints (like Cloudflare R2, MinIO, etc.), converts s3:// URLs to HTTPS URLs that WebDataset can stream directly.\nFor standard AWS S3 (no custom endpoint), URLs are returned unchanged since WebDataset’s built-in s3fs integration handles them.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nurl\nstr\nS3 URL to resolve (e.g., ‘s3://bucket/path/file.tar’).\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nHTTPS URL if custom endpoint is configured, otherwise unchanged.\n\n\nExample\nstr\n‘s3://bucket/path’ -&gt; ‘https://endpoint.com/bucket/path’\n\n\n\n\n\n\n\nlocal.S3DataStore.supports_streaming()\nS3 supports streaming reads.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nbool\nTrue.\n\n\n\n\n\n\n\nlocal.S3DataStore.write_shards(\n    ds,\n    *,\n    prefix,\n    cache_local=False,\n    manifest=False,\n    schema_version='1.0.0',\n    source_job_id=None,\n    parent_shards=None,\n    pipeline_version=None,\n    **kwargs,\n)\nWrite dataset shards to S3.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nds\nDataset\nThe Dataset to write.\nrequired\n\n\nprefix\nstr\nPath prefix within bucket (e.g., ‘datasets/mnist/v1’).\nrequired\n\n\ncache_local\nbool\nIf True, write locally first then copy to S3.\nFalse\n\n\nmanifest\nbool\nIf True, generate per-shard manifest files alongside each tar shard (.manifest.json + .manifest.parquet).\nFalse\n\n\nschema_version\nstr\nSchema version for manifest headers.\n'1.0.0'\n\n\nsource_job_id\nstr | None\nOptional provenance job identifier for manifests.\nNone\n\n\nparent_shards\nlist[str] | None\nOptional list of input shard identifiers for provenance.\nNone\n\n\npipeline_version\nstr | None\nOptional pipeline version string for provenance.\nNone\n\n\n**kwargs\n\nAdditional args passed to wds.ShardWriter (e.g., maxcount).\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[str]\nList of S3 URLs for the written shards.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nRuntimeError\nIf no shards were written."
  },
  {
    "objectID": "api/local.S3DataStore.html#attributes",
    "href": "api/local.S3DataStore.html#attributes",
    "title": "local.S3DataStore",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ncredentials\n\nS3 credentials dictionary.\n\n\nbucket\n\nTarget bucket name.\n\n\n_fs\n\nS3FileSystem instance."
  },
  {
    "objectID": "api/local.S3DataStore.html#methods",
    "href": "api/local.S3DataStore.html#methods",
    "title": "local.S3DataStore",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nread_url\nResolve an S3 URL for reading/streaming.\n\n\nsupports_streaming\nS3 supports streaming reads.\n\n\nwrite_shards\nWrite dataset shards to S3.\n\n\n\n\n\nlocal.S3DataStore.read_url(url)\nResolve an S3 URL for reading/streaming.\nFor S3-compatible stores with custom endpoints (like Cloudflare R2, MinIO, etc.), converts s3:// URLs to HTTPS URLs that WebDataset can stream directly.\nFor standard AWS S3 (no custom endpoint), URLs are returned unchanged since WebDataset’s built-in s3fs integration handles them.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nurl\nstr\nS3 URL to resolve (e.g., ‘s3://bucket/path/file.tar’).\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nHTTPS URL if custom endpoint is configured, otherwise unchanged.\n\n\nExample\nstr\n‘s3://bucket/path’ -&gt; ‘https://endpoint.com/bucket/path’\n\n\n\n\n\n\n\nlocal.S3DataStore.supports_streaming()\nS3 supports streaming reads.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nbool\nTrue.\n\n\n\n\n\n\n\nlocal.S3DataStore.write_shards(\n    ds,\n    *,\n    prefix,\n    cache_local=False,\n    manifest=False,\n    schema_version='1.0.0',\n    source_job_id=None,\n    parent_shards=None,\n    pipeline_version=None,\n    **kwargs,\n)\nWrite dataset shards to S3.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nds\nDataset\nThe Dataset to write.\nrequired\n\n\nprefix\nstr\nPath prefix within bucket (e.g., ‘datasets/mnist/v1’).\nrequired\n\n\ncache_local\nbool\nIf True, write locally first then copy to S3.\nFalse\n\n\nmanifest\nbool\nIf True, generate per-shard manifest files alongside each tar shard (.manifest.json + .manifest.parquet).\nFalse\n\n\nschema_version\nstr\nSchema version for manifest headers.\n'1.0.0'\n\n\nsource_job_id\nstr | None\nOptional provenance job identifier for manifests.\nNone\n\n\nparent_shards\nlist[str] | None\nOptional list of input shard identifiers for provenance.\nNone\n\n\npipeline_version\nstr | None\nOptional pipeline version string for provenance.\nNone\n\n\n**kwargs\n\nAdditional args passed to wds.ShardWriter (e.g., maxcount).\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[str]\nList of S3 URLs for the written shards.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nRuntimeError\nIf no shards were written."
  },
  {
    "objectID": "api/AbstractDataStore.html",
    "href": "api/AbstractDataStore.html",
    "title": "AbstractDataStore",
    "section": "",
    "text": "AbstractDataStore()\nProtocol for data storage backends (S3, local disk, PDS blobs).\nSeparates index (metadata) from data store (shard files), enabling flexible deployment combinations.\n\n\n&gt;&gt;&gt; store = S3DataStore(credentials, bucket=\"my-bucket\")\n&gt;&gt;&gt; urls = store.write_shards(dataset, prefix=\"training/v1\")\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nread_url\nResolve a storage URL for reading (e.g., sign S3 URLs).\n\n\nwrite_shards\nWrite dataset shards to storage.\n\n\n\n\n\nAbstractDataStore.read_url(url)\nResolve a storage URL for reading (e.g., sign S3 URLs).\n\n\n\nAbstractDataStore.write_shards(ds, *, prefix, **kwargs)\nWrite dataset shards to storage.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nds\nDataset\nThe Dataset to write.\nrequired\n\n\nprefix\nstr\nPath prefix (e.g., 'datasets/mnist/v1').\nrequired\n\n\n**kwargs\n\nBackend-specific options (maxcount, maxsize, etc.).\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[str]\nList of shard URLs suitable for atdata.Dataset()."
  },
  {
    "objectID": "api/AbstractDataStore.html#examples",
    "href": "api/AbstractDataStore.html#examples",
    "title": "AbstractDataStore",
    "section": "",
    "text": "&gt;&gt;&gt; store = S3DataStore(credentials, bucket=\"my-bucket\")\n&gt;&gt;&gt; urls = store.write_shards(dataset, prefix=\"training/v1\")"
  },
  {
    "objectID": "api/AbstractDataStore.html#methods",
    "href": "api/AbstractDataStore.html#methods",
    "title": "AbstractDataStore",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nread_url\nResolve a storage URL for reading (e.g., sign S3 URLs).\n\n\nwrite_shards\nWrite dataset shards to storage.\n\n\n\n\n\nAbstractDataStore.read_url(url)\nResolve a storage URL for reading (e.g., sign S3 URLs).\n\n\n\nAbstractDataStore.write_shards(ds, *, prefix, **kwargs)\nWrite dataset shards to storage.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nds\nDataset\nThe Dataset to write.\nrequired\n\n\nprefix\nstr\nPath prefix (e.g., 'datasets/mnist/v1').\nrequired\n\n\n**kwargs\n\nBackend-specific options (maxcount, maxsize, etc.).\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[str]\nList of shard URLs suitable for atdata.Dataset()."
  },
  {
    "objectID": "api/Dataset.html",
    "href": "api/Dataset.html",
    "title": "Dataset",
    "section": "",
    "text": "Dataset(source=None, metadata_url=None, *, url=None)\nA typed dataset built on WebDataset with lens transformations.\nThis class wraps WebDataset tar archives and provides type-safe iteration over samples of a specific PackableSample type. Samples are stored as msgpack-serialized data within WebDataset shards.\nThe dataset supports: - Ordered and shuffled iteration - Automatic batching with SampleBatch - Type transformations via the lens system (as_type()) - Export to parquet format\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nST\n\nThe sample type for this dataset, must derive from PackableSample.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nurl\n\nWebDataset brace-notation URL for the tar file(s).\n\n\n\n\n\n\n&gt;&gt;&gt; ds = Dataset[MyData](\"path/to/data-{000000..000009}.tar\")\n&gt;&gt;&gt; for sample in ds.ordered(batch_size=32):\n...     # sample is SampleBatch[MyData] with batch_size samples\n...     embeddings = sample.embeddings  # shape: (32, ...)\n...\n&gt;&gt;&gt; # Transform to a different view\n&gt;&gt;&gt; ds_view = ds.as_type(MyDataView)\n\n\n\nThis class uses Python’s __orig_class__ mechanism to extract the type parameter at runtime. Instances must be created using the subscripted syntax Dataset[MyType](url) rather than calling the constructor directly with an unsubscripted class.\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nas_type\nView this dataset through a different sample type via a registered lens.\n\n\ndescribe\nSummary statistics: sample_type, fields, num_shards, shards, url, metadata.\n\n\nfilter\nReturn a new dataset that yields only samples matching predicate.\n\n\nget\nRetrieve a single sample by its __key__.\n\n\nhead\nReturn the first n samples from the dataset.\n\n\nlist_shards\nReturn all shard paths/URLs as a list.\n\n\nmap\nReturn a new dataset that applies fn to each sample during iteration.\n\n\nordered\nIterate over the dataset in order.\n\n\nprocess_shards\nProcess each shard independently, collecting per-shard results.\n\n\nquery\nQuery this dataset using per-shard manifest metadata.\n\n\nselect\nReturn samples at the given integer indices.\n\n\nshuffled\nIterate over the dataset in random order.\n\n\nto_dict\nMaterialize the dataset as a column-oriented dictionary.\n\n\nto_pandas\nMaterialize the dataset (or first limit samples) as a DataFrame.\n\n\nto_parquet\nExport dataset to parquet file(s).\n\n\nwrap\nDeserialize a raw WDS sample dict into type ST.\n\n\nwrap_batch\nDeserialize a raw WDS batch dict into SampleBatch[ST].\n\n\n\n\n\nDataset.as_type(other)\nView this dataset through a different sample type via a registered lens.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf no lens exists between the current and target types.\n\n\n\n\n\n\n\nDataset.describe()\nSummary statistics: sample_type, fields, num_shards, shards, url, metadata.\n\n\n\nDataset.filter(predicate)\nReturn a new dataset that yields only samples matching predicate.\nThe filter is applied lazily during iteration — no data is copied.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npredicate\nCallable[[ST], bool]\nA function that takes a sample and returns True to keep it or False to discard it.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset[ST]\nA new Dataset whose iterators apply the filter.\n\n\n\n\n\n\n&gt;&gt;&gt; long_names = ds.filter(lambda s: len(s.name) &gt; 10)\n&gt;&gt;&gt; for sample in long_names:\n...     assert len(sample.name) &gt; 10\n\n\n\n\nDataset.get(key)\nRetrieve a single sample by its __key__.\nScans shards sequentially until a sample with a matching key is found. This is O(n) for streaming datasets.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkey\nstr\nThe WebDataset __key__ string to search for.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nST\nThe matching sample.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nSampleKeyError\nIf no sample with the given key exists.\n\n\n\n\n\n\n&gt;&gt;&gt; sample = ds.get(\"00000001-0001-1000-8000-010000000000\")\n\n\n\n\nDataset.head(n=5)\nReturn the first n samples from the dataset.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint\nNumber of samples to return. Default: 5.\n5\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[ST]\nList of up to n samples in shard order.\n\n\n\n\n\n\n&gt;&gt;&gt; samples = ds.head(3)\n&gt;&gt;&gt; len(samples)\n3\n\n\n\n\nDataset.list_shards()\nReturn all shard paths/URLs as a list.\n\n\n\nDataset.map(fn)\nReturn a new dataset that applies fn to each sample during iteration.\nThe mapping is applied lazily during iteration — no data is copied.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfn\nCallable[[ST], Any]\nA function that takes a sample of type ST and returns a transformed value.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\nA new Dataset whose iterators apply the mapping.\n\n\n\n\n\n\n&gt;&gt;&gt; names = ds.map(lambda s: s.name)\n&gt;&gt;&gt; for name in names:\n...     print(name)\n\n\n\n\nDataset.ordered(batch_size=None)\nIterate over the dataset in order.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbatch_size\nint | None\nThe size of iterated batches. Default: None (unbatched). If None, iterates over one sample at a time with no batch dimension.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nIterable[ST] | Iterable[SampleBatch[ST]]\nA data pipeline that iterates over the dataset in its original\n\n\n\nIterable[ST] | Iterable[SampleBatch[ST]]\nsample order. When batch_size is None, yields individual\n\n\n\nIterable[ST] | Iterable[SampleBatch[ST]]\nsamples of type ST. When batch_size is an integer, yields\n\n\n\nIterable[ST] | Iterable[SampleBatch[ST]]\nSampleBatch[ST] instances containing that many samples.\n\n\n\n\n\n\n&gt;&gt;&gt; for sample in ds.ordered():\n...     process(sample)  # sample is ST\n&gt;&gt;&gt; for batch in ds.ordered(batch_size=32):\n...     process(batch)  # batch is SampleBatch[ST]\n\n\n\n\nDataset.process_shards(fn, *, shards=None)\nProcess each shard independently, collecting per-shard results.\nUnlike :meth:map (which is lazy and per-sample), this method eagerly processes each shard in turn, calling fn with the full list of samples from that shard. If some shards fail, raises :class:~atdata._exceptions.PartialFailureError containing both the successful results and the per-shard errors.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfn\nCallable[[list[ST]], Any]\nFunction receiving a list of samples from one shard and returning an arbitrary result.\nrequired\n\n\nshards\nlist[str] | None\nOptional list of shard identifiers to process. If None, processes all shards in the dataset. Useful for retrying only the failed shards from a previous PartialFailureError.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict[str, Any]\nDict mapping shard identifier to fn’s return value for each shard.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nPartialFailureError\nIf at least one shard fails. The exception carries .succeeded_shards, .failed_shards, .errors, and .results for inspection and retry.\n\n\n\n\n\n\n&gt;&gt;&gt; results = ds.process_shards(lambda samples: len(samples))\n&gt;&gt;&gt; # On partial failure, retry just the failed shards:\n&gt;&gt;&gt; try:\n...     results = ds.process_shards(expensive_fn)\n... except PartialFailureError as e:\n...     retry = ds.process_shards(expensive_fn, shards=e.failed_shards)\n\n\n\n\nDataset.query(where)\nQuery this dataset using per-shard manifest metadata.\nRequires manifests to have been generated during shard writing. Discovers manifest files alongside the tar shards, loads them, and executes a two-phase query (shard-level aggregate pruning, then sample-level parquet filtering).\nThe where argument accepts either a lambda/function that operates on a pandas DataFrame, or a Predicate built from the proxy DSL.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nCallable[[pd.DataFrame], pd.Series] | Predicate\nPredicate function or Predicate object that selects matching rows from the per-sample manifest DataFrame.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[SampleLocation]\nList of SampleLocation for matching samples.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nFileNotFoundError\nIf no manifest files are found alongside shards.\n\n\n\n\n\n\n&gt;&gt;&gt; locs = ds.query(where=lambda df: df[\"confidence\"] &gt; 0.9)\n&gt;&gt;&gt; len(locs)\n42\n&gt;&gt;&gt; Q = ds.fields\n&gt;&gt;&gt; locs = ds.query(where=(Q.confidence &gt; 0.9))\n\n\n\n\nDataset.select(indices)\nReturn samples at the given integer indices.\nIterates through the dataset in order and collects samples whose positional index matches. This is O(n) for streaming datasets.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nindices\nSequence[int]\nSequence of zero-based indices to select.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[ST]\nList of samples at the requested positions, in index order.\n\n\n\n\n\n\n&gt;&gt;&gt; samples = ds.select([0, 5, 10])\n&gt;&gt;&gt; len(samples)\n3\n\n\n\n\nDataset.shuffled(buffer_shards=100, buffer_samples=10000, batch_size=None)\nIterate over the dataset in random order.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbuffer_shards\nint\nNumber of shards to buffer for shuffling at the shard level. Larger values increase randomness but use more memory. Default: 100.\n100\n\n\nbuffer_samples\nint\nNumber of samples to buffer for shuffling within shards. Larger values increase randomness but use more memory. Default: 10,000.\n10000\n\n\nbatch_size\nint | None\nThe size of iterated batches. Default: None (unbatched). If None, iterates over one sample at a time with no batch dimension.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nIterable[ST] | Iterable[SampleBatch[ST]]\nA data pipeline that iterates over the dataset in randomized order.\n\n\n\nIterable[ST] | Iterable[SampleBatch[ST]]\nWhen batch_size is None, yields individual samples of type\n\n\n\nIterable[ST] | Iterable[SampleBatch[ST]]\nST. When batch_size is an integer, yields SampleBatch[ST]\n\n\n\nIterable[ST] | Iterable[SampleBatch[ST]]\ninstances containing that many samples.\n\n\n\n\n\n\n&gt;&gt;&gt; for sample in ds.shuffled():\n...     process(sample)  # sample is ST\n&gt;&gt;&gt; for batch in ds.shuffled(batch_size=32):\n...     process(batch)  # batch is SampleBatch[ST]\n\n\n\n\nDataset.to_dict(limit=None)\nMaterialize the dataset as a column-oriented dictionary.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlimit\nint | None\nMaximum number of samples to include. None means all.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict[str, list[Any]]\nDictionary mapping field names to lists of values (one entry\n\n\n\ndict[str, list[Any]]\nper sample).\n\n\n\n\n\n\nWith limit=None this loads the entire dataset into memory.\n\n\n\n&gt;&gt;&gt; d = ds.to_dict(limit=10)\n&gt;&gt;&gt; d.keys()\ndict_keys(['name', 'embedding'])\n&gt;&gt;&gt; len(d['name'])\n10\n\n\n\n\nDataset.to_pandas(limit=None)\nMaterialize the dataset (or first limit samples) as a DataFrame.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlimit\nint | None\nMaximum number of samples to include. None means all samples (may use significant memory for large datasets).\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\nA pandas DataFrame with one row per sample and columns matching\n\n\n\npd.DataFrame\nthe sample fields.\n\n\n\n\n\n\nWith limit=None this loads the entire dataset into memory.\n\n\n\n&gt;&gt;&gt; df = ds.to_pandas(limit=100)\n&gt;&gt;&gt; df.columns.tolist()\n['name', 'embedding']\n\n\n\n\nDataset.to_parquet(path, sample_map=None, maxcount=None, **kwargs)\nExport dataset to parquet file(s).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nPathlike\nOutput path. With maxcount, files are named {stem}-{segment:06d}.parquet.\nrequired\n\n\nsample_map\nOptional[SampleExportMap]\nConvert sample to dict. Defaults to dataclasses.asdict.\nNone\n\n\nmaxcount\nOptional[int]\nSplit into files of at most this many samples. Without it, the entire dataset is loaded into memory.\nNone\n\n\n**kwargs\n\nPassed to pandas.DataFrame.to_parquet().\n{}\n\n\n\n\n\n\n&gt;&gt;&gt; ds.to_parquet(\"output.parquet\", maxcount=50000)\n\n\n\n\nDataset.wrap(sample)\nDeserialize a raw WDS sample dict into type ST.\n\n\n\nDataset.wrap_batch(batch)\nDeserialize a raw WDS batch dict into SampleBatch[ST]."
  },
  {
    "objectID": "api/Dataset.html#parameters",
    "href": "api/Dataset.html#parameters",
    "title": "Dataset",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nST\n\nThe sample type for this dataset, must derive from PackableSample.\nrequired"
  },
  {
    "objectID": "api/Dataset.html#attributes",
    "href": "api/Dataset.html#attributes",
    "title": "Dataset",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nurl\n\nWebDataset brace-notation URL for the tar file(s)."
  },
  {
    "objectID": "api/Dataset.html#examples",
    "href": "api/Dataset.html#examples",
    "title": "Dataset",
    "section": "",
    "text": "&gt;&gt;&gt; ds = Dataset[MyData](\"path/to/data-{000000..000009}.tar\")\n&gt;&gt;&gt; for sample in ds.ordered(batch_size=32):\n...     # sample is SampleBatch[MyData] with batch_size samples\n...     embeddings = sample.embeddings  # shape: (32, ...)\n...\n&gt;&gt;&gt; # Transform to a different view\n&gt;&gt;&gt; ds_view = ds.as_type(MyDataView)"
  },
  {
    "objectID": "api/Dataset.html#note",
    "href": "api/Dataset.html#note",
    "title": "Dataset",
    "section": "",
    "text": "This class uses Python’s __orig_class__ mechanism to extract the type parameter at runtime. Instances must be created using the subscripted syntax Dataset[MyType](url) rather than calling the constructor directly with an unsubscripted class."
  },
  {
    "objectID": "api/Dataset.html#methods",
    "href": "api/Dataset.html#methods",
    "title": "Dataset",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nas_type\nView this dataset through a different sample type via a registered lens.\n\n\ndescribe\nSummary statistics: sample_type, fields, num_shards, shards, url, metadata.\n\n\nfilter\nReturn a new dataset that yields only samples matching predicate.\n\n\nget\nRetrieve a single sample by its __key__.\n\n\nhead\nReturn the first n samples from the dataset.\n\n\nlist_shards\nReturn all shard paths/URLs as a list.\n\n\nmap\nReturn a new dataset that applies fn to each sample during iteration.\n\n\nordered\nIterate over the dataset in order.\n\n\nprocess_shards\nProcess each shard independently, collecting per-shard results.\n\n\nquery\nQuery this dataset using per-shard manifest metadata.\n\n\nselect\nReturn samples at the given integer indices.\n\n\nshuffled\nIterate over the dataset in random order.\n\n\nto_dict\nMaterialize the dataset as a column-oriented dictionary.\n\n\nto_pandas\nMaterialize the dataset (or first limit samples) as a DataFrame.\n\n\nto_parquet\nExport dataset to parquet file(s).\n\n\nwrap\nDeserialize a raw WDS sample dict into type ST.\n\n\nwrap_batch\nDeserialize a raw WDS batch dict into SampleBatch[ST].\n\n\n\n\n\nDataset.as_type(other)\nView this dataset through a different sample type via a registered lens.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf no lens exists between the current and target types.\n\n\n\n\n\n\n\nDataset.describe()\nSummary statistics: sample_type, fields, num_shards, shards, url, metadata.\n\n\n\nDataset.filter(predicate)\nReturn a new dataset that yields only samples matching predicate.\nThe filter is applied lazily during iteration — no data is copied.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npredicate\nCallable[[ST], bool]\nA function that takes a sample and returns True to keep it or False to discard it.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset[ST]\nA new Dataset whose iterators apply the filter.\n\n\n\n\n\n\n&gt;&gt;&gt; long_names = ds.filter(lambda s: len(s.name) &gt; 10)\n&gt;&gt;&gt; for sample in long_names:\n...     assert len(sample.name) &gt; 10\n\n\n\n\nDataset.get(key)\nRetrieve a single sample by its __key__.\nScans shards sequentially until a sample with a matching key is found. This is O(n) for streaming datasets.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkey\nstr\nThe WebDataset __key__ string to search for.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nST\nThe matching sample.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nSampleKeyError\nIf no sample with the given key exists.\n\n\n\n\n\n\n&gt;&gt;&gt; sample = ds.get(\"00000001-0001-1000-8000-010000000000\")\n\n\n\n\nDataset.head(n=5)\nReturn the first n samples from the dataset.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint\nNumber of samples to return. Default: 5.\n5\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[ST]\nList of up to n samples in shard order.\n\n\n\n\n\n\n&gt;&gt;&gt; samples = ds.head(3)\n&gt;&gt;&gt; len(samples)\n3\n\n\n\n\nDataset.list_shards()\nReturn all shard paths/URLs as a list.\n\n\n\nDataset.map(fn)\nReturn a new dataset that applies fn to each sample during iteration.\nThe mapping is applied lazily during iteration — no data is copied.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfn\nCallable[[ST], Any]\nA function that takes a sample of type ST and returns a transformed value.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\nA new Dataset whose iterators apply the mapping.\n\n\n\n\n\n\n&gt;&gt;&gt; names = ds.map(lambda s: s.name)\n&gt;&gt;&gt; for name in names:\n...     print(name)\n\n\n\n\nDataset.ordered(batch_size=None)\nIterate over the dataset in order.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbatch_size\nint | None\nThe size of iterated batches. Default: None (unbatched). If None, iterates over one sample at a time with no batch dimension.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nIterable[ST] | Iterable[SampleBatch[ST]]\nA data pipeline that iterates over the dataset in its original\n\n\n\nIterable[ST] | Iterable[SampleBatch[ST]]\nsample order. When batch_size is None, yields individual\n\n\n\nIterable[ST] | Iterable[SampleBatch[ST]]\nsamples of type ST. When batch_size is an integer, yields\n\n\n\nIterable[ST] | Iterable[SampleBatch[ST]]\nSampleBatch[ST] instances containing that many samples.\n\n\n\n\n\n\n&gt;&gt;&gt; for sample in ds.ordered():\n...     process(sample)  # sample is ST\n&gt;&gt;&gt; for batch in ds.ordered(batch_size=32):\n...     process(batch)  # batch is SampleBatch[ST]\n\n\n\n\nDataset.process_shards(fn, *, shards=None)\nProcess each shard independently, collecting per-shard results.\nUnlike :meth:map (which is lazy and per-sample), this method eagerly processes each shard in turn, calling fn with the full list of samples from that shard. If some shards fail, raises :class:~atdata._exceptions.PartialFailureError containing both the successful results and the per-shard errors.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfn\nCallable[[list[ST]], Any]\nFunction receiving a list of samples from one shard and returning an arbitrary result.\nrequired\n\n\nshards\nlist[str] | None\nOptional list of shard identifiers to process. If None, processes all shards in the dataset. Useful for retrying only the failed shards from a previous PartialFailureError.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict[str, Any]\nDict mapping shard identifier to fn’s return value for each shard.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nPartialFailureError\nIf at least one shard fails. The exception carries .succeeded_shards, .failed_shards, .errors, and .results for inspection and retry.\n\n\n\n\n\n\n&gt;&gt;&gt; results = ds.process_shards(lambda samples: len(samples))\n&gt;&gt;&gt; # On partial failure, retry just the failed shards:\n&gt;&gt;&gt; try:\n...     results = ds.process_shards(expensive_fn)\n... except PartialFailureError as e:\n...     retry = ds.process_shards(expensive_fn, shards=e.failed_shards)\n\n\n\n\nDataset.query(where)\nQuery this dataset using per-shard manifest metadata.\nRequires manifests to have been generated during shard writing. Discovers manifest files alongside the tar shards, loads them, and executes a two-phase query (shard-level aggregate pruning, then sample-level parquet filtering).\nThe where argument accepts either a lambda/function that operates on a pandas DataFrame, or a Predicate built from the proxy DSL.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nCallable[[pd.DataFrame], pd.Series] | Predicate\nPredicate function or Predicate object that selects matching rows from the per-sample manifest DataFrame.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[SampleLocation]\nList of SampleLocation for matching samples.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nFileNotFoundError\nIf no manifest files are found alongside shards.\n\n\n\n\n\n\n&gt;&gt;&gt; locs = ds.query(where=lambda df: df[\"confidence\"] &gt; 0.9)\n&gt;&gt;&gt; len(locs)\n42\n&gt;&gt;&gt; Q = ds.fields\n&gt;&gt;&gt; locs = ds.query(where=(Q.confidence &gt; 0.9))\n\n\n\n\nDataset.select(indices)\nReturn samples at the given integer indices.\nIterates through the dataset in order and collects samples whose positional index matches. This is O(n) for streaming datasets.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nindices\nSequence[int]\nSequence of zero-based indices to select.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[ST]\nList of samples at the requested positions, in index order.\n\n\n\n\n\n\n&gt;&gt;&gt; samples = ds.select([0, 5, 10])\n&gt;&gt;&gt; len(samples)\n3\n\n\n\n\nDataset.shuffled(buffer_shards=100, buffer_samples=10000, batch_size=None)\nIterate over the dataset in random order.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbuffer_shards\nint\nNumber of shards to buffer for shuffling at the shard level. Larger values increase randomness but use more memory. Default: 100.\n100\n\n\nbuffer_samples\nint\nNumber of samples to buffer for shuffling within shards. Larger values increase randomness but use more memory. Default: 10,000.\n10000\n\n\nbatch_size\nint | None\nThe size of iterated batches. Default: None (unbatched). If None, iterates over one sample at a time with no batch dimension.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nIterable[ST] | Iterable[SampleBatch[ST]]\nA data pipeline that iterates over the dataset in randomized order.\n\n\n\nIterable[ST] | Iterable[SampleBatch[ST]]\nWhen batch_size is None, yields individual samples of type\n\n\n\nIterable[ST] | Iterable[SampleBatch[ST]]\nST. When batch_size is an integer, yields SampleBatch[ST]\n\n\n\nIterable[ST] | Iterable[SampleBatch[ST]]\ninstances containing that many samples.\n\n\n\n\n\n\n&gt;&gt;&gt; for sample in ds.shuffled():\n...     process(sample)  # sample is ST\n&gt;&gt;&gt; for batch in ds.shuffled(batch_size=32):\n...     process(batch)  # batch is SampleBatch[ST]\n\n\n\n\nDataset.to_dict(limit=None)\nMaterialize the dataset as a column-oriented dictionary.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlimit\nint | None\nMaximum number of samples to include. None means all.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict[str, list[Any]]\nDictionary mapping field names to lists of values (one entry\n\n\n\ndict[str, list[Any]]\nper sample).\n\n\n\n\n\n\nWith limit=None this loads the entire dataset into memory.\n\n\n\n&gt;&gt;&gt; d = ds.to_dict(limit=10)\n&gt;&gt;&gt; d.keys()\ndict_keys(['name', 'embedding'])\n&gt;&gt;&gt; len(d['name'])\n10\n\n\n\n\nDataset.to_pandas(limit=None)\nMaterialize the dataset (or first limit samples) as a DataFrame.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlimit\nint | None\nMaximum number of samples to include. None means all samples (may use significant memory for large datasets).\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\nA pandas DataFrame with one row per sample and columns matching\n\n\n\npd.DataFrame\nthe sample fields.\n\n\n\n\n\n\nWith limit=None this loads the entire dataset into memory.\n\n\n\n&gt;&gt;&gt; df = ds.to_pandas(limit=100)\n&gt;&gt;&gt; df.columns.tolist()\n['name', 'embedding']\n\n\n\n\nDataset.to_parquet(path, sample_map=None, maxcount=None, **kwargs)\nExport dataset to parquet file(s).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nPathlike\nOutput path. With maxcount, files are named {stem}-{segment:06d}.parquet.\nrequired\n\n\nsample_map\nOptional[SampleExportMap]\nConvert sample to dict. Defaults to dataclasses.asdict.\nNone\n\n\nmaxcount\nOptional[int]\nSplit into files of at most this many samples. Without it, the entire dataset is loaded into memory.\nNone\n\n\n**kwargs\n\nPassed to pandas.DataFrame.to_parquet().\n{}\n\n\n\n\n\n\n&gt;&gt;&gt; ds.to_parquet(\"output.parquet\", maxcount=50000)\n\n\n\n\nDataset.wrap(sample)\nDeserialize a raw WDS sample dict into type ST.\n\n\n\nDataset.wrap_batch(batch)\nDeserialize a raw WDS batch dict into SampleBatch[ST]."
  },
  {
    "objectID": "api/local.Index.html",
    "href": "api/local.Index.html",
    "title": "local.Index",
    "section": "",
    "text": "local.Index(\n    provider=None,\n    *,\n    path=None,\n    dsn=None,\n    redis=None,\n    data_store=None,\n    repos=None,\n    atmosphere=_ATMOSPHERE_DEFAULT,\n    auto_stubs=False,\n    stub_dir=None,\n    **kwargs,\n)\nUnified index for tracking datasets across multiple repositories.\nImplements the AbstractIndex protocol. Maintains a registry of dataset entries across named repositories (always including a built-in \"local\" repository) and an optional atmosphere (ATProto) backend.\nThe \"local\" repository is always present and uses the storage backend determined by the provider argument. When no provider is given, defaults to SQLite (zero external dependencies). Pass a redis connection or Redis **kwargs for backwards-compatible Redis behaviour.\nAdditional named repositories can be mounted via the repos parameter, each pairing an IndexProvider with an optional data store.\nAn Atmosphere is available by default for anonymous read-only resolution of @handle/dataset paths. Pass an authenticated client for write operations, or atmosphere=None to disable.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n_repos\ndict[str, _Repo]\nAll repositories keyed by name. \"local\" is always present.\n\n\n_atmosphere\n_AtmosphereBackend | None\nOptional atmosphere backend for ATProto operations.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nadd_entry\nAdd a dataset to the local repository index.\n\n\nclear_stubs\nRemove all auto-generated stub files.\n\n\ndecode_schema\nReconstruct a Python PackableSample type from a stored schema.\n\n\ndecode_schema_as\nDecode a schema with explicit type hint for IDE support.\n\n\nget_dataset\nGet a dataset entry by name or prefixed reference.\n\n\nget_entry\nGet an entry by its CID.\n\n\nget_entry_by_name\nGet an entry by its human-readable name.\n\n\nget_import_path\nGet the import path for a schema’s generated module.\n\n\nget_schema\nGet a schema record by reference (AbstractIndex protocol).\n\n\nget_schema_record\nGet a schema record as LocalSchemaRecord object.\n\n\ninsert_dataset\nInsert a dataset into the index (AbstractIndex protocol).\n\n\nlist_datasets\nGet dataset entries as a materialized list (AbstractIndex protocol).\n\n\nlist_entries\nGet all index entries as a materialized list.\n\n\nlist_schemas\nGet all schema records as a materialized list (AbstractIndex protocol).\n\n\nload_schema\nLoad a schema and make it available in the types namespace.\n\n\npromote_dataset\nPublish a Dataset directly to the atmosphere.\n\n\npromote_entry\nPromote a locally-indexed dataset to the atmosphere.\n\n\npublish_schema\nPublish a schema for a sample type to Redis.\n\n\nwrite\nWrite samples and create an index entry in one step.\n\n\n\n\n\nlocal.Index.add_entry(ds, *, name, schema_ref=None, metadata=None)\nAdd a dataset to the local repository index.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nds\nDataset\nThe dataset to add to the index.\nrequired\n\n\nname\nstr\nHuman-readable name for the dataset.\nrequired\n\n\nschema_ref\nstr | None\nOptional schema reference. If None, generates from sample type.\nNone\n\n\nmetadata\ndict | None\nOptional metadata dictionary. If None, uses ds._metadata if available.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nLocalDatasetEntry\nThe created LocalDatasetEntry object.\n\n\n\n\n\n\n\nlocal.Index.clear_stubs()\nRemove all auto-generated stub files.\nOnly works if auto_stubs was enabled when creating the Index.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nint\nNumber of stub files removed, or 0 if auto_stubs is disabled.\n\n\n\n\n\n\n\nlocal.Index.decode_schema(ref)\nReconstruct a Python PackableSample type from a stored schema.\nThis method enables loading datasets without knowing the sample type ahead of time. The index retrieves the schema record and dynamically generates a PackableSample subclass matching the schema definition.\nIf auto_stubs is enabled, a Python module will be generated and the class will be imported from it, providing full IDE autocomplete support. The returned class has proper type information that IDEs can understand.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nref\nstr\nSchema reference string (atdata://local/schema/… or legacy local://schemas/…).\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nType[Packable]\nA PackableSample subclass - either imported from a generated module\n\n\n\nType[Packable]\n(if auto_stubs is enabled) or dynamically created.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf schema not found.\n\n\n\nValueError\nIf schema cannot be decoded.\n\n\n\n\n\n\n\nlocal.Index.decode_schema_as(ref, type_hint)\nDecode a schema with explicit type hint for IDE support.\nThis is a typed wrapper around decode_schema() that preserves the type information for IDE autocomplete. Use this when you have a stub file for the schema and want full IDE support.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nref\nstr\nSchema reference string.\nrequired\n\n\ntype_hint\ntype[T]\nThe stub type to use for type hints. Import this from the generated stub file.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntype[T]\nThe decoded type, cast to match the type_hint for IDE support.\n\n\n\n\n\n\n&gt;&gt;&gt; # After enabling auto_stubs and configuring IDE extraPaths:\n&gt;&gt;&gt; from local.MySample_1_0_0 import MySample\n&gt;&gt;&gt;\n&gt;&gt;&gt; # This gives full IDE autocomplete:\n&gt;&gt;&gt; DecodedType = index.decode_schema_as(ref, MySample)\n&gt;&gt;&gt; sample = DecodedType(text=\"hello\", value=42)  # IDE knows signature!\n\n\n\nThe type_hint is only used for static type checking - at runtime, the actual decoded type from the schema is returned. Ensure the stub matches the schema to avoid runtime surprises.\n\n\n\n\nlocal.Index.get_dataset(ref)\nGet a dataset entry by name or prefixed reference.\nSupports repository-prefixed lookups (e.g. \"lab/mnist\"), atmosphere paths (\"@handle/dataset\"), AT URIs, and bare names (which default to the \"local\" repository).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nref\nstr\nDataset name, prefixed name, or AT URI.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n'IndexEntry'\nIndexEntry for the dataset.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf dataset not found.\n\n\n\nValueError\nIf the atmosphere backend is required but unavailable.\n\n\n\n\n\n\n\nlocal.Index.get_entry(cid)\nGet an entry by its CID.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncid\nstr\nContent identifier of the entry.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nLocalDatasetEntry\nLocalDatasetEntry for the given CID.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf entry not found.\n\n\n\n\n\n\n\nlocal.Index.get_entry_by_name(name)\nGet an entry by its human-readable name.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nHuman-readable name of the entry.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nLocalDatasetEntry\nLocalDatasetEntry with the given name.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf no entry with that name exists.\n\n\n\n\n\n\n\nlocal.Index.get_import_path(ref)\nGet the import path for a schema’s generated module.\nWhen auto_stubs is enabled, this returns the import path that can be used to import the schema type with full IDE support.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nref\nstr\nSchema reference string.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr | None\nImport path like “local.MySample_1_0_0”, or None if auto_stubs\n\n\n\nstr | None\nis disabled.\n\n\n\n\n\n\n&gt;&gt;&gt; index = Index(auto_stubs=True)\n&gt;&gt;&gt; ref = index.publish_schema(MySample, version=\"1.0.0\")\n&gt;&gt;&gt; index.load_schema(ref)\n&gt;&gt;&gt; print(index.get_import_path(ref))\nlocal.MySample_1_0_0\n&gt;&gt;&gt; # Then in your code:\n&gt;&gt;&gt; # from local.MySample_1_0_0 import MySample\n\n\n\n\nlocal.Index.get_schema(ref)\nGet a schema record by reference (AbstractIndex protocol).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nref\nstr\nSchema reference string. Supports both new format (atdata://local/schema/{name}@version) and legacy format (local://schemas/{module.Class}@version).\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict\nSchema record as a dictionary with keys ‘name’, ‘version’,\n\n\n\ndict\n‘fields’, ‘$ref’, etc.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf schema not found.\n\n\n\nValueError\nIf reference format is invalid.\n\n\n\n\n\n\n\nlocal.Index.get_schema_record(ref)\nGet a schema record as LocalSchemaRecord object.\nUse this when you need the full LocalSchemaRecord with typed properties. For Protocol-compliant dict access, use get_schema() instead.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nref\nstr\nSchema reference string.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nLocalSchemaRecord\nLocalSchemaRecord with schema details.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf schema not found.\n\n\n\nValueError\nIf reference format is invalid.\n\n\n\n\n\n\n\nlocal.Index.insert_dataset(ds, *, name, schema_ref=None, **kwargs)\nInsert a dataset into the index (AbstractIndex protocol).\nThe target repository is determined by a prefix in the name argument (e.g. \"lab/mnist\"). If no prefix is given, or the prefix is \"local\", the built-in local repository is used.\nIf the target repository has a data_store, shards are written to storage first, then indexed. Otherwise, the dataset’s existing URL is indexed directly.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nds\nDataset\nThe Dataset to register.\nrequired\n\n\nname\nstr\nHuman-readable name for the dataset, optionally prefixed with a repository name (e.g. \"lab/mnist\").\nrequired\n\n\nschema_ref\nstr | None\nOptional schema reference.\nNone\n\n\n**kwargs\n\nAdditional options: - metadata: Optional metadata dict - prefix: Storage prefix (default: dataset name) - cache_local: If True, cache writes locally first\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n'IndexEntry'\nIndexEntry for the inserted dataset.\n\n\n\n\n\n\n\nlocal.Index.list_datasets(repo=None)\nGet dataset entries as a materialized list (AbstractIndex protocol).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrepo\nstr | None\nOptional repository filter. If None, aggregates entries from \"local\" and all named repositories. Use \"local\" for only the built-in repository, a named repo key, or \"_atmosphere\" for atmosphere entries.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist['IndexEntry']\nList of IndexEntry for each dataset.\n\n\n\n\n\n\n\nlocal.Index.list_entries()\nGet all index entries as a materialized list.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[LocalDatasetEntry]\nList of all LocalDatasetEntry objects in the index.\n\n\n\n\n\n\n\nlocal.Index.list_schemas()\nGet all schema records as a materialized list (AbstractIndex protocol).\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[dict]\nList of schema records as dictionaries.\n\n\n\n\n\n\n\nlocal.Index.load_schema(ref)\nLoad a schema and make it available in the types namespace.\nThis method decodes the schema, optionally generates a Python module for IDE support (if auto_stubs is enabled), and registers the type in the :attr:types namespace for easy access.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nref\nstr\nSchema reference string (atdata://local/schema/… or legacy local://schemas/…).\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nType[Packable]\nThe decoded PackableSample subclass. Also available via\n\n\n\nType[Packable]\nindex.types.&lt;ClassName&gt; after this call.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf schema not found.\n\n\n\nValueError\nIf schema cannot be decoded.\n\n\n\n\n\n\n&gt;&gt;&gt; # Load and use immediately\n&gt;&gt;&gt; MyType = index.load_schema(\"atdata://local/schema/MySample@1.0.0\")\n&gt;&gt;&gt; sample = MyType(field1=\"hello\", field2=42)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Or access later via namespace\n&gt;&gt;&gt; index.load_schema(\"atdata://local/schema/OtherType@1.0.0\")\n&gt;&gt;&gt; other = index.types.OtherType(data=\"test\")\n\n\n\n\nlocal.Index.promote_dataset(\n    dataset,\n    *,\n    name,\n    sample_type=None,\n    schema_version='1.0.0',\n    description=None,\n    tags=None,\n    license=None,\n)\nPublish a Dataset directly to the atmosphere.\nPublishes the schema (with deduplication) and creates a dataset record on ATProto. Uses the index’s atmosphere backend.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset\nDataset\nThe Dataset to publish.\nrequired\n\n\nname\nstr\nName for the atmosphere dataset record.\nrequired\n\n\nsample_type\ntype | None\nSample type for schema publishing. Inferred from dataset.sample_type if not provided.\nNone\n\n\nschema_version\nstr\nSemantic version for the schema. Default: \"1.0.0\".\n'1.0.0'\n\n\ndescription\nstr | None\nOptional description for the dataset.\nNone\n\n\ntags\nlist[str] | None\nOptional tags for discovery.\nNone\n\n\nlicense\nstr | None\nOptional license identifier.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nAT URI of the created atmosphere dataset record.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf atmosphere backend is not available.\n\n\n\n\n\n\n&gt;&gt;&gt; index = Index(atmosphere=client)\n&gt;&gt;&gt; ds = atdata.load_dataset(\"./data.tar\", MySample, split=\"train\")\n&gt;&gt;&gt; uri = index.promote_dataset(ds, name=\"my-dataset\")\n\n\n\n\nlocal.Index.promote_entry(\n    entry_name,\n    *,\n    name=None,\n    description=None,\n    tags=None,\n    license=None,\n)\nPromote a locally-indexed dataset to the atmosphere.\nLooks up the entry by name in the local index, resolves its schema, and publishes both schema and dataset record to ATProto via the index’s atmosphere backend.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nentry_name\nstr\nName of the local dataset entry to promote.\nrequired\n\n\nname\nstr | None\nOverride name for the atmosphere record. Defaults to the local entry name.\nNone\n\n\ndescription\nstr | None\nOptional description for the dataset.\nNone\n\n\ntags\nlist[str] | None\nOptional tags for discovery.\nNone\n\n\nlicense\nstr | None\nOptional license identifier.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nAT URI of the created atmosphere dataset record.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf atmosphere backend is not available, or the local entry has no data URLs.\n\n\n\nKeyError\nIf the entry or its schema is not found.\n\n\n\n\n\n\n&gt;&gt;&gt; index = Index(atmosphere=client)\n&gt;&gt;&gt; uri = index.promote_entry(\"mnist-train\")\n\n\n\n\nlocal.Index.publish_schema(sample_type, *, version=None, description=None)\nPublish a schema for a sample type to Redis.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsample_type\ntype\nA Packable type (@packable-decorated or PackableSample subclass).\nrequired\n\n\nversion\nstr | None\nSemantic version string (e.g., ‘1.0.0’). If None, auto-increments from the latest published version (patch bump), or starts at ‘1.0.0’ if no previous version exists.\nNone\n\n\ndescription\nstr | None\nOptional human-readable description. If None, uses the class docstring.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nSchema reference string: ‘atdata://local/schema/{name}@version’.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf sample_type is not a dataclass.\n\n\n\nTypeError\nIf sample_type doesn’t satisfy the Packable protocol, or if a field type is not supported.\n\n\n\n\n\n\n\nlocal.Index.write(\n    samples,\n    *,\n    name,\n    schema_ref=None,\n    description=None,\n    tags=None,\n    license=None,\n    maxcount=10000,\n    maxsize=None,\n    metadata=None,\n    manifest=False,\n)\nWrite samples and create an index entry in one step.\nThis is the primary method for publishing data. It serializes samples to WebDataset tar files, stores them via the appropriate backend, and creates an index entry.\nThe target backend is determined by the name prefix:\n\nBare name (e.g., \"mnist\"): writes to the local repository.\n\"@handle/name\": writes and publishes to the atmosphere.\n\"repo/name\": writes to a named repository.\n\nWhen the local backend has no data_store configured, a LocalDiskStore is created automatically at ~/.atdata/data/ so that samples have persistent storage.\n.. note::\nThis method is synchronous. Samples are written to a temporary\nlocation first, then copied to permanent storage by the backend.\nAvoid passing lazily-evaluated iterators that depend on external\nstate that may change during the call.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsamples\nIterable\nIterable of Packable samples. Must be non-empty.\nrequired\n\n\nname\nstr\nDataset name, optionally prefixed with target.\nrequired\n\n\nschema_ref\nstr | None\nOptional schema reference. Auto-generated if None.\nNone\n\n\ndescription\nstr | None\nOptional dataset description (atmosphere only).\nNone\n\n\ntags\nlist[str] | None\nOptional tags for discovery (atmosphere only).\nNone\n\n\nlicense\nstr | None\nOptional license identifier (atmosphere only).\nNone\n\n\nmaxcount\nint\nMax samples per shard. Default: 10,000.\n10000\n\n\nmaxsize\nint | None\nMax bytes per shard. Default: None.\nNone\n\n\nmetadata\ndict | None\nOptional metadata dict stored with the entry.\nNone\n\n\nmanifest\nbool\nIf True, write per-shard manifest sidecar files alongside each tar. Default: False.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n'IndexEntry'\nIndexEntry for the created dataset.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf samples is empty.\n\n\n\n\n\n\n&gt;&gt;&gt; index = Index()\n&gt;&gt;&gt; samples = [MySample(key=\"0\", text=\"hello\")]\n&gt;&gt;&gt; entry = index.write(samples, name=\"my-dataset\")"
  },
  {
    "objectID": "api/local.Index.html#attributes",
    "href": "api/local.Index.html#attributes",
    "title": "local.Index",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n_repos\ndict[str, _Repo]\nAll repositories keyed by name. \"local\" is always present.\n\n\n_atmosphere\n_AtmosphereBackend | None\nOptional atmosphere backend for ATProto operations."
  },
  {
    "objectID": "api/local.Index.html#methods",
    "href": "api/local.Index.html#methods",
    "title": "local.Index",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nadd_entry\nAdd a dataset to the local repository index.\n\n\nclear_stubs\nRemove all auto-generated stub files.\n\n\ndecode_schema\nReconstruct a Python PackableSample type from a stored schema.\n\n\ndecode_schema_as\nDecode a schema with explicit type hint for IDE support.\n\n\nget_dataset\nGet a dataset entry by name or prefixed reference.\n\n\nget_entry\nGet an entry by its CID.\n\n\nget_entry_by_name\nGet an entry by its human-readable name.\n\n\nget_import_path\nGet the import path for a schema’s generated module.\n\n\nget_schema\nGet a schema record by reference (AbstractIndex protocol).\n\n\nget_schema_record\nGet a schema record as LocalSchemaRecord object.\n\n\ninsert_dataset\nInsert a dataset into the index (AbstractIndex protocol).\n\n\nlist_datasets\nGet dataset entries as a materialized list (AbstractIndex protocol).\n\n\nlist_entries\nGet all index entries as a materialized list.\n\n\nlist_schemas\nGet all schema records as a materialized list (AbstractIndex protocol).\n\n\nload_schema\nLoad a schema and make it available in the types namespace.\n\n\npromote_dataset\nPublish a Dataset directly to the atmosphere.\n\n\npromote_entry\nPromote a locally-indexed dataset to the atmosphere.\n\n\npublish_schema\nPublish a schema for a sample type to Redis.\n\n\nwrite\nWrite samples and create an index entry in one step.\n\n\n\n\n\nlocal.Index.add_entry(ds, *, name, schema_ref=None, metadata=None)\nAdd a dataset to the local repository index.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nds\nDataset\nThe dataset to add to the index.\nrequired\n\n\nname\nstr\nHuman-readable name for the dataset.\nrequired\n\n\nschema_ref\nstr | None\nOptional schema reference. If None, generates from sample type.\nNone\n\n\nmetadata\ndict | None\nOptional metadata dictionary. If None, uses ds._metadata if available.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nLocalDatasetEntry\nThe created LocalDatasetEntry object.\n\n\n\n\n\n\n\nlocal.Index.clear_stubs()\nRemove all auto-generated stub files.\nOnly works if auto_stubs was enabled when creating the Index.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nint\nNumber of stub files removed, or 0 if auto_stubs is disabled.\n\n\n\n\n\n\n\nlocal.Index.decode_schema(ref)\nReconstruct a Python PackableSample type from a stored schema.\nThis method enables loading datasets without knowing the sample type ahead of time. The index retrieves the schema record and dynamically generates a PackableSample subclass matching the schema definition.\nIf auto_stubs is enabled, a Python module will be generated and the class will be imported from it, providing full IDE autocomplete support. The returned class has proper type information that IDEs can understand.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nref\nstr\nSchema reference string (atdata://local/schema/… or legacy local://schemas/…).\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nType[Packable]\nA PackableSample subclass - either imported from a generated module\n\n\n\nType[Packable]\n(if auto_stubs is enabled) or dynamically created.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf schema not found.\n\n\n\nValueError\nIf schema cannot be decoded.\n\n\n\n\n\n\n\nlocal.Index.decode_schema_as(ref, type_hint)\nDecode a schema with explicit type hint for IDE support.\nThis is a typed wrapper around decode_schema() that preserves the type information for IDE autocomplete. Use this when you have a stub file for the schema and want full IDE support.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nref\nstr\nSchema reference string.\nrequired\n\n\ntype_hint\ntype[T]\nThe stub type to use for type hints. Import this from the generated stub file.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntype[T]\nThe decoded type, cast to match the type_hint for IDE support.\n\n\n\n\n\n\n&gt;&gt;&gt; # After enabling auto_stubs and configuring IDE extraPaths:\n&gt;&gt;&gt; from local.MySample_1_0_0 import MySample\n&gt;&gt;&gt;\n&gt;&gt;&gt; # This gives full IDE autocomplete:\n&gt;&gt;&gt; DecodedType = index.decode_schema_as(ref, MySample)\n&gt;&gt;&gt; sample = DecodedType(text=\"hello\", value=42)  # IDE knows signature!\n\n\n\nThe type_hint is only used for static type checking - at runtime, the actual decoded type from the schema is returned. Ensure the stub matches the schema to avoid runtime surprises.\n\n\n\n\nlocal.Index.get_dataset(ref)\nGet a dataset entry by name or prefixed reference.\nSupports repository-prefixed lookups (e.g. \"lab/mnist\"), atmosphere paths (\"@handle/dataset\"), AT URIs, and bare names (which default to the \"local\" repository).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nref\nstr\nDataset name, prefixed name, or AT URI.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n'IndexEntry'\nIndexEntry for the dataset.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf dataset not found.\n\n\n\nValueError\nIf the atmosphere backend is required but unavailable.\n\n\n\n\n\n\n\nlocal.Index.get_entry(cid)\nGet an entry by its CID.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncid\nstr\nContent identifier of the entry.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nLocalDatasetEntry\nLocalDatasetEntry for the given CID.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf entry not found.\n\n\n\n\n\n\n\nlocal.Index.get_entry_by_name(name)\nGet an entry by its human-readable name.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nHuman-readable name of the entry.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nLocalDatasetEntry\nLocalDatasetEntry with the given name.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf no entry with that name exists.\n\n\n\n\n\n\n\nlocal.Index.get_import_path(ref)\nGet the import path for a schema’s generated module.\nWhen auto_stubs is enabled, this returns the import path that can be used to import the schema type with full IDE support.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nref\nstr\nSchema reference string.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr | None\nImport path like “local.MySample_1_0_0”, or None if auto_stubs\n\n\n\nstr | None\nis disabled.\n\n\n\n\n\n\n&gt;&gt;&gt; index = Index(auto_stubs=True)\n&gt;&gt;&gt; ref = index.publish_schema(MySample, version=\"1.0.0\")\n&gt;&gt;&gt; index.load_schema(ref)\n&gt;&gt;&gt; print(index.get_import_path(ref))\nlocal.MySample_1_0_0\n&gt;&gt;&gt; # Then in your code:\n&gt;&gt;&gt; # from local.MySample_1_0_0 import MySample\n\n\n\n\nlocal.Index.get_schema(ref)\nGet a schema record by reference (AbstractIndex protocol).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nref\nstr\nSchema reference string. Supports both new format (atdata://local/schema/{name}@version) and legacy format (local://schemas/{module.Class}@version).\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict\nSchema record as a dictionary with keys ‘name’, ‘version’,\n\n\n\ndict\n‘fields’, ‘$ref’, etc.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf schema not found.\n\n\n\nValueError\nIf reference format is invalid.\n\n\n\n\n\n\n\nlocal.Index.get_schema_record(ref)\nGet a schema record as LocalSchemaRecord object.\nUse this when you need the full LocalSchemaRecord with typed properties. For Protocol-compliant dict access, use get_schema() instead.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nref\nstr\nSchema reference string.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nLocalSchemaRecord\nLocalSchemaRecord with schema details.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf schema not found.\n\n\n\nValueError\nIf reference format is invalid.\n\n\n\n\n\n\n\nlocal.Index.insert_dataset(ds, *, name, schema_ref=None, **kwargs)\nInsert a dataset into the index (AbstractIndex protocol).\nThe target repository is determined by a prefix in the name argument (e.g. \"lab/mnist\"). If no prefix is given, or the prefix is \"local\", the built-in local repository is used.\nIf the target repository has a data_store, shards are written to storage first, then indexed. Otherwise, the dataset’s existing URL is indexed directly.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nds\nDataset\nThe Dataset to register.\nrequired\n\n\nname\nstr\nHuman-readable name for the dataset, optionally prefixed with a repository name (e.g. \"lab/mnist\").\nrequired\n\n\nschema_ref\nstr | None\nOptional schema reference.\nNone\n\n\n**kwargs\n\nAdditional options: - metadata: Optional metadata dict - prefix: Storage prefix (default: dataset name) - cache_local: If True, cache writes locally first\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n'IndexEntry'\nIndexEntry for the inserted dataset.\n\n\n\n\n\n\n\nlocal.Index.list_datasets(repo=None)\nGet dataset entries as a materialized list (AbstractIndex protocol).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrepo\nstr | None\nOptional repository filter. If None, aggregates entries from \"local\" and all named repositories. Use \"local\" for only the built-in repository, a named repo key, or \"_atmosphere\" for atmosphere entries.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist['IndexEntry']\nList of IndexEntry for each dataset.\n\n\n\n\n\n\n\nlocal.Index.list_entries()\nGet all index entries as a materialized list.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[LocalDatasetEntry]\nList of all LocalDatasetEntry objects in the index.\n\n\n\n\n\n\n\nlocal.Index.list_schemas()\nGet all schema records as a materialized list (AbstractIndex protocol).\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[dict]\nList of schema records as dictionaries.\n\n\n\n\n\n\n\nlocal.Index.load_schema(ref)\nLoad a schema and make it available in the types namespace.\nThis method decodes the schema, optionally generates a Python module for IDE support (if auto_stubs is enabled), and registers the type in the :attr:types namespace for easy access.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nref\nstr\nSchema reference string (atdata://local/schema/… or legacy local://schemas/…).\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nType[Packable]\nThe decoded PackableSample subclass. Also available via\n\n\n\nType[Packable]\nindex.types.&lt;ClassName&gt; after this call.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf schema not found.\n\n\n\nValueError\nIf schema cannot be decoded.\n\n\n\n\n\n\n&gt;&gt;&gt; # Load and use immediately\n&gt;&gt;&gt; MyType = index.load_schema(\"atdata://local/schema/MySample@1.0.0\")\n&gt;&gt;&gt; sample = MyType(field1=\"hello\", field2=42)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Or access later via namespace\n&gt;&gt;&gt; index.load_schema(\"atdata://local/schema/OtherType@1.0.0\")\n&gt;&gt;&gt; other = index.types.OtherType(data=\"test\")\n\n\n\n\nlocal.Index.promote_dataset(\n    dataset,\n    *,\n    name,\n    sample_type=None,\n    schema_version='1.0.0',\n    description=None,\n    tags=None,\n    license=None,\n)\nPublish a Dataset directly to the atmosphere.\nPublishes the schema (with deduplication) and creates a dataset record on ATProto. Uses the index’s atmosphere backend.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset\nDataset\nThe Dataset to publish.\nrequired\n\n\nname\nstr\nName for the atmosphere dataset record.\nrequired\n\n\nsample_type\ntype | None\nSample type for schema publishing. Inferred from dataset.sample_type if not provided.\nNone\n\n\nschema_version\nstr\nSemantic version for the schema. Default: \"1.0.0\".\n'1.0.0'\n\n\ndescription\nstr | None\nOptional description for the dataset.\nNone\n\n\ntags\nlist[str] | None\nOptional tags for discovery.\nNone\n\n\nlicense\nstr | None\nOptional license identifier.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nAT URI of the created atmosphere dataset record.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf atmosphere backend is not available.\n\n\n\n\n\n\n&gt;&gt;&gt; index = Index(atmosphere=client)\n&gt;&gt;&gt; ds = atdata.load_dataset(\"./data.tar\", MySample, split=\"train\")\n&gt;&gt;&gt; uri = index.promote_dataset(ds, name=\"my-dataset\")\n\n\n\n\nlocal.Index.promote_entry(\n    entry_name,\n    *,\n    name=None,\n    description=None,\n    tags=None,\n    license=None,\n)\nPromote a locally-indexed dataset to the atmosphere.\nLooks up the entry by name in the local index, resolves its schema, and publishes both schema and dataset record to ATProto via the index’s atmosphere backend.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nentry_name\nstr\nName of the local dataset entry to promote.\nrequired\n\n\nname\nstr | None\nOverride name for the atmosphere record. Defaults to the local entry name.\nNone\n\n\ndescription\nstr | None\nOptional description for the dataset.\nNone\n\n\ntags\nlist[str] | None\nOptional tags for discovery.\nNone\n\n\nlicense\nstr | None\nOptional license identifier.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nAT URI of the created atmosphere dataset record.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf atmosphere backend is not available, or the local entry has no data URLs.\n\n\n\nKeyError\nIf the entry or its schema is not found.\n\n\n\n\n\n\n&gt;&gt;&gt; index = Index(atmosphere=client)\n&gt;&gt;&gt; uri = index.promote_entry(\"mnist-train\")\n\n\n\n\nlocal.Index.publish_schema(sample_type, *, version=None, description=None)\nPublish a schema for a sample type to Redis.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsample_type\ntype\nA Packable type (@packable-decorated or PackableSample subclass).\nrequired\n\n\nversion\nstr | None\nSemantic version string (e.g., ‘1.0.0’). If None, auto-increments from the latest published version (patch bump), or starts at ‘1.0.0’ if no previous version exists.\nNone\n\n\ndescription\nstr | None\nOptional human-readable description. If None, uses the class docstring.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nSchema reference string: ‘atdata://local/schema/{name}@version’.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf sample_type is not a dataclass.\n\n\n\nTypeError\nIf sample_type doesn’t satisfy the Packable protocol, or if a field type is not supported.\n\n\n\n\n\n\n\nlocal.Index.write(\n    samples,\n    *,\n    name,\n    schema_ref=None,\n    description=None,\n    tags=None,\n    license=None,\n    maxcount=10000,\n    maxsize=None,\n    metadata=None,\n    manifest=False,\n)\nWrite samples and create an index entry in one step.\nThis is the primary method for publishing data. It serializes samples to WebDataset tar files, stores them via the appropriate backend, and creates an index entry.\nThe target backend is determined by the name prefix:\n\nBare name (e.g., \"mnist\"): writes to the local repository.\n\"@handle/name\": writes and publishes to the atmosphere.\n\"repo/name\": writes to a named repository.\n\nWhen the local backend has no data_store configured, a LocalDiskStore is created automatically at ~/.atdata/data/ so that samples have persistent storage.\n.. note::\nThis method is synchronous. Samples are written to a temporary\nlocation first, then copied to permanent storage by the backend.\nAvoid passing lazily-evaluated iterators that depend on external\nstate that may change during the call.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsamples\nIterable\nIterable of Packable samples. Must be non-empty.\nrequired\n\n\nname\nstr\nDataset name, optionally prefixed with target.\nrequired\n\n\nschema_ref\nstr | None\nOptional schema reference. Auto-generated if None.\nNone\n\n\ndescription\nstr | None\nOptional dataset description (atmosphere only).\nNone\n\n\ntags\nlist[str] | None\nOptional tags for discovery (atmosphere only).\nNone\n\n\nlicense\nstr | None\nOptional license identifier (atmosphere only).\nNone\n\n\nmaxcount\nint\nMax samples per shard. Default: 10,000.\n10000\n\n\nmaxsize\nint | None\nMax bytes per shard. Default: None.\nNone\n\n\nmetadata\ndict | None\nOptional metadata dict stored with the entry.\nNone\n\n\nmanifest\nbool\nIf True, write per-shard manifest sidecar files alongside each tar. Default: False.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n'IndexEntry'\nIndexEntry for the created dataset.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf samples is empty.\n\n\n\n\n\n\n&gt;&gt;&gt; index = Index()\n&gt;&gt;&gt; samples = [MySample(key=\"0\", text=\"hello\")]\n&gt;&gt;&gt; entry = index.write(samples, name=\"my-dataset\")"
  },
  {
    "objectID": "api/Lens.html",
    "href": "api/Lens.html",
    "title": "lens",
    "section": "",
    "text": "lens\nLens-based type transformations for datasets.\nThis module implements a lens system for bidirectional transformations between different sample types. Lenses enable viewing a dataset through different type schemas without duplicating the underlying data.\nKey components:\n\nLens: Bidirectional transformation with getter (S -&gt; V) and optional putter (V, S -&gt; S)\nLensNetwork: Global singleton registry for lens transformations\n@lens: Decorator to create and register lens transformations\n\nLenses support the functional programming concept of composable, well-behaved transformations that satisfy lens laws (GetPut and PutGet).\n\n\n&gt;&gt;&gt; @packable\n... class FullData:\n...     name: str\n...     age: int\n...     embedding: NDArray\n...\n&gt;&gt;&gt; @packable\n... class NameOnly:\n...     name: str\n...\n&gt;&gt;&gt; @lens\n... def name_view(full: FullData) -&gt; NameOnly:\n...     return NameOnly(name=full.name)\n...\n&gt;&gt;&gt; @name_view.putter\n... def name_view_put(view: NameOnly, source: FullData) -&gt; FullData:\n...     return FullData(name=view.name, age=source.age,\n...                     embedding=source.embedding)\n...\n&gt;&gt;&gt; ds = Dataset[FullData](\"data.tar\")\n&gt;&gt;&gt; ds_names = ds.as_type(NameOnly)  # Uses registered lens\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nLens\nA bidirectional transformation between two sample types.\n\n\nLensNetwork\nGlobal registry for lens transformations between sample types.\n\n\n\n\n\nlens.Lens(get, put=None)\nA bidirectional transformation between two sample types.\nA lens provides a way to view and update data of type S (source) as if it were type V (view). It consists of a getter that transforms S -&gt; V and an optional putter that transforms (V, S) -&gt; S, enabling updates to the view to be reflected back in the source.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nS\n\nThe source type, must derive from PackableSample.\nrequired\n\n\nV\n\nThe view type, must derive from PackableSample.\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; @lens\n... def name_lens(full: FullData) -&gt; NameOnly:\n...     return NameOnly(name=full.name)\n...\n&gt;&gt;&gt; @name_lens.putter\n... def name_lens_put(view: NameOnly, source: FullData) -&gt; FullData:\n...     return FullData(name=view.name, age=source.age)\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget\nTransform the source into the view type.\n\n\nput\nUpdate the source based on a modified view.\n\n\nputter\nDecorator to register a putter function for this lens.\n\n\n\n\n\nlens.Lens.get(s)\nTransform the source into the view type.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ns\nS\nThe source sample of type S.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nV\nA view of the source as type V.\n\n\n\n\n\n\n\nlens.Lens.put(v, s)\nUpdate the source based on a modified view.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nv\nV\nThe modified view of type V.\nrequired\n\n\ns\nS\nThe original source of type S.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nS\nAn updated source of type S that reflects changes from the view.\n\n\n\n\n\n\n\nlens.Lens.putter(put)\nDecorator to register a putter function for this lens.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nput\nLensPutter[S, V]\nA function that takes a view of type V and source of type S, and returns an updated source of type S.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nLensPutter[S, V]\nThe putter function, allowing this to be used as a decorator.\n\n\n\n\n\n\n&gt;&gt;&gt; @my_lens.putter\n... def my_lens_put(view: ViewType, source: SourceType) -&gt; SourceType:\n...     return SourceType(field=view.field, other=source.other)\n\n\n\n\n\n\nlens.LensNetwork()\nGlobal registry for lens transformations between sample types.\nThis class implements a singleton pattern to maintain a global registry of all lenses decorated with @lens. It enables looking up transformations between different PackableSample types.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n_instance\n\nThe singleton instance of this class.\n\n\n_registry\nDict[LensSignature, Lens]\nDictionary mapping (source_type, view_type) tuples to their corresponding Lens objects.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nregister\nRegister a lens as the canonical transformation between two types.\n\n\ntransform\nLook up the lens transformation between two sample types.\n\n\n\n\n\nlens.LensNetwork.register(_lens)\nRegister a lens as the canonical transformation between two types.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n_lens\nLens\nThe lens to register. Will be stored in the registry under the key (_lens.source_type, _lens.view_type).\nrequired\n\n\n\n\n\n\nIf a lens already exists for the same type pair, it will be overwritten.\n\n\n\n\nlens.LensNetwork.transform(source, view)\nLook up the lens transformation between two sample types.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource\nDatasetType\nThe source sample type (must derive from PackableSample).\nrequired\n\n\nview\nDatasetType\nThe target view type (must derive from PackableSample).\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nLens\nThe registered Lens that transforms from source to view.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf no lens has been registered for the given type pair.\n\n\n\n\n\n\nCurrently only supports direct transformations. Compositional transformations (chaining multiple lenses) are not yet implemented.\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nlens\nDecorator to create and register a lens transformation.\n\n\n\n\n\nlens.lens(f)\nDecorator to create and register a lens transformation.\nThis decorator converts a getter function into a Lens object and automatically registers it in the global LensNetwork registry.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nf\nLensGetter[S, V]\nA getter function that transforms from source type S to view type V. Must have exactly one parameter with a type annotation.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nLens[S, V]\nA Lens[S, V] object that can be called to apply the transformation\n\n\n\nLens[S, V]\nor decorated with @lens_name.putter to add a putter function.\n\n\n\n\n\n\n&gt;&gt;&gt; @lens\n... def extract_name(full: FullData) -&gt; NameOnly:\n...     return NameOnly(name=full.name)\n...\n&gt;&gt;&gt; @extract_name.putter\n... def extract_name_put(view: NameOnly, source: FullData) -&gt; FullData:\n...     return FullData(name=view.name, age=source.age)"
  },
  {
    "objectID": "api/Lens.html#examples",
    "href": "api/Lens.html#examples",
    "title": "lens",
    "section": "",
    "text": "&gt;&gt;&gt; @packable\n... class FullData:\n...     name: str\n...     age: int\n...     embedding: NDArray\n...\n&gt;&gt;&gt; @packable\n... class NameOnly:\n...     name: str\n...\n&gt;&gt;&gt; @lens\n... def name_view(full: FullData) -&gt; NameOnly:\n...     return NameOnly(name=full.name)\n...\n&gt;&gt;&gt; @name_view.putter\n... def name_view_put(view: NameOnly, source: FullData) -&gt; FullData:\n...     return FullData(name=view.name, age=source.age,\n...                     embedding=source.embedding)\n...\n&gt;&gt;&gt; ds = Dataset[FullData](\"data.tar\")\n&gt;&gt;&gt; ds_names = ds.as_type(NameOnly)  # Uses registered lens"
  },
  {
    "objectID": "api/Lens.html#classes",
    "href": "api/Lens.html#classes",
    "title": "lens",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nLens\nA bidirectional transformation between two sample types.\n\n\nLensNetwork\nGlobal registry for lens transformations between sample types.\n\n\n\n\n\nlens.Lens(get, put=None)\nA bidirectional transformation between two sample types.\nA lens provides a way to view and update data of type S (source) as if it were type V (view). It consists of a getter that transforms S -&gt; V and an optional putter that transforms (V, S) -&gt; S, enabling updates to the view to be reflected back in the source.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nS\n\nThe source type, must derive from PackableSample.\nrequired\n\n\nV\n\nThe view type, must derive from PackableSample.\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; @lens\n... def name_lens(full: FullData) -&gt; NameOnly:\n...     return NameOnly(name=full.name)\n...\n&gt;&gt;&gt; @name_lens.putter\n... def name_lens_put(view: NameOnly, source: FullData) -&gt; FullData:\n...     return FullData(name=view.name, age=source.age)\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget\nTransform the source into the view type.\n\n\nput\nUpdate the source based on a modified view.\n\n\nputter\nDecorator to register a putter function for this lens.\n\n\n\n\n\nlens.Lens.get(s)\nTransform the source into the view type.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ns\nS\nThe source sample of type S.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nV\nA view of the source as type V.\n\n\n\n\n\n\n\nlens.Lens.put(v, s)\nUpdate the source based on a modified view.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nv\nV\nThe modified view of type V.\nrequired\n\n\ns\nS\nThe original source of type S.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nS\nAn updated source of type S that reflects changes from the view.\n\n\n\n\n\n\n\nlens.Lens.putter(put)\nDecorator to register a putter function for this lens.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nput\nLensPutter[S, V]\nA function that takes a view of type V and source of type S, and returns an updated source of type S.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nLensPutter[S, V]\nThe putter function, allowing this to be used as a decorator.\n\n\n\n\n\n\n&gt;&gt;&gt; @my_lens.putter\n... def my_lens_put(view: ViewType, source: SourceType) -&gt; SourceType:\n...     return SourceType(field=view.field, other=source.other)\n\n\n\n\n\n\nlens.LensNetwork()\nGlobal registry for lens transformations between sample types.\nThis class implements a singleton pattern to maintain a global registry of all lenses decorated with @lens. It enables looking up transformations between different PackableSample types.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n_instance\n\nThe singleton instance of this class.\n\n\n_registry\nDict[LensSignature, Lens]\nDictionary mapping (source_type, view_type) tuples to their corresponding Lens objects.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nregister\nRegister a lens as the canonical transformation between two types.\n\n\ntransform\nLook up the lens transformation between two sample types.\n\n\n\n\n\nlens.LensNetwork.register(_lens)\nRegister a lens as the canonical transformation between two types.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n_lens\nLens\nThe lens to register. Will be stored in the registry under the key (_lens.source_type, _lens.view_type).\nrequired\n\n\n\n\n\n\nIf a lens already exists for the same type pair, it will be overwritten.\n\n\n\n\nlens.LensNetwork.transform(source, view)\nLook up the lens transformation between two sample types.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource\nDatasetType\nThe source sample type (must derive from PackableSample).\nrequired\n\n\nview\nDatasetType\nThe target view type (must derive from PackableSample).\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nLens\nThe registered Lens that transforms from source to view.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf no lens has been registered for the given type pair.\n\n\n\n\n\n\nCurrently only supports direct transformations. Compositional transformations (chaining multiple lenses) are not yet implemented."
  },
  {
    "objectID": "api/Lens.html#functions",
    "href": "api/Lens.html#functions",
    "title": "lens",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nlens\nDecorator to create and register a lens transformation.\n\n\n\n\n\nlens.lens(f)\nDecorator to create and register a lens transformation.\nThis decorator converts a getter function into a Lens object and automatically registers it in the global LensNetwork registry.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nf\nLensGetter[S, V]\nA getter function that transforms from source type S to view type V. Must have exactly one parameter with a type annotation.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nLens[S, V]\nA Lens[S, V] object that can be called to apply the transformation\n\n\n\nLens[S, V]\nor decorated with @lens_name.putter to add a putter function.\n\n\n\n\n\n\n&gt;&gt;&gt; @lens\n... def extract_name(full: FullData) -&gt; NameOnly:\n...     return NameOnly(name=full.name)\n...\n&gt;&gt;&gt; @extract_name.putter\n... def extract_name_put(view: NameOnly, source: FullData) -&gt; FullData:\n...     return FullData(name=view.name, age=source.age)"
  },
  {
    "objectID": "api/DatasetLoader.html",
    "href": "api/DatasetLoader.html",
    "title": "DatasetLoader",
    "section": "",
    "text": "atmosphere.DatasetLoader(client)\nLoads dataset records from ATProto.\nThis class fetches dataset index records and can create Dataset objects from them. Note that loading a dataset requires having the corresponding Python class for the sample type.\n\n\n&gt;&gt;&gt; atmo = Atmosphere.login(\"handle\", \"password\")\n&gt;&gt;&gt; loader = DatasetLoader(atmo)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # List available datasets\n&gt;&gt;&gt; datasets = loader.list()\n&gt;&gt;&gt; for ds in datasets:\n...     print(ds[\"name\"], ds[\"schemaRef\"])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get a specific dataset record\n&gt;&gt;&gt; record = loader.get(\"at://did:plc:abc/ac.foundation.dataset.record/xyz\")\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget\nFetch a dataset record by AT URI.\n\n\nget_blob_urls\nGet fetchable URLs for blob-stored dataset shards.\n\n\nget_blobs\nGet the blob references from a dataset record.\n\n\nget_metadata\nGet the metadata from a dataset record.\n\n\nget_storage_type\nGet the storage type of a dataset record.\n\n\nget_urls\nGet the WebDataset URLs from a dataset record.\n\n\nlist_all\nList dataset records from a repository.\n\n\nto_dataset\nCreate a Dataset object from an ATProto record.\n\n\n\n\n\natmosphere.DatasetLoader.get(uri)\nFetch a dataset record by AT URI.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr | AtUri\nThe AT URI of the dataset record.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict\nThe dataset record as a dictionary.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the record is not a dataset record.\n\n\n\n\n\n\n\natmosphere.DatasetLoader.get_blob_urls(uri)\nGet fetchable URLs for blob-stored dataset shards.\nThis resolves the PDS endpoint and constructs URLs that can be used to fetch the blob data directly.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr | AtUri\nThe AT URI of the dataset record.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[str]\nList of URLs for fetching the blob data.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf storage type is not blobs or PDS cannot be resolved.\n\n\n\n\n\n\n\natmosphere.DatasetLoader.get_blobs(uri)\nGet the blob references from a dataset record.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr | AtUri\nThe AT URI of the dataset record.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[dict]\nList of blob reference dicts with keys: $type, ref, mimeType, size.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the storage type is not blobs.\n\n\n\n\n\n\n\natmosphere.DatasetLoader.get_metadata(uri)\nGet the metadata from a dataset record.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr | AtUri\nThe AT URI of the dataset record.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nOptional[dict]\nThe metadata dictionary, or None if no metadata.\n\n\n\n\n\n\n\natmosphere.DatasetLoader.get_storage_type(uri)\nGet the storage type of a dataset record.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr | AtUri\nThe AT URI of the dataset record.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nEither “external” or “blobs”.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf storage type is unknown.\n\n\n\n\n\n\n\natmosphere.DatasetLoader.get_urls(uri)\nGet the WebDataset URLs from a dataset record.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr | AtUri\nThe AT URI of the dataset record.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[str]\nList of WebDataset URLs.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the storage type is not external URLs.\n\n\n\n\n\n\n\natmosphere.DatasetLoader.list_all(repo=None, limit=100)\nList dataset records from a repository.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrepo\nOptional[str]\nThe DID of the repository. Defaults to authenticated user.\nNone\n\n\nlimit\nint\nMaximum number of records to return.\n100\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[dict]\nList of dataset records.\n\n\n\n\n\n\n\natmosphere.DatasetLoader.to_dataset(uri, sample_type)\nCreate a Dataset object from an ATProto record.\nThis method creates a Dataset instance from a published record. You must provide the sample type class, which should match the schema referenced by the record.\nSupports both external URL storage and ATProto blob storage.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr | AtUri\nThe AT URI of the dataset record.\nrequired\n\n\nsample_type\nType[ST]\nThe Python class for the sample type.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset[ST]\nA Dataset instance configured from the record.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf no storage URLs can be resolved.\n\n\n\n\n\n\n&gt;&gt;&gt; loader = DatasetLoader(client)\n&gt;&gt;&gt; dataset = loader.to_dataset(uri, MySampleType)\n&gt;&gt;&gt; for batch in dataset.shuffled(batch_size=32):\n...     process(batch)"
  },
  {
    "objectID": "api/DatasetLoader.html#examples",
    "href": "api/DatasetLoader.html#examples",
    "title": "DatasetLoader",
    "section": "",
    "text": "&gt;&gt;&gt; atmo = Atmosphere.login(\"handle\", \"password\")\n&gt;&gt;&gt; loader = DatasetLoader(atmo)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # List available datasets\n&gt;&gt;&gt; datasets = loader.list()\n&gt;&gt;&gt; for ds in datasets:\n...     print(ds[\"name\"], ds[\"schemaRef\"])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get a specific dataset record\n&gt;&gt;&gt; record = loader.get(\"at://did:plc:abc/ac.foundation.dataset.record/xyz\")"
  },
  {
    "objectID": "api/DatasetLoader.html#methods",
    "href": "api/DatasetLoader.html#methods",
    "title": "DatasetLoader",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget\nFetch a dataset record by AT URI.\n\n\nget_blob_urls\nGet fetchable URLs for blob-stored dataset shards.\n\n\nget_blobs\nGet the blob references from a dataset record.\n\n\nget_metadata\nGet the metadata from a dataset record.\n\n\nget_storage_type\nGet the storage type of a dataset record.\n\n\nget_urls\nGet the WebDataset URLs from a dataset record.\n\n\nlist_all\nList dataset records from a repository.\n\n\nto_dataset\nCreate a Dataset object from an ATProto record.\n\n\n\n\n\natmosphere.DatasetLoader.get(uri)\nFetch a dataset record by AT URI.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr | AtUri\nThe AT URI of the dataset record.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict\nThe dataset record as a dictionary.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the record is not a dataset record.\n\n\n\n\n\n\n\natmosphere.DatasetLoader.get_blob_urls(uri)\nGet fetchable URLs for blob-stored dataset shards.\nThis resolves the PDS endpoint and constructs URLs that can be used to fetch the blob data directly.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr | AtUri\nThe AT URI of the dataset record.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[str]\nList of URLs for fetching the blob data.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf storage type is not blobs or PDS cannot be resolved.\n\n\n\n\n\n\n\natmosphere.DatasetLoader.get_blobs(uri)\nGet the blob references from a dataset record.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr | AtUri\nThe AT URI of the dataset record.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[dict]\nList of blob reference dicts with keys: $type, ref, mimeType, size.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the storage type is not blobs.\n\n\n\n\n\n\n\natmosphere.DatasetLoader.get_metadata(uri)\nGet the metadata from a dataset record.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr | AtUri\nThe AT URI of the dataset record.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nOptional[dict]\nThe metadata dictionary, or None if no metadata.\n\n\n\n\n\n\n\natmosphere.DatasetLoader.get_storage_type(uri)\nGet the storage type of a dataset record.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr | AtUri\nThe AT URI of the dataset record.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nEither “external” or “blobs”.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf storage type is unknown.\n\n\n\n\n\n\n\natmosphere.DatasetLoader.get_urls(uri)\nGet the WebDataset URLs from a dataset record.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr | AtUri\nThe AT URI of the dataset record.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[str]\nList of WebDataset URLs.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the storage type is not external URLs.\n\n\n\n\n\n\n\natmosphere.DatasetLoader.list_all(repo=None, limit=100)\nList dataset records from a repository.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrepo\nOptional[str]\nThe DID of the repository. Defaults to authenticated user.\nNone\n\n\nlimit\nint\nMaximum number of records to return.\n100\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[dict]\nList of dataset records.\n\n\n\n\n\n\n\natmosphere.DatasetLoader.to_dataset(uri, sample_type)\nCreate a Dataset object from an ATProto record.\nThis method creates a Dataset instance from a published record. You must provide the sample type class, which should match the schema referenced by the record.\nSupports both external URL storage and ATProto blob storage.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr | AtUri\nThe AT URI of the dataset record.\nrequired\n\n\nsample_type\nType[ST]\nThe Python class for the sample type.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset[ST]\nA Dataset instance configured from the record.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf no storage URLs can be resolved.\n\n\n\n\n\n\n&gt;&gt;&gt; loader = DatasetLoader(client)\n&gt;&gt;&gt; dataset = loader.to_dataset(uri, MySampleType)\n&gt;&gt;&gt; for batch in dataset.shuffled(batch_size=32):\n...     process(batch)"
  },
  {
    "objectID": "api/DataSource.html",
    "href": "api/DataSource.html",
    "title": "DataSource",
    "section": "",
    "text": "DataSource()\nProtocol for data sources that stream shard data to Dataset.\nImplementations (URLSource, S3Source, BlobSource) yield (identifier, stream) pairs fed to WebDataset’s tar expander, bypassing URL resolution. This enables private S3, custom endpoints, and ATProto blob streaming.\n\n\n&gt;&gt;&gt; source = S3Source(bucket=\"my-bucket\", keys=[\"data-000.tar\"])\n&gt;&gt;&gt; ds = Dataset[MySample](source)\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nshards\nLazily yield (shard_id, stream) pairs for each shard.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nlist_shards\nShard identifiers without opening streams.\n\n\nopen_shard\nOpen a single shard for random access (e.g., DataLoader splitting).\n\n\n\n\n\nDataSource.list_shards()\nShard identifiers without opening streams.\n\n\n\nDataSource.open_shard(shard_id)\nOpen a single shard for random access (e.g., DataLoader splitting).\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf shard_id is not in list_shards()."
  },
  {
    "objectID": "api/DataSource.html#examples",
    "href": "api/DataSource.html#examples",
    "title": "DataSource",
    "section": "",
    "text": "&gt;&gt;&gt; source = S3Source(bucket=\"my-bucket\", keys=[\"data-000.tar\"])\n&gt;&gt;&gt; ds = Dataset[MySample](source)"
  },
  {
    "objectID": "api/DataSource.html#attributes",
    "href": "api/DataSource.html#attributes",
    "title": "DataSource",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nshards\nLazily yield (shard_id, stream) pairs for each shard."
  },
  {
    "objectID": "api/DataSource.html#methods",
    "href": "api/DataSource.html#methods",
    "title": "DataSource",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nlist_shards\nShard identifiers without opening streams.\n\n\nopen_shard\nOpen a single shard for random access (e.g., DataLoader splitting).\n\n\n\n\n\nDataSource.list_shards()\nShard identifiers without opening streams.\n\n\n\nDataSource.open_shard(shard_id)\nOpen a single shard for random access (e.g., DataLoader splitting).\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf shard_id is not in list_shards()."
  },
  {
    "objectID": "api/AtmosphereIndex.html",
    "href": "api/AtmosphereIndex.html",
    "title": "AtmosphereIndex",
    "section": "",
    "text": "atmosphere.AtmosphereIndex(client, *, data_store=None)\nATProto index implementing AbstractIndex protocol.\n.. deprecated:: Use atdata.Index(atmosphere=client) instead. AtmosphereIndex is retained for backwards compatibility and will be removed in a future release.\nWraps SchemaPublisher/Loader and DatasetPublisher/Loader to provide a unified interface compatible with Index.\nOptionally accepts a PDSBlobStore for writing dataset shards as ATProto blobs, enabling fully decentralized dataset storage.\n\n\n&gt;&gt;&gt; # Preferred: use unified Index\n&gt;&gt;&gt; from atdata.local import Index\n&gt;&gt;&gt; from atdata.atmosphere import AtmosphereClient\n&gt;&gt;&gt; index = Index(atmosphere=client)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Legacy (deprecated)\n&gt;&gt;&gt; index = AtmosphereIndex(client)\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ndata_store\nThe PDS blob store for writing shards, or None if not configured.\n\n\ndatasets\nLazily iterate over all dataset entries (AbstractIndex protocol).\n\n\nschemas\nLazily iterate over all schema records (AbstractIndex protocol).\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ndecode_schema\nReconstruct a Python type from a schema record.\n\n\nget_dataset\nGet a dataset by AT URI.\n\n\nget_schema\nGet a schema record by AT URI.\n\n\ninsert_dataset\nInsert a dataset into ATProto.\n\n\nlist_datasets\nGet all dataset entries as a materialized list (AbstractIndex protocol).\n\n\nlist_schemas\nGet all schema records as a materialized list (AbstractIndex protocol).\n\n\npublish_schema\nPublish a schema to ATProto.\n\n\n\n\n\natmosphere.AtmosphereIndex.decode_schema(ref)\nReconstruct a Python type from a schema record.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nref\nstr\nAT URI of the schema record.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nType[Packable]\nDynamically generated Packable type.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf schema cannot be decoded.\n\n\n\n\n\n\n\natmosphere.AtmosphereIndex.get_dataset(ref)\nGet a dataset by AT URI.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nref\nstr\nAT URI of the dataset record.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAtmosphereIndexEntry\nAtmosphereIndexEntry for the dataset.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf record is not a dataset.\n\n\n\n\n\n\n\natmosphere.AtmosphereIndex.get_schema(ref)\nGet a schema record by AT URI.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nref\nstr\nAT URI of the schema record.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict\nSchema record dictionary.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf record is not a schema.\n\n\n\n\n\n\n\natmosphere.AtmosphereIndex.insert_dataset(\n    ds,\n    *,\n    name,\n    schema_ref=None,\n    **kwargs,\n)\nInsert a dataset into ATProto.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nds\nDataset\nThe Dataset to publish.\nrequired\n\n\nname\nstr\nHuman-readable name.\nrequired\n\n\nschema_ref\nOptional[str]\nOptional schema AT URI. If None, auto-publishes schema.\nNone\n\n\n**kwargs\n\nAdditional options (description, tags, license).\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAtmosphereIndexEntry\nAtmosphereIndexEntry for the inserted dataset.\n\n\n\n\n\n\n\natmosphere.AtmosphereIndex.list_datasets(repo=None)\nGet all dataset entries as a materialized list (AbstractIndex protocol).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrepo\nOptional[str]\nDID of repository. Defaults to authenticated user.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[AtmosphereIndexEntry]\nList of AtmosphereIndexEntry for each dataset.\n\n\n\n\n\n\n\natmosphere.AtmosphereIndex.list_schemas(repo=None)\nGet all schema records as a materialized list (AbstractIndex protocol).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrepo\nOptional[str]\nDID of repository. Defaults to authenticated user.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[dict]\nList of schema records as dictionaries.\n\n\n\n\n\n\n\natmosphere.AtmosphereIndex.publish_schema(\n    sample_type,\n    *,\n    version='1.0.0',\n    **kwargs,\n)\nPublish a schema to ATProto.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsample_type\nType[Packable]\nA Packable type (PackableSample subclass or @packable-decorated).\nrequired\n\n\nversion\nstr\nSemantic version string.\n'1.0.0'\n\n\n**kwargs\n\nAdditional options (description, metadata).\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nAT URI of the schema record."
  },
  {
    "objectID": "api/AtmosphereIndex.html#examples",
    "href": "api/AtmosphereIndex.html#examples",
    "title": "AtmosphereIndex",
    "section": "",
    "text": "&gt;&gt;&gt; # Preferred: use unified Index\n&gt;&gt;&gt; from atdata.local import Index\n&gt;&gt;&gt; from atdata.atmosphere import AtmosphereClient\n&gt;&gt;&gt; index = Index(atmosphere=client)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Legacy (deprecated)\n&gt;&gt;&gt; index = AtmosphereIndex(client)"
  },
  {
    "objectID": "api/AtmosphereIndex.html#attributes",
    "href": "api/AtmosphereIndex.html#attributes",
    "title": "AtmosphereIndex",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndata_store\nThe PDS blob store for writing shards, or None if not configured.\n\n\ndatasets\nLazily iterate over all dataset entries (AbstractIndex protocol).\n\n\nschemas\nLazily iterate over all schema records (AbstractIndex protocol)."
  },
  {
    "objectID": "api/AtmosphereIndex.html#methods",
    "href": "api/AtmosphereIndex.html#methods",
    "title": "AtmosphereIndex",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndecode_schema\nReconstruct a Python type from a schema record.\n\n\nget_dataset\nGet a dataset by AT URI.\n\n\nget_schema\nGet a schema record by AT URI.\n\n\ninsert_dataset\nInsert a dataset into ATProto.\n\n\nlist_datasets\nGet all dataset entries as a materialized list (AbstractIndex protocol).\n\n\nlist_schemas\nGet all schema records as a materialized list (AbstractIndex protocol).\n\n\npublish_schema\nPublish a schema to ATProto.\n\n\n\n\n\natmosphere.AtmosphereIndex.decode_schema(ref)\nReconstruct a Python type from a schema record.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nref\nstr\nAT URI of the schema record.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nType[Packable]\nDynamically generated Packable type.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf schema cannot be decoded.\n\n\n\n\n\n\n\natmosphere.AtmosphereIndex.get_dataset(ref)\nGet a dataset by AT URI.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nref\nstr\nAT URI of the dataset record.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAtmosphereIndexEntry\nAtmosphereIndexEntry for the dataset.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf record is not a dataset.\n\n\n\n\n\n\n\natmosphere.AtmosphereIndex.get_schema(ref)\nGet a schema record by AT URI.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nref\nstr\nAT URI of the schema record.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict\nSchema record dictionary.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf record is not a schema.\n\n\n\n\n\n\n\natmosphere.AtmosphereIndex.insert_dataset(\n    ds,\n    *,\n    name,\n    schema_ref=None,\n    **kwargs,\n)\nInsert a dataset into ATProto.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nds\nDataset\nThe Dataset to publish.\nrequired\n\n\nname\nstr\nHuman-readable name.\nrequired\n\n\nschema_ref\nOptional[str]\nOptional schema AT URI. If None, auto-publishes schema.\nNone\n\n\n**kwargs\n\nAdditional options (description, tags, license).\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAtmosphereIndexEntry\nAtmosphereIndexEntry for the inserted dataset.\n\n\n\n\n\n\n\natmosphere.AtmosphereIndex.list_datasets(repo=None)\nGet all dataset entries as a materialized list (AbstractIndex protocol).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrepo\nOptional[str]\nDID of repository. Defaults to authenticated user.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[AtmosphereIndexEntry]\nList of AtmosphereIndexEntry for each dataset.\n\n\n\n\n\n\n\natmosphere.AtmosphereIndex.list_schemas(repo=None)\nGet all schema records as a materialized list (AbstractIndex protocol).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrepo\nOptional[str]\nDID of repository. Defaults to authenticated user.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[dict]\nList of schema records as dictionaries.\n\n\n\n\n\n\n\natmosphere.AtmosphereIndex.publish_schema(\n    sample_type,\n    *,\n    version='1.0.0',\n    **kwargs,\n)\nPublish a schema to ATProto.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsample_type\nType[Packable]\nA Packable type (PackableSample subclass or @packable-decorated).\nrequired\n\n\nversion\nstr\nSemantic version string.\n'1.0.0'\n\n\n**kwargs\n\nAdditional options (description, metadata).\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nAT URI of the schema record."
  },
  {
    "objectID": "api/LensLoader.html",
    "href": "api/LensLoader.html",
    "title": "LensLoader",
    "section": "",
    "text": "atmosphere.LensLoader(client)\nLoads lens records from ATProto.\nThis class fetches lens transformation records. Note that actually using a lens requires installing the referenced code and importing it manually.\n\n\n&gt;&gt;&gt; atmo = Atmosphere.login(\"handle\", \"password\")\n&gt;&gt;&gt; loader = LensLoader(atmo)\n&gt;&gt;&gt;\n&gt;&gt;&gt; record = loader.get(\"at://did:plc:abc/ac.foundation.dataset.lens/xyz\")\n&gt;&gt;&gt; print(record[\"name\"])\n&gt;&gt;&gt; print(record[\"sourceSchema\"])\n&gt;&gt;&gt; print(record.get(\"getterCode\", {}).get(\"repository\"))\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfind_by_schemas\nFind lenses that transform between specific schemas.\n\n\nget\nFetch a lens record by AT URI.\n\n\nlist_all\nList lens records from a repository.\n\n\n\n\n\natmosphere.LensLoader.find_by_schemas(\n    source_schema_uri,\n    target_schema_uri=None,\n    repo=None,\n)\nFind lenses that transform between specific schemas.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource_schema_uri\nstr\nAT URI of the source schema.\nrequired\n\n\ntarget_schema_uri\nOptional[str]\nOptional AT URI of the target schema. If not provided, returns all lenses from the source.\nNone\n\n\nrepo\nOptional[str]\nThe DID of the repository to search.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[dict]\nList of matching lens records.\n\n\n\n\n\n\n\natmosphere.LensLoader.get(uri)\nFetch a lens record by AT URI.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr | AtUri\nThe AT URI of the lens record.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict\nThe lens record as a dictionary.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the record is not a lens record.\n\n\n\n\n\n\n\natmosphere.LensLoader.list_all(repo=None, limit=100)\nList lens records from a repository.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrepo\nOptional[str]\nThe DID of the repository. Defaults to authenticated user.\nNone\n\n\nlimit\nint\nMaximum number of records to return.\n100\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[dict]\nList of lens records."
  },
  {
    "objectID": "api/LensLoader.html#examples",
    "href": "api/LensLoader.html#examples",
    "title": "LensLoader",
    "section": "",
    "text": "&gt;&gt;&gt; atmo = Atmosphere.login(\"handle\", \"password\")\n&gt;&gt;&gt; loader = LensLoader(atmo)\n&gt;&gt;&gt;\n&gt;&gt;&gt; record = loader.get(\"at://did:plc:abc/ac.foundation.dataset.lens/xyz\")\n&gt;&gt;&gt; print(record[\"name\"])\n&gt;&gt;&gt; print(record[\"sourceSchema\"])\n&gt;&gt;&gt; print(record.get(\"getterCode\", {}).get(\"repository\"))"
  },
  {
    "objectID": "api/LensLoader.html#methods",
    "href": "api/LensLoader.html#methods",
    "title": "LensLoader",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfind_by_schemas\nFind lenses that transform between specific schemas.\n\n\nget\nFetch a lens record by AT URI.\n\n\nlist_all\nList lens records from a repository.\n\n\n\n\n\natmosphere.LensLoader.find_by_schemas(\n    source_schema_uri,\n    target_schema_uri=None,\n    repo=None,\n)\nFind lenses that transform between specific schemas.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource_schema_uri\nstr\nAT URI of the source schema.\nrequired\n\n\ntarget_schema_uri\nOptional[str]\nOptional AT URI of the target schema. If not provided, returns all lenses from the source.\nNone\n\n\nrepo\nOptional[str]\nThe DID of the repository to search.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[dict]\nList of matching lens records.\n\n\n\n\n\n\n\natmosphere.LensLoader.get(uri)\nFetch a lens record by AT URI.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr | AtUri\nThe AT URI of the lens record.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict\nThe lens record as a dictionary.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the record is not a lens record.\n\n\n\n\n\n\n\natmosphere.LensLoader.list_all(repo=None, limit=100)\nList lens records from a repository.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrepo\nOptional[str]\nThe DID of the repository. Defaults to authenticated user.\nNone\n\n\nlimit\nint\nMaximum number of records to return.\n100\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[dict]\nList of lens records."
  },
  {
    "objectID": "api/DictSample.html",
    "href": "api/DictSample.html",
    "title": "DictSample",
    "section": "",
    "text": "DictSample(_data=None, **kwargs)\nDynamic sample type providing dict-like access to raw msgpack data.\nThis class is the default sample type for datasets when no explicit type is specified. It stores the raw unpacked msgpack data and provides both attribute-style (sample.field) and dict-style (sample[\"field\"]) access to fields.\nDictSample is useful for: - Exploring datasets without defining a schema first - Working with datasets that have variable schemas - Prototyping before committing to a typed schema\nTo convert to a typed schema, use Dataset.as_type() with a @packable-decorated class. Every @packable class automatically registers a lens from DictSample, making this conversion seamless.\n\n\n&gt;&gt;&gt; ds = load_dataset(\"path/to/data.tar\")  # Returns Dataset[DictSample]\n&gt;&gt;&gt; for sample in ds.ordered():\n...     print(sample.some_field)      # Attribute access\n...     print(sample[\"other_field\"])  # Dict access\n...     print(sample.keys())          # Inspect available fields\n...\n&gt;&gt;&gt; # Convert to typed schema\n&gt;&gt;&gt; typed_ds = ds.as_type(MyTypedSample)\n\n\n\nNDArray fields are stored as raw bytes in DictSample. They are only converted to numpy arrays when accessed through a typed sample class.\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nas_wds\nSerialize for writing to WebDataset (__key__ + msgpack).\n\n\npacked\nSerialize to msgpack bytes.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfrom_bytes\nCreate a DictSample from raw msgpack bytes.\n\n\nfrom_data\nCreate a DictSample from unpacked msgpack data.\n\n\nget\nGet a field value, returning default if missing.\n\n\nkeys\nReturn list of field names.\n\n\nto_dict\nReturn a copy of the underlying data dictionary.\n\n\n\n\n\nDictSample.from_bytes(bs)\nCreate a DictSample from raw msgpack bytes.\n\n\n\nDictSample.from_data(data)\nCreate a DictSample from unpacked msgpack data.\n\n\n\nDictSample.get(key, default=None)\nGet a field value, returning default if missing.\n\n\n\nDictSample.keys()\nReturn list of field names.\n\n\n\nDictSample.to_dict()\nReturn a copy of the underlying data dictionary."
  },
  {
    "objectID": "api/DictSample.html#examples",
    "href": "api/DictSample.html#examples",
    "title": "DictSample",
    "section": "",
    "text": "&gt;&gt;&gt; ds = load_dataset(\"path/to/data.tar\")  # Returns Dataset[DictSample]\n&gt;&gt;&gt; for sample in ds.ordered():\n...     print(sample.some_field)      # Attribute access\n...     print(sample[\"other_field\"])  # Dict access\n...     print(sample.keys())          # Inspect available fields\n...\n&gt;&gt;&gt; # Convert to typed schema\n&gt;&gt;&gt; typed_ds = ds.as_type(MyTypedSample)"
  },
  {
    "objectID": "api/DictSample.html#note",
    "href": "api/DictSample.html#note",
    "title": "DictSample",
    "section": "",
    "text": "NDArray fields are stored as raw bytes in DictSample. They are only converted to numpy arrays when accessed through a typed sample class."
  },
  {
    "objectID": "api/DictSample.html#attributes",
    "href": "api/DictSample.html#attributes",
    "title": "DictSample",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nas_wds\nSerialize for writing to WebDataset (__key__ + msgpack).\n\n\npacked\nSerialize to msgpack bytes."
  },
  {
    "objectID": "api/DictSample.html#methods",
    "href": "api/DictSample.html#methods",
    "title": "DictSample",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfrom_bytes\nCreate a DictSample from raw msgpack bytes.\n\n\nfrom_data\nCreate a DictSample from unpacked msgpack data.\n\n\nget\nGet a field value, returning default if missing.\n\n\nkeys\nReturn list of field names.\n\n\nto_dict\nReturn a copy of the underlying data dictionary.\n\n\n\n\n\nDictSample.from_bytes(bs)\nCreate a DictSample from raw msgpack bytes.\n\n\n\nDictSample.from_data(data)\nCreate a DictSample from unpacked msgpack data.\n\n\n\nDictSample.get(key, default=None)\nGet a field value, returning default if missing.\n\n\n\nDictSample.keys()\nReturn list of field names.\n\n\n\nDictSample.to_dict()\nReturn a copy of the underlying data dictionary."
  },
  {
    "objectID": "api/PDSBlobStore.html",
    "href": "api/PDSBlobStore.html",
    "title": "PDSBlobStore",
    "section": "",
    "text": "atmosphere.PDSBlobStore(client)\nPDS blob store implementing AbstractDataStore protocol.\nStores dataset shards as ATProto blobs, enabling decentralized dataset storage on the AT Protocol network.\nEach shard is written to a temporary tar file, then uploaded as a blob to the user’s PDS. The returned URLs are AT URIs that can be resolved to HTTP URLs for streaming.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nclient\n'Atmosphere'\nAuthenticated Atmosphere instance.\n\n\n\n\n\n\n&gt;&gt;&gt; store = PDSBlobStore(client)\n&gt;&gt;&gt; urls = store.write_shards(dataset, prefix=\"training/v1\")\n&gt;&gt;&gt; # Returns AT URIs like:\n&gt;&gt;&gt; # ['at://did:plc:abc/blob/bafyrei...', ...]\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncreate_source\nCreate a BlobSource for reading these AT URIs.\n\n\nread_url\nResolve an AT URI blob reference to an HTTP URL.\n\n\nsupports_streaming\nPDS blobs support streaming via HTTP.\n\n\nwrite_shards\nWrite dataset shards as PDS blobs.\n\n\n\n\n\natmosphere.PDSBlobStore.create_source(urls)\nCreate a BlobSource for reading these AT URIs.\nThis is a convenience method for creating a DataSource that can stream the blobs written by this store.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nurls\nlist[str]\nList of AT URIs from write_shards().\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n'BlobSource'\nBlobSource configured for the given URLs.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf URLs are not valid AT URIs.\n\n\n\n\n\n\n\natmosphere.PDSBlobStore.read_url(url)\nResolve an AT URI blob reference to an HTTP URL.\nTransforms at://did/blob/cid URIs to HTTP URLs that can be streamed by WebDataset.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nurl\nstr\nAT URI in format at://{did}/blob/{cid}.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nHTTP URL for fetching the blob via PDS API.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf URL format is invalid or PDS cannot be resolved.\n\n\n\n\n\n\n\natmosphere.PDSBlobStore.supports_streaming()\nPDS blobs support streaming via HTTP.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nbool\nTrue.\n\n\n\n\n\n\n\natmosphere.PDSBlobStore.write_shards(\n    ds,\n    *,\n    prefix,\n    maxcount=10000,\n    maxsize=3000000000.0,\n    **kwargs,\n)\nWrite dataset shards as PDS blobs.\nCreates tar archives from the dataset and uploads each as a blob to the authenticated user’s PDS.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nds\n'Dataset'\nThe Dataset to write.\nrequired\n\n\nprefix\nstr\nLogical path prefix for naming (used in shard names only).\nrequired\n\n\nmaxcount\nint\nMaximum samples per shard (default: 10000).\n10000\n\n\nmaxsize\nfloat\nMaximum shard size in bytes (default: 3GB, PDS limit).\n3000000000.0\n\n\n**kwargs\nAny\nAdditional args passed to wds.ShardWriter.\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[str]\nList of AT URIs for the written blobs, in format:\n\n\n\nlist[str]\nat://{did}/blob/{cid}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf not authenticated.\n\n\n\nRuntimeError\nIf no shards were written.\n\n\n\n\n\n\nPDS blobs have size limits (typically 50MB-5GB depending on PDS). Adjust maxcount/maxsize to stay within limits."
  },
  {
    "objectID": "api/PDSBlobStore.html#attributes",
    "href": "api/PDSBlobStore.html#attributes",
    "title": "PDSBlobStore",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nclient\n'Atmosphere'\nAuthenticated Atmosphere instance."
  },
  {
    "objectID": "api/PDSBlobStore.html#examples",
    "href": "api/PDSBlobStore.html#examples",
    "title": "PDSBlobStore",
    "section": "",
    "text": "&gt;&gt;&gt; store = PDSBlobStore(client)\n&gt;&gt;&gt; urls = store.write_shards(dataset, prefix=\"training/v1\")\n&gt;&gt;&gt; # Returns AT URIs like:\n&gt;&gt;&gt; # ['at://did:plc:abc/blob/bafyrei...', ...]"
  },
  {
    "objectID": "api/PDSBlobStore.html#methods",
    "href": "api/PDSBlobStore.html#methods",
    "title": "PDSBlobStore",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncreate_source\nCreate a BlobSource for reading these AT URIs.\n\n\nread_url\nResolve an AT URI blob reference to an HTTP URL.\n\n\nsupports_streaming\nPDS blobs support streaming via HTTP.\n\n\nwrite_shards\nWrite dataset shards as PDS blobs.\n\n\n\n\n\natmosphere.PDSBlobStore.create_source(urls)\nCreate a BlobSource for reading these AT URIs.\nThis is a convenience method for creating a DataSource that can stream the blobs written by this store.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nurls\nlist[str]\nList of AT URIs from write_shards().\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n'BlobSource'\nBlobSource configured for the given URLs.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf URLs are not valid AT URIs.\n\n\n\n\n\n\n\natmosphere.PDSBlobStore.read_url(url)\nResolve an AT URI blob reference to an HTTP URL.\nTransforms at://did/blob/cid URIs to HTTP URLs that can be streamed by WebDataset.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nurl\nstr\nAT URI in format at://{did}/blob/{cid}.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nHTTP URL for fetching the blob via PDS API.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf URL format is invalid or PDS cannot be resolved.\n\n\n\n\n\n\n\natmosphere.PDSBlobStore.supports_streaming()\nPDS blobs support streaming via HTTP.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nbool\nTrue.\n\n\n\n\n\n\n\natmosphere.PDSBlobStore.write_shards(\n    ds,\n    *,\n    prefix,\n    maxcount=10000,\n    maxsize=3000000000.0,\n    **kwargs,\n)\nWrite dataset shards as PDS blobs.\nCreates tar archives from the dataset and uploads each as a blob to the authenticated user’s PDS.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nds\n'Dataset'\nThe Dataset to write.\nrequired\n\n\nprefix\nstr\nLogical path prefix for naming (used in shard names only).\nrequired\n\n\nmaxcount\nint\nMaximum samples per shard (default: 10000).\n10000\n\n\nmaxsize\nfloat\nMaximum shard size in bytes (default: 3GB, PDS limit).\n3000000000.0\n\n\n**kwargs\nAny\nAdditional args passed to wds.ShardWriter.\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[str]\nList of AT URIs for the written blobs, in format:\n\n\n\nlist[str]\nat://{did}/blob/{cid}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf not authenticated.\n\n\n\nRuntimeError\nIf no shards were written.\n\n\n\n\n\n\nPDS blobs have size limits (typically 50MB-5GB depending on PDS). Adjust maxcount/maxsize to stay within limits."
  },
  {
    "objectID": "api/PackableSample.html",
    "href": "api/PackableSample.html",
    "title": "PackableSample",
    "section": "",
    "text": "PackableSample()\nBase class for samples that can be serialized with msgpack.\nThis abstract base class provides automatic serialization/deserialization for dataclass-based samples. Fields annotated as NDArray or NDArray | None are automatically converted between numpy arrays and bytes during packing/unpacking.\nSubclasses should be defined either by: 1. Direct inheritance with the @dataclass decorator 2. Using the @packable decorator (recommended)\n\n\n&gt;&gt;&gt; @packable\n... class MyData:\n...     name: str\n...     embeddings: NDArray\n...\n&gt;&gt;&gt; sample = MyData(name=\"test\", embeddings=np.array([1.0, 2.0]))\n&gt;&gt;&gt; packed = sample.packed  # Serialize to bytes\n&gt;&gt;&gt; restored = MyData.from_bytes(packed)  # Deserialize\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nas_wds\nSerialize for writing to WebDataset (__key__ + msgpack).\n\n\npacked\nSerialize to msgpack bytes. NDArray fields are auto-converted.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfrom_bytes\nCreate an instance from raw msgpack bytes.\n\n\nfrom_data\nCreate an instance from unpacked msgpack data.\n\n\n\n\n\nPackableSample.from_bytes(bs)\nCreate an instance from raw msgpack bytes.\n\n\n\nPackableSample.from_data(data)\nCreate an instance from unpacked msgpack data."
  },
  {
    "objectID": "api/PackableSample.html#examples",
    "href": "api/PackableSample.html#examples",
    "title": "PackableSample",
    "section": "",
    "text": "&gt;&gt;&gt; @packable\n... class MyData:\n...     name: str\n...     embeddings: NDArray\n...\n&gt;&gt;&gt; sample = MyData(name=\"test\", embeddings=np.array([1.0, 2.0]))\n&gt;&gt;&gt; packed = sample.packed  # Serialize to bytes\n&gt;&gt;&gt; restored = MyData.from_bytes(packed)  # Deserialize"
  },
  {
    "objectID": "api/PackableSample.html#attributes",
    "href": "api/PackableSample.html#attributes",
    "title": "PackableSample",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nas_wds\nSerialize for writing to WebDataset (__key__ + msgpack).\n\n\npacked\nSerialize to msgpack bytes. NDArray fields are auto-converted."
  },
  {
    "objectID": "api/PackableSample.html#methods",
    "href": "api/PackableSample.html#methods",
    "title": "PackableSample",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfrom_bytes\nCreate an instance from raw msgpack bytes.\n\n\nfrom_data\nCreate an instance from unpacked msgpack data.\n\n\n\n\n\nPackableSample.from_bytes(bs)\nCreate an instance from raw msgpack bytes.\n\n\n\nPackableSample.from_data(data)\nCreate an instance from unpacked msgpack data."
  },
  {
    "objectID": "api/DatasetDict.html",
    "href": "api/DatasetDict.html",
    "title": "DatasetDict",
    "section": "",
    "text": "DatasetDict(splits=None, sample_type=None, streaming=False)\nA dictionary of split names to Dataset instances.\nSimilar to HuggingFace’s DatasetDict, this provides a container for multiple dataset splits (train, test, validation, etc.) with convenience methods that operate across all splits.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nST\n\nThe sample type for all datasets in this dict.\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; ds_dict = load_dataset(\"path/to/data\", MyData)\n&gt;&gt;&gt; train = ds_dict[\"train\"]\n&gt;&gt;&gt; test = ds_dict[\"test\"]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Iterate over all splits\n&gt;&gt;&gt; for split_name, dataset in ds_dict.items():\n...     print(f\"{split_name}: {len(dataset.list_shards())} shards\")\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nnum_shards\nNumber of shards in each split.\n\n\nsample_type\nThe sample type for datasets in this dict.\n\n\nstreaming\nWhether this DatasetDict was loaded in streaming mode."
  },
  {
    "objectID": "api/DatasetDict.html#parameters",
    "href": "api/DatasetDict.html#parameters",
    "title": "DatasetDict",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nST\n\nThe sample type for all datasets in this dict.\nrequired"
  },
  {
    "objectID": "api/DatasetDict.html#examples",
    "href": "api/DatasetDict.html#examples",
    "title": "DatasetDict",
    "section": "",
    "text": "&gt;&gt;&gt; ds_dict = load_dataset(\"path/to/data\", MyData)\n&gt;&gt;&gt; train = ds_dict[\"train\"]\n&gt;&gt;&gt; test = ds_dict[\"test\"]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Iterate over all splits\n&gt;&gt;&gt; for split_name, dataset in ds_dict.items():\n...     print(f\"{split_name}: {len(dataset.list_shards())} shards\")"
  },
  {
    "objectID": "api/DatasetDict.html#attributes",
    "href": "api/DatasetDict.html#attributes",
    "title": "DatasetDict",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nnum_shards\nNumber of shards in each split.\n\n\nsample_type\nThe sample type for datasets in this dict.\n\n\nstreaming\nWhether this DatasetDict was loaded in streaming mode."
  },
  {
    "objectID": "tutorials/promotion.html",
    "href": "tutorials/promotion.html",
    "title": "Promotion Workflow",
    "section": "",
    "text": "This tutorial demonstrates the workflow for migrating datasets from local Index-managed storage to the federated ATProto atmosphere network. Promotion is the bridge between Layer 2 (managed storage) and Layer 3 (federation).",
    "crumbs": [
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/promotion.html#why-promotion",
    "href": "tutorials/promotion.html#why-promotion",
    "title": "Promotion Workflow",
    "section": "Why Promotion?",
    "text": "Why Promotion?\nA common pattern in data science:\n\nStart private: Develop and validate datasets within your team\nGo public: Share successful datasets with the broader community\n\nPromotion handles this transition without re-processing your data. Instead of creating a new dataset from scratch, you’re lifting an existing local dataset entry into the federated atmosphere.\nThe workflow handles several complexities automatically:\n\nSchema deduplication: If you’ve already published the same schema type and version, promotion reuses it\nURL preservation: Data stays in place (unless you explicitly want to copy it)\nCID consistency: Content identifiers remain valid across the transition",
    "crumbs": [
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/promotion.html#overview",
    "href": "tutorials/promotion.html#overview",
    "title": "Promotion Workflow",
    "section": "Overview",
    "text": "Overview\nThe promotion workflow moves datasets from local storage to the atmosphere:\nLOCAL                           ATMOSPHERE\n-----                           ----------\nIndex (SQLite/Redis)            ATProto PDS\nLocalDiskStore / S3   --&gt;       (same storage or new location)\natdata://local/schema/...       at://did:plc:.../schema/...\nKey features:\n\nSchema deduplication: Won’t republish identical schemas\nFlexible data handling: Keep existing URLs or copy to new storage\nMetadata preservation: Local metadata carries over to atmosphere",
    "crumbs": [
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/promotion.html#setup",
    "href": "tutorials/promotion.html#setup",
    "title": "Promotion Workflow",
    "section": "Setup",
    "text": "Setup\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport atdata\nfrom atdata.atmosphere import Atmosphere",
    "crumbs": [
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/promotion.html#prepare-a-local-dataset",
    "href": "tutorials/promotion.html#prepare-a-local-dataset",
    "title": "Promotion Workflow",
    "section": "Prepare a Local Dataset",
    "text": "Prepare a Local Dataset\nFirst, set up a dataset in local storage using index.write():\n\n# 1. Define sample type\n@atdata.packable\nclass ExperimentSample:\n    \"\"\"A sample from a scientific experiment.\"\"\"\n    measurement: NDArray\n    timestamp: float\n    sensor_id: str\n\n# 2. Create samples\nsamples = [\n    ExperimentSample(\n        measurement=np.random.randn(64).astype(np.float32),\n        timestamp=float(i),\n        sensor_id=f\"sensor_{i % 4}\",\n    )\n    for i in range(1000)\n]\n\n# 3. Write through index (handles sharding, schema, and storage)\nindex = atdata.Index(data_store=atdata.LocalDiskStore())\nlocal_entry = index.write(samples, name=\"experiment-2024-001\", maxcount=500)\n\nprint(f\"Local entry name: {local_entry.name}\")\nprint(f\"Local entry CID: {local_entry.cid}\")\nprint(f\"Data URLs: {local_entry.data_urls}\")",
    "crumbs": [
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/promotion.html#basic-promotion",
    "href": "tutorials/promotion.html#basic-promotion",
    "title": "Promotion Workflow",
    "section": "Basic Promotion",
    "text": "Basic Promotion\nPromote the dataset to ATProto using index.promote_entry():\n\n# Connect to atmosphere and attach to the index\nclient = Atmosphere.login(\"myhandle.bsky.social\", \"app-password\")\nindex = atdata.Index(atmosphere=client, data_store=atdata.LocalDiskStore())\n\n# Promote by entry name\nat_uri = index.promote_entry(\"experiment-2024-001\")\nprint(f\"Published: {at_uri}\")",
    "crumbs": [
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/promotion.html#promotion-with-metadata",
    "href": "tutorials/promotion.html#promotion-with-metadata",
    "title": "Promotion Workflow",
    "section": "Promotion with Metadata",
    "text": "Promotion with Metadata\nAdd description, tags, and license:\n\nat_uri = index.promote_entry(\n    \"experiment-2024-001\",\n    name=\"experiment-2024-001-v2\",   # Override name\n    description=\"Sensor measurements from Lab 302\",\n    tags=[\"experiment\", \"physics\", \"2024\"],\n    license=\"CC-BY-4.0\",\n)\nprint(f\"Published with metadata: {at_uri}\")",
    "crumbs": [
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/promotion.html#schema-deduplication",
    "href": "tutorials/promotion.html#schema-deduplication",
    "title": "Promotion Workflow",
    "section": "Schema Deduplication",
    "text": "Schema Deduplication\nThe promotion workflow automatically checks for existing schemas on the atmosphere. When you promote multiple datasets with the same sample type, the schema is only published once:\n\n# First promotion: publishes schema to atmosphere\nuri1 = index.promote_entry(\"experiment-batch-1\")\n\n# Second promotion with same schema type + version: reuses existing schema\nuri2 = index.promote_entry(\"experiment-batch-2\")",
    "crumbs": [
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/promotion.html#data-migration-options",
    "href": "tutorials/promotion.html#data-migration-options",
    "title": "Promotion Workflow",
    "section": "Data Migration Options",
    "text": "Data Migration Options\n\nKeep Existing URLsCopy to New Storage\n\n\nBy default, promotion keeps the original data URLs:\n\n# Data stays in original storage location\nat_uri = index.promote_entry(\"experiment-2024-001\")\n\nBenefits:\n\nFastest option, no data copying\nDataset record points to existing URLs\nRequires original storage to remain accessible\n\n\n\nTo copy data to a different storage location, use promote_dataset() with a Dataset loaded from the entry’s URLs:\n\n# Load the dataset and promote directly\nentry = index.get_entry_by_name(\"experiment-2024-001\")\nds = atdata.Dataset[ExperimentSample](entry.data_urls[0])\n\nat_uri = index.promote_dataset(\n    ds,\n    name=\"experiment-2024-001\",\n    description=\"Sensor measurements from Lab 302\",\n)\n\nBenefits:\n\nData is copied to new bucket\nGood for moving from private to public storage\nOriginal storage can be retired",
    "crumbs": [
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/promotion.html#verify-on-atmosphere",
    "href": "tutorials/promotion.html#verify-on-atmosphere",
    "title": "Promotion Workflow",
    "section": "Verify on Atmosphere",
    "text": "Verify on Atmosphere\nAfter promotion, verify the dataset is accessible:\n\nentry = index.get_dataset(at_uri)\n\nprint(f\"Name: {entry.name}\")\nprint(f\"Schema: {entry.schema_ref}\")\nprint(f\"URLs: {entry.data_urls}\")\n\n# Load and iterate — schema auto-resolved\nds = atdata.load_dataset(at_uri, split=\"train\")\n\nfor batch in ds.ordered(batch_size=32):\n    print(f\"Measurement shape: {batch.measurement.shape}\")\n    break",
    "crumbs": [
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/promotion.html#error-handling",
    "href": "tutorials/promotion.html#error-handling",
    "title": "Promotion Workflow",
    "section": "Error Handling",
    "text": "Error Handling\n\ntry:\n    at_uri = index.promote_entry(\"experiment-2024-001\")\nexcept KeyError as e:\n    # Entry or schema not found in index\n    print(f\"Not found: {e}\")\nexcept ValueError as e:\n    # Entry has no data URLs or atmosphere not available\n    print(f\"Invalid state: {e}\")",
    "crumbs": [
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/promotion.html#requirements-checklist",
    "href": "tutorials/promotion.html#requirements-checklist",
    "title": "Promotion Workflow",
    "section": "Requirements Checklist",
    "text": "Requirements Checklist\nBefore promotion:\n\nDataset is in the index (via index.write() or index.insert_dataset())\nSchema is persisted (automatic when using index.write())\nAtmosphere client is authenticated and attached to the index\nData URLs are publicly accessible (or will be copied)",
    "crumbs": [
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/promotion.html#complete-workflow",
    "href": "tutorials/promotion.html#complete-workflow",
    "title": "Promotion Workflow",
    "section": "Complete Workflow",
    "text": "Complete Workflow\n\n# Complete local-to-atmosphere workflow\nimport numpy as np\nfrom numpy.typing import NDArray\nimport atdata\nfrom atdata.atmosphere import Atmosphere\n\n# 1. Define sample type\n@atdata.packable\nclass FeatureSample:\n    features: NDArray\n    label: int\n\n# 2. Create samples\nsamples = [\n    FeatureSample(\n        features=np.random.randn(128).astype(np.float32),\n        label=i % 10,\n    )\n    for i in range(1000)\n]\n\n# 3. Write through index (schema persisted automatically)\nindex = atdata.Index(data_store=atdata.LocalDiskStore())\nentry = index.write(samples, name=\"feature-vectors-v1\", maxcount=500)\n\n# 4. Promote to atmosphere\nclient = Atmosphere.login(\"myhandle.bsky.social\", \"app-password\")\nindex = atdata.Index(atmosphere=client, data_store=atdata.LocalDiskStore())\n\nat_uri = index.promote_entry(\n    \"feature-vectors-v1\",\n    description=\"Feature vectors for classification\",\n    tags=[\"features\", \"embeddings\"],\n    license=\"MIT\",\n)\n\nprint(f\"Dataset published: {at_uri}\")\n\n# 5. Others can now discover and load\n# ds = atdata.load_dataset(\"@myhandle.bsky.social/feature-vectors-v1\", split=\"train\")",
    "crumbs": [
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/promotion.html#what-youve-learned",
    "href": "tutorials/promotion.html#what-youve-learned",
    "title": "Promotion Workflow",
    "section": "What You’ve Learned",
    "text": "What You’ve Learned\nYou now understand the promotion workflow:\n\n\n\n\n\n\n\nConcept\nPurpose\n\n\n\n\nindex.promote_entry()\nLift local entries to federated network by name\n\n\nindex.promote_dataset()\nPromote a Dataset object directly\n\n\nSchema deduplication\nAvoid publishing duplicate schemas\n\n\nData URL preservation\nKeep data in place or copy to new storage\n\n\nMetadata enrichment\nAdd description, tags, license during promotion\n\n\n\nPromotion completes atdata’s three-layer story: you can now move seamlessly from local experimentation to team collaboration to public sharing, all with the same typed sample definitions.",
    "crumbs": [
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/promotion.html#the-complete-journey",
    "href": "tutorials/promotion.html#the-complete-journey",
    "title": "Promotion Workflow",
    "section": "The Complete Journey",
    "text": "The Complete Journey\n┌──────────────────┐   index.write  ┌──────────────────┐   promote    ┌──────────────────┐\n│  Local Files     │ ────────────→  │  Managed Storage │ ───────────→ │  Federation      │\n│                  │                │                  │              │                  │\n│  write_samples() │                │  Index (SQLite)  │              │  Index +         │\n│  Dataset[T]      │                │  LocalDiskStore  │              │  Atmosphere      │\n└──────────────────┘                └──────────────────┘              └──────────────────┘",
    "crumbs": [
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/promotion.html#next-steps",
    "href": "tutorials/promotion.html#next-steps",
    "title": "Promotion Workflow",
    "section": "Next Steps",
    "text": "Next Steps\n\nAtmosphere Reference - Complete atmosphere API\nProtocols - Abstract interfaces\nLocal Storage - Local storage reference",
    "crumbs": [
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/local-workflow.html",
    "href": "tutorials/local-workflow.html",
    "title": "Local Workflow",
    "section": "",
    "text": "This tutorial demonstrates how to use the Index class to store and manage datasets locally. This is Layer 2 of atdata’s architecture—managed storage that bridges local development and federated sharing.",
    "crumbs": [
      "Getting Started",
      "Local Workflow"
    ]
  },
  {
    "objectID": "tutorials/local-workflow.html#why-managed-storage",
    "href": "tutorials/local-workflow.html#why-managed-storage",
    "title": "Local Workflow",
    "section": "Why Managed Storage?",
    "text": "Why Managed Storage?\nLocal tar files work well for individual experiments, but growing projects need:\n\nDiscovery: “What datasets do we have? What schema does this one use?”\nConsistency: “Is everyone using the same version of this dataset?”\nDurability: “Where’s the canonical copy of our training data?”\n\natdata’s Index class addresses these needs with a two-component architecture:\n\n\n\n\n\n\n\nComponent\nPurpose\n\n\n\n\nIndex Provider\nMetadata queries, schema registry, dataset discovery (SQLite default, also Redis/PostgreSQL)\n\n\nData Store\nPersistent shard storage (LocalDiskStore or S3DataStore)\n\n\n\nThis separation means metadata operations (listing datasets, resolving schemas) are fast and don’t touch large data files, while the data itself lives in persistent storage.",
    "crumbs": [
      "Getting Started",
      "Local Workflow"
    ]
  },
  {
    "objectID": "tutorials/local-workflow.html#prerequisites",
    "href": "tutorials/local-workflow.html#prerequisites",
    "title": "Local Workflow",
    "section": "Prerequisites",
    "text": "Prerequisites\nFor the simplest setup, no external services are needed—Index() uses SQLite and LocalDiskStore by default.\nFor team-scale deployments you can optionally use:\n\nRedis or PostgreSQL as the index provider\nS3-compatible storage (MinIO, AWS S3, etc.) as the data store",
    "crumbs": [
      "Getting Started",
      "Local Workflow"
    ]
  },
  {
    "objectID": "tutorials/local-workflow.html#setup",
    "href": "tutorials/local-workflow.html#setup",
    "title": "Local Workflow",
    "section": "Setup",
    "text": "Setup\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport atdata",
    "crumbs": [
      "Getting Started",
      "Local Workflow"
    ]
  },
  {
    "objectID": "tutorials/local-workflow.html#define-sample-types",
    "href": "tutorials/local-workflow.html#define-sample-types",
    "title": "Local Workflow",
    "section": "Define Sample Types",
    "text": "Define Sample Types\n\n@atdata.packable\nclass TrainingSample:\n    \"\"\"A sample containing features and label for training.\"\"\"\n    features: NDArray\n    label: int\n\n@atdata.packable\nclass TextSample:\n    \"\"\"A sample containing text data.\"\"\"\n    text: str\n    category: str",
    "crumbs": [
      "Getting Started",
      "Local Workflow"
    ]
  },
  {
    "objectID": "tutorials/local-workflow.html#localdatasetentry",
    "href": "tutorials/local-workflow.html#localdatasetentry",
    "title": "Local Workflow",
    "section": "LocalDatasetEntry",
    "text": "LocalDatasetEntry\nEvery dataset in the index is represented by a LocalDatasetEntry. A key design decision: entries use content-addressable CIDs (Content Identifiers) as their identity. This means:\n\nIdentical content always has the same CID\nYou can verify data integrity by checking the CID\nDeduplication happens automatically\n\nCIDs are computed from the entry’s schema reference and data URLs, so the same logical dataset will have the same CID regardless of where it’s stored.\nCreate entries with content-addressable CIDs:\n\n# Create an entry manually\nentry = LocalDatasetEntry(\n    _name=\"my-dataset\",\n    _schema_ref=\"local://schemas/examples.TrainingSample@1.0.0\",\n    _data_urls=[\"s3://bucket/data-000000.tar\", \"s3://bucket/data-000001.tar\"],\n    _metadata={\"source\": \"example\", \"samples\": 10000},\n)\n\nprint(f\"Entry name: {entry.name}\")\nprint(f\"Schema ref: {entry.schema_ref}\")\nprint(f\"Data URLs: {entry.data_urls}\")\nprint(f\"Metadata: {entry.metadata}\")\nprint(f\"CID: {entry.cid}\")\n\n\n\n\n\n\n\nNote\n\n\n\nCIDs are generated from content (schema_ref + data_urls), so identical data produces identical CIDs.",
    "crumbs": [
      "Getting Started",
      "Local Workflow"
    ]
  },
  {
    "objectID": "tutorials/local-workflow.html#the-index",
    "href": "tutorials/local-workflow.html#the-index",
    "title": "Local Workflow",
    "section": "The Index",
    "text": "The Index\nThe Index is your dataset registry. It implements the AbstractIndex protocol, meaning code written against Index will also work with atmosphere backends when you’re ready for federated sharing.\nBy default, Index() uses SQLite (zero external dependencies):\n\n# Zero-config: SQLite in memory\nindex = atdata.Index()\n\n# Or persist to disk\nindex = atdata.Index(path=\"~/.atdata/index.db\")\n\n# With a data store for shard storage\nindex = atdata.Index(\n    data_store=atdata.LocalDiskStore(root=\"~/.atdata/data/\"),\n)\n\nFor team deployments, swap in Redis or PostgreSQL:\n\nfrom redis import Redis\n\nindex = atdata.Index(redis=Redis(host=\"localhost\", port=6379))\n# or\nindex = atdata.Index(provider=\"postgres\", dsn=\"postgresql://user:pass@host/db\")\n\n\nSchema Management\nSchema publishing is how you ensure type consistency across your team. When you publish a schema, atdata stores the complete type definition (field names, types, metadata) so anyone can reconstruct the Python class from just the schema reference.\nWhen you use index.write(), schemas are persisted automatically—no separate publish_schema() call needed. Consumers can load datasets by name and the sample type is reconstructed from the stored schema.\n\n# Explicit schema publishing (optional — index.write() does this for you)\nschema_ref = index.publish_schema(TrainingSample, version=\"1.0.0\")\nprint(f\"Published schema: {schema_ref}\")\n\n# List all schemas\nfor schema in index.list_schemas():\n    print(f\"  - {schema.get('name', 'Unknown')} v{schema.get('version', '?')}\")\n\n# Decode schema back to a PackableSample class\ndecoded_type = index.decode_schema(schema_ref)\nprint(f\"Decoded type: {decoded_type.__name__}\")",
    "crumbs": [
      "Getting Started",
      "Local Workflow"
    ]
  },
  {
    "objectID": "tutorials/local-workflow.html#data-stores",
    "href": "tutorials/local-workflow.html#data-stores",
    "title": "Local Workflow",
    "section": "Data Stores",
    "text": "Data Stores\nData stores implement the AbstractDataStore protocol for persistent shard storage:\n\nLocalDiskStore\nThe simplest option—stores shards on the local filesystem:\n\nstore = atdata.LocalDiskStore(root=\"~/.atdata/data/\")\n\n# Or accept the default root (~/.atdata/data/)\nstore = atdata.LocalDiskStore()\n\n\n\nS3DataStore\nFor team-scale storage with S3-compatible backends (AWS S3, MinIO, Cloudflare R2):\n\nfrom atdata.stores import S3DataStore\n\nstore = S3DataStore(\n    credentials={\n        \"AWS_ENDPOINT\": \"http://localhost:9000\",\n        \"AWS_ACCESS_KEY_ID\": \"minioadmin\",\n        \"AWS_SECRET_ACCESS_KEY\": \"minioadmin\",\n    },\n    bucket=\"my-bucket\",\n)",
    "crumbs": [
      "Getting Started",
      "Local Workflow"
    ]
  },
  {
    "objectID": "tutorials/local-workflow.html#complete-index-workflow",
    "href": "tutorials/local-workflow.html#complete-index-workflow",
    "title": "Local Workflow",
    "section": "Complete Index Workflow",
    "text": "Complete Index Workflow\nThe typical workflow has just three steps:\n\nCreate samples using your @packable type\nWrite through the index (index.write() handles sharding, storage, schema persistence, and indexing)\nLoad by name (load_dataset() auto-resolves schema and data URLs)\n\n\n# 1. Create sample data\nsamples = [\n    TrainingSample(\n        features=np.random.randn(128).astype(np.float32),\n        label=i % 10\n    )\n    for i in range(1000)\n]\n\n# 2. Write through the index (schema is persisted automatically)\nindex = atdata.Index(data_store=atdata.LocalDiskStore())\nentry = index.write(samples, name=\"training-v1\", maxcount=500)\nprint(f\"CID: {entry.cid}\")\nprint(f\"Shards: {entry.data_urls}\")\n\n# 3. Load by name — no sample_type needed\natdata.set_default_index(index)\nds = atdata.load_dataset(\"@local/training-v1\", split=\"train\")\n\nfor batch in ds.ordered(batch_size=32):\n    print(f\"Batch features shape: {batch.features.shape}\")\n    break\n\nFor S3-backed team storage, just swap the data store:\n\nfrom atdata.stores import S3DataStore\n\nstore = S3DataStore(\n    credentials={\n        \"AWS_ENDPOINT\": \"http://localhost:9000\",\n        \"AWS_ACCESS_KEY_ID\": \"minioadmin\",\n        \"AWS_SECRET_ACCESS_KEY\": \"minioadmin\",\n    },\n    bucket=\"team-datasets\",\n)\nindex = atdata.Index(data_store=store)\nentry = index.write(samples, name=\"training-v1\")",
    "crumbs": [
      "Getting Started",
      "Local Workflow"
    ]
  },
  {
    "objectID": "tutorials/local-workflow.html#using-load_dataset-with-index",
    "href": "tutorials/local-workflow.html#using-load_dataset-with-index",
    "title": "Local Workflow",
    "section": "Using load_dataset with Index",
    "text": "Using load_dataset with Index\nThe load_dataset() function provides a HuggingFace-style API that abstracts away the details of where data lives. It resolves @local/ prefixed paths to data URLs and auto-resolves the schema so you don’t need the original Python class.\n\nfrom atdata import load_dataset\n\n# Schema auto-resolved from index — no sample_type needed\nds = load_dataset(\"@local/training-v1\", split=\"train\")\n\nfor batch in ds.shuffled(batch_size=32):\n    print(f\"Features: {batch.features.shape}\")\n    break",
    "crumbs": [
      "Getting Started",
      "Local Workflow"
    ]
  },
  {
    "objectID": "tutorials/local-workflow.html#what-youve-learned",
    "href": "tutorials/local-workflow.html#what-youve-learned",
    "title": "Local Workflow",
    "section": "What You’ve Learned",
    "text": "What You’ve Learned\nYou now understand managed storage in atdata:\n\n\n\n\n\n\n\nConcept\nPurpose\n\n\n\n\nIndex\nDataset registry with pluggable providers (SQLite, Redis, PostgreSQL)\n\n\nindex.write()\nWrite samples + persist schema + create entry in one call\n\n\nLocalDiskStore / S3DataStore\nPersistent shard storage implementing AbstractDataStore\n\n\nload_dataset(\"@local/...\")\nLoad by name with auto-resolved schema\n\n\nLocalDatasetEntry\nContent-addressed dataset entries with CIDs\n\n\n\nThe same sample types you defined in the Quick Start work seamlessly here—the only change is where the data lives.",
    "crumbs": [
      "Getting Started",
      "Local Workflow"
    ]
  },
  {
    "objectID": "tutorials/local-workflow.html#next-steps",
    "href": "tutorials/local-workflow.html#next-steps",
    "title": "Local Workflow",
    "section": "Next Steps",
    "text": "Next Steps\n\n\n\n\n\n\nReady for Public Sharing?\n\n\n\nThe Atmosphere Publishing tutorial shows how to publish datasets to the ATProto network for decentralized, cross-organization discovery.\n\n\n\nAtmosphere Publishing - Publish to ATProto federation\nPromotion Workflow - Migrate from local to atmosphere\nLocal Storage Reference - Complete API reference",
    "crumbs": [
      "Getting Started",
      "Local Workflow"
    ]
  },
  {
    "objectID": "reference/promotion.html",
    "href": "reference/promotion.html",
    "title": "Promotion Workflow",
    "section": "",
    "text": "The promotion workflow migrates datasets from local storage (Redis + S3) to the ATProto atmosphere network, enabling federation and discovery.",
    "crumbs": [
      "Reference",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "reference/promotion.html#overview",
    "href": "reference/promotion.html#overview",
    "title": "Promotion Workflow",
    "section": "Overview",
    "text": "Overview\nPromotion handles:\n\nSchema deduplication: Avoids publishing duplicate schemas\nData URL preservation: Keeps existing S3 URLs or copies to new storage\nMetadata transfer: Preserves tags, descriptions, and custom metadata",
    "crumbs": [
      "Reference",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "reference/promotion.html#basic-usage",
    "href": "reference/promotion.html#basic-usage",
    "title": "Promotion Workflow",
    "section": "Basic Usage",
    "text": "Basic Usage\n\nfrom atdata.local import LocalIndex\nfrom atdata.atmosphere import AtmosphereClient\nfrom atdata.promote import promote_to_atmosphere\n\n# Setup\nlocal_index = LocalIndex()\nclient = AtmosphereClient()\nclient.login(\"handle.bsky.social\", \"app-password\")\n\n# Get local entry\nentry = local_index.get_entry_by_name(\"my-dataset\")\n\n# Promote to atmosphere\nat_uri = promote_to_atmosphere(entry, local_index, client)\nprint(f\"Published: {at_uri}\")",
    "crumbs": [
      "Reference",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "reference/promotion.html#with-metadata",
    "href": "reference/promotion.html#with-metadata",
    "title": "Promotion Workflow",
    "section": "With Metadata",
    "text": "With Metadata\n\nat_uri = promote_to_atmosphere(\n    entry,\n    local_index,\n    client,\n    name=\"my-dataset-v2\",           # Override name\n    description=\"Training images\",  # Add description\n    tags=[\"images\", \"training\"],    # Add discovery tags\n    license=\"MIT\",                  # Specify license\n)",
    "crumbs": [
      "Reference",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "reference/promotion.html#schema-deduplication",
    "href": "reference/promotion.html#schema-deduplication",
    "title": "Promotion Workflow",
    "section": "Schema Deduplication",
    "text": "Schema Deduplication\nThe promotion workflow automatically checks for existing schemas:\n\n# First promotion: publishes schema\nuri1 = promote_to_atmosphere(entry1, local_index, client)\n\n# Second promotion with same schema type + version: reuses existing schema\nuri2 = promote_to_atmosphere(entry2, local_index, client)\n\nSchema matching is based on:\n\n{module}.{class_name} (e.g., mymodule.ImageSample)\nVersion string (e.g., 1.0.0)",
    "crumbs": [
      "Reference",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "reference/promotion.html#data-storage-options",
    "href": "reference/promotion.html#data-storage-options",
    "title": "Promotion Workflow",
    "section": "Data Storage Options",
    "text": "Data Storage Options\n\nKeep Existing URLs (Default)Copy to New Storage\n\n\nBy default, promotion keeps the original data URLs:\n\n# Data stays in original S3 location\nat_uri = promote_to_atmosphere(entry, local_index, client)\n\n\nData stays in original S3 location\nDataset record points to existing URLs\nFastest option, no data copying\nRequires original storage to remain accessible\n\n\n\nTo copy data to a different storage location:\n\nfrom atdata.local import S3DataStore\n\n# Create new data store\nnew_store = S3DataStore(\n    credentials=\"new-s3-creds.env\",\n    bucket=\"public-datasets\",\n)\n\n# Promote with data copy\nat_uri = promote_to_atmosphere(\n    entry,\n    local_index,\n    client,\n    data_store=new_store,  # Copy data to new storage\n)\n\n\nData is copied to new bucket\nDataset record points to new URLs\nGood for moving from private to public storage",
    "crumbs": [
      "Reference",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "reference/promotion.html#complete-workflow-example",
    "href": "reference/promotion.html#complete-workflow-example",
    "title": "Promotion Workflow",
    "section": "Complete Workflow Example",
    "text": "Complete Workflow Example\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport atdata\nfrom atdata.local import LocalIndex, S3DataStore\nfrom atdata.atmosphere import AtmosphereClient\nfrom atdata.promote import promote_to_atmosphere\nimport webdataset as wds\n\n# 1. Define sample type\n@atdata.packable\nclass FeatureSample:\n    features: NDArray\n    label: int\n\n# 2. Create local dataset\nsamples = [\n    FeatureSample(\n        features=np.random.randn(128).astype(np.float32),\n        label=i % 10,\n    )\n    for i in range(1000)\n]\n\nwith wds.writer.TarWriter(\"features.tar\") as sink:\n    for i, s in enumerate(samples):\n        sink.write({**s.as_wds, \"__key__\": f\"{i:06d}\"})\n\n# 3. Set up index with S3 data store\nstore = S3DataStore(\n    credentials={\n        \"AWS_ENDPOINT\": \"http://localhost:9000\",\n        \"AWS_ACCESS_KEY_ID\": \"minioadmin\",\n        \"AWS_SECRET_ACCESS_KEY\": \"minioadmin\",\n    },\n    bucket=\"datasets-bucket\",\n)\nlocal_index = LocalIndex(data_store=store)\n\n# 4. Publish schema and insert dataset\nlocal_index.publish_schema(FeatureSample, version=\"1.0.0\")\ndataset = atdata.Dataset[FeatureSample](\"features.tar\")\nlocal_entry = local_index.insert_dataset(dataset, name=\"feature-vectors-v1\", prefix=\"features\")\n\n# 5. Promote to atmosphere\nclient = AtmosphereClient()\nclient.login(\"myhandle.bsky.social\", \"app-password\")\n\nat_uri = promote_to_atmosphere(\n    local_entry,\n    local_index,\n    client,\n    description=\"Feature vectors for classification\",\n    tags=[\"features\", \"embeddings\"],\n    license=\"MIT\",\n)\n\nprint(f\"Dataset published: {at_uri}\")\n\n# 6. Verify on atmosphere\nfrom atdata.atmosphere import AtmosphereIndex\n\natm_index = AtmosphereIndex(client)\nentry = atm_index.get_dataset(at_uri)\nprint(f\"Name: {entry.name}\")\nprint(f\"Schema: {entry.schema_ref}\")\nprint(f\"URLs: {entry.data_urls}\")",
    "crumbs": [
      "Reference",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "reference/promotion.html#error-handling",
    "href": "reference/promotion.html#error-handling",
    "title": "Promotion Workflow",
    "section": "Error Handling",
    "text": "Error Handling\n\ntry:\n    at_uri = promote_to_atmosphere(entry, local_index, client)\nexcept KeyError as e:\n    # Schema not found in local index\n    print(f\"Missing schema: {e}\")\nexcept ValueError as e:\n    # Entry has no data URLs\n    print(f\"Invalid entry: {e}\")",
    "crumbs": [
      "Reference",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "reference/promotion.html#requirements",
    "href": "reference/promotion.html#requirements",
    "title": "Promotion Workflow",
    "section": "Requirements",
    "text": "Requirements\nBefore promotion:\n\nDataset must be in local index (via Index.insert_dataset() or Index.add_entry())\nSchema must be published to local index (via Index.publish_schema())\nAtmosphereClient must be authenticated",
    "crumbs": [
      "Reference",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "reference/promotion.html#related",
    "href": "reference/promotion.html#related",
    "title": "Promotion Workflow",
    "section": "Related",
    "text": "Related\n\nLocal Storage - Setting up local datasets\nAtmosphere - ATProto integration\nProtocols - AbstractIndex and AbstractDataStore",
    "crumbs": [
      "Reference",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "reference/load-dataset.html",
    "href": "reference/load-dataset.html",
    "title": "load_dataset API",
    "section": "",
    "text": "The load_dataset() function provides a HuggingFace Datasets-style interface for loading typed datasets.",
    "crumbs": [
      "Reference",
      "load_dataset API"
    ]
  },
  {
    "objectID": "reference/load-dataset.html#overview",
    "href": "reference/load-dataset.html#overview",
    "title": "load_dataset API",
    "section": "Overview",
    "text": "Overview\nKey differences from HuggingFace Datasets:\n\nRequires explicit sample_type parameter (typed dataclass) unless using index\nReturns atdata.Dataset[ST] instead of HF Dataset\nBuilt on WebDataset for efficient streaming\nNo Arrow caching layer",
    "crumbs": [
      "Reference",
      "load_dataset API"
    ]
  },
  {
    "objectID": "reference/load-dataset.html#basic-usage",
    "href": "reference/load-dataset.html#basic-usage",
    "title": "load_dataset API",
    "section": "Basic Usage",
    "text": "Basic Usage\n\nimport atdata\nfrom atdata import load_dataset\nfrom numpy.typing import NDArray\n\n@atdata.packable\nclass TextSample:\n    text: str\n    label: int\n\n# Load a specific split\ntrain_ds = load_dataset(\"path/to/data.tar\", TextSample, split=\"train\")\n\n# Load all splits (returns DatasetDict)\nds_dict = load_dataset(\"path/to/data/\", TextSample)\ntrain_ds = ds_dict[\"train\"]\ntest_ds = ds_dict[\"test\"]",
    "crumbs": [
      "Reference",
      "load_dataset API"
    ]
  },
  {
    "objectID": "reference/load-dataset.html#path-formats",
    "href": "reference/load-dataset.html#path-formats",
    "title": "load_dataset API",
    "section": "Path Formats",
    "text": "Path Formats\n\nWebDataset Brace Notation\n\n# Range notation\nds = load_dataset(\"data-{000000..000099}.tar\", MySample, split=\"train\")\n\n# List notation\nds = load_dataset(\"data-{train,test,val}.tar\", MySample, split=\"train\")\n\n\n\nGlob Patterns\n\n# Match all tar files\nds = load_dataset(\"path/to/*.tar\", MySample)\n\n# Match pattern\nds = load_dataset(\"path/to/train-*.tar\", MySample, split=\"train\")\n\n\n\nLocal Directory\n\n# Scans for .tar files\nds = load_dataset(\"./my-dataset/\", MySample)\n\n\n\nRemote URLs\n\n# S3 (public buckets)\nds = load_dataset(\"s3://bucket/data-{000..099}.tar\", MySample, split=\"train\")\n\n# HTTP/HTTPS\nds = load_dataset(\"https://example.com/data.tar\", MySample, split=\"train\")\n\n# Google Cloud Storage\nds = load_dataset(\"gs://bucket/data.tar\", MySample, split=\"train\")\n\n\n\n\n\n\n\nNote\n\n\n\nFor private S3 buckets or S3-compatible storage with authentication, use atdata.S3Source with Dataset directly. See Datasets for details.\n\n\n\n\nIndex Lookup\n\nfrom atdata.local import LocalIndex\n\nindex = LocalIndex()\n\n# Load from local index (auto-resolves type from schema)\nds = load_dataset(\"@local/my-dataset\", index=index, split=\"train\")\n\n# With explicit type\nds = load_dataset(\"@local/my-dataset\", MySample, index=index, split=\"train\")",
    "crumbs": [
      "Reference",
      "load_dataset API"
    ]
  },
  {
    "objectID": "reference/load-dataset.html#split-detection",
    "href": "reference/load-dataset.html#split-detection",
    "title": "load_dataset API",
    "section": "Split Detection",
    "text": "Split Detection\nSplits are automatically detected from filenames and directories:\n\n\n\nPattern\nDetected Split\n\n\n\n\ntrain-*.tar, training-*.tar\ntrain\n\n\ntest-*.tar, testing-*.tar\ntest\n\n\nval-*.tar, valid-*.tar, validation-*.tar\nvalidation\n\n\ndev-*.tar, development-*.tar\nvalidation\n\n\ntrain/*.tar (directory)\ntrain\n\n\ntest/*.tar (directory)\ntest\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFiles without a detected split default to “train”.",
    "crumbs": [
      "Reference",
      "load_dataset API"
    ]
  },
  {
    "objectID": "reference/load-dataset.html#datasetdict",
    "href": "reference/load-dataset.html#datasetdict",
    "title": "load_dataset API",
    "section": "DatasetDict",
    "text": "DatasetDict\nWhen loading without split=, returns a DatasetDict:\n\nds_dict = load_dataset(\"path/to/data/\", MySample)\n\n# Access splits\ntrain_ds = ds_dict[\"train\"]\ntest_ds = ds_dict[\"test\"]\n\n# Iterate splits\nfor name, dataset in ds_dict.items():\n    print(f\"{name}: {len(dataset.shard_list)} shards\")\n\n# Properties\nprint(ds_dict.num_shards)    # {'train': 10, 'test': 2}\nprint(ds_dict.sample_type)   # &lt;class 'MySample'&gt;\nprint(ds_dict.streaming)     # False",
    "crumbs": [
      "Reference",
      "load_dataset API"
    ]
  },
  {
    "objectID": "reference/load-dataset.html#explicit-data-files",
    "href": "reference/load-dataset.html#explicit-data-files",
    "title": "load_dataset API",
    "section": "Explicit Data Files",
    "text": "Explicit Data Files\nOverride automatic detection with data_files:\n\n# Single pattern\nds = load_dataset(\n    \"path/to/\",\n    MySample,\n    data_files=\"custom-*.tar\",\n)\n\n# List of patterns\nds = load_dataset(\n    \"path/to/\",\n    MySample,\n    data_files=[\"shard-000.tar\", \"shard-001.tar\"],\n)\n\n# Explicit split mapping\nds = load_dataset(\n    \"path/to/\",\n    MySample,\n    data_files={\n        \"train\": \"training-shards-*.tar\",\n        \"test\": \"eval-data.tar\",\n    },\n)",
    "crumbs": [
      "Reference",
      "load_dataset API"
    ]
  },
  {
    "objectID": "reference/load-dataset.html#streaming-mode",
    "href": "reference/load-dataset.html#streaming-mode",
    "title": "load_dataset API",
    "section": "Streaming Mode",
    "text": "Streaming Mode\nThe streaming parameter signals intent for streaming mode:\n\n# Mark as streaming\nds_dict = load_dataset(\"path/to/data.tar\", MySample, streaming=True)\n\n# Check streaming status\nif ds_dict.streaming:\n    print(\"Streaming mode\")\n\n\n\n\n\n\n\nTip\n\n\n\natdata datasets are always lazy/streaming via WebDataset pipelines. This parameter primarily signals intent.",
    "crumbs": [
      "Reference",
      "load_dataset API"
    ]
  },
  {
    "objectID": "reference/load-dataset.html#auto-type-resolution",
    "href": "reference/load-dataset.html#auto-type-resolution",
    "title": "load_dataset API",
    "section": "Auto Type Resolution",
    "text": "Auto Type Resolution\nWhen using index lookup, the sample type can be resolved automatically:\n\nfrom atdata.local import LocalIndex\n\nindex = LocalIndex()\n\n# No sample_type needed - resolved from schema\nds = load_dataset(\"@local/my-dataset\", index=index, split=\"train\")\n\n# Type is inferred from the stored schema\nsample_type = ds.sample_type",
    "crumbs": [
      "Reference",
      "load_dataset API"
    ]
  },
  {
    "objectID": "reference/load-dataset.html#error-handling",
    "href": "reference/load-dataset.html#error-handling",
    "title": "load_dataset API",
    "section": "Error Handling",
    "text": "Error Handling\n\ntry:\n    ds = load_dataset(\"path/to/data.tar\", MySample, split=\"train\")\nexcept FileNotFoundError:\n    print(\"No data files found\")\nexcept ValueError as e:\n    if \"Split\" in str(e):\n        print(\"Requested split not found\")\n    else:\n        print(f\"Invalid configuration: {e}\")\nexcept KeyError:\n    print(\"Dataset not found in index\")",
    "crumbs": [
      "Reference",
      "load_dataset API"
    ]
  },
  {
    "objectID": "reference/load-dataset.html#complete-example",
    "href": "reference/load-dataset.html#complete-example",
    "title": "load_dataset API",
    "section": "Complete Example",
    "text": "Complete Example\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport atdata\nfrom atdata import load_dataset\nimport webdataset as wds\n\n# 1. Define sample type\n@atdata.packable\nclass ImageSample:\n    image: NDArray\n    label: str\n\n# 2. Create dataset files\nfor split in [\"train\", \"test\"]:\n    with wds.writer.TarWriter(f\"{split}-000.tar\") as sink:\n        for i in range(100):\n            sample = ImageSample(\n                image=np.random.rand(64, 64, 3).astype(np.float32),\n                label=f\"sample_{i}\",\n            )\n            sink.write({**sample.as_wds, \"__key__\": f\"{i:06d}\"})\n\n# 3. Load with split detection\nds_dict = load_dataset(\"./\", ImageSample)\nprint(ds_dict.keys())  # dict_keys(['train', 'test'])\n\n# 4. Iterate\nfor batch in ds_dict[\"train\"].ordered(batch_size=16):\n    print(batch.image.shape)  # (16, 64, 64, 3)\n    print(batch.label)        # ['sample_0', 'sample_1', ...]\n    break\n\n# 5. Load specific split\ntrain_ds = load_dataset(\"./\", ImageSample, split=\"train\")\nfor batch in train_ds.ordered(batch_size=32):\n    process(batch)",
    "crumbs": [
      "Reference",
      "load_dataset API"
    ]
  },
  {
    "objectID": "reference/load-dataset.html#related",
    "href": "reference/load-dataset.html#related",
    "title": "load_dataset API",
    "section": "Related",
    "text": "Related\n\nDatasets - Dataset iteration and batching\nPackable Samples - Defining sample types\nLocal Storage - LocalIndex for index lookup\nProtocols - AbstractIndex interface",
    "crumbs": [
      "Reference",
      "load_dataset API"
    ]
  },
  {
    "objectID": "reference/lenses.html",
    "href": "reference/lenses.html",
    "title": "Lenses",
    "section": "",
    "text": "Lenses provide bidirectional transformations between sample types, enabling datasets to be viewed through different schemas without duplicating data.",
    "crumbs": [
      "Reference",
      "Lenses"
    ]
  },
  {
    "objectID": "reference/lenses.html#overview",
    "href": "reference/lenses.html#overview",
    "title": "Lenses",
    "section": "Overview",
    "text": "Overview\nA lens consists of:\n\nGetter: Transforms source type S to view type V\nPutter: Updates source based on a modified view (optional)",
    "crumbs": [
      "Reference",
      "Lenses"
    ]
  },
  {
    "objectID": "reference/lenses.html#creating-a-lens",
    "href": "reference/lenses.html#creating-a-lens",
    "title": "Lenses",
    "section": "Creating a Lens",
    "text": "Creating a Lens\nUse the @lens decorator to define a getter:\n\nimport atdata\nfrom numpy.typing import NDArray\n\n@atdata.packable\nclass FullSample:\n    image: NDArray\n    label: str\n    confidence: float\n    metadata: dict\n\n@atdata.packable\nclass SimpleSample:\n    label: str\n    confidence: float\n\n@atdata.lens\ndef simplify(src: FullSample) -&gt; SimpleSample:\n    return SimpleSample(label=src.label, confidence=src.confidence)\n\nThe decorator:\n\nCreates a Lens object from the getter function\nRegisters it in the global LensNetwork registry\nExtracts source/view types from annotations",
    "crumbs": [
      "Reference",
      "Lenses"
    ]
  },
  {
    "objectID": "reference/lenses.html#adding-a-putter",
    "href": "reference/lenses.html#adding-a-putter",
    "title": "Lenses",
    "section": "Adding a Putter",
    "text": "Adding a Putter\nTo enable bidirectional updates, add a putter:\n\n@simplify.putter\ndef simplify_put(view: SimpleSample, source: FullSample) -&gt; FullSample:\n    return FullSample(\n        image=source.image,\n        label=view.label,\n        confidence=view.confidence,\n        metadata=source.metadata,\n    )\n\nThe putter receives:\n\nview: The modified view value\nsource: The original source value\n\nIt returns an updated source that reflects changes from the view.",
    "crumbs": [
      "Reference",
      "Lenses"
    ]
  },
  {
    "objectID": "reference/lenses.html#using-lenses-with-datasets",
    "href": "reference/lenses.html#using-lenses-with-datasets",
    "title": "Lenses",
    "section": "Using Lenses with Datasets",
    "text": "Using Lenses with Datasets\nLenses integrate with Dataset.as_type():\n\ndataset = atdata.Dataset[FullSample](\"data-{000000..000009}.tar\")\n\n# View through a different type\nsimple_ds = dataset.as_type(SimpleSample)\n\nfor batch in simple_ds.ordered(batch_size=32):\n    # Only SimpleSample fields available\n    labels = batch.label\n    scores = batch.confidence",
    "crumbs": [
      "Reference",
      "Lenses"
    ]
  },
  {
    "objectID": "reference/lenses.html#direct-lens-usage",
    "href": "reference/lenses.html#direct-lens-usage",
    "title": "Lenses",
    "section": "Direct Lens Usage",
    "text": "Direct Lens Usage\nLenses can also be called directly:\n\nimport numpy as np\n\nfull = FullSample(\n    image=np.zeros((224, 224, 3)),\n    label=\"cat\",\n    confidence=0.95,\n    metadata={\"source\": \"training\"}\n)\n\n# Apply getter\nsimple = simplify(full)\n# Or: simple = simplify.get(full)\n\n# Apply putter\nmodified_simple = SimpleSample(label=\"dog\", confidence=0.87)\nupdated_full = simplify.put(modified_simple, full)\n# updated_full has label=\"dog\", confidence=0.87, but retains\n# original image and metadata",
    "crumbs": [
      "Reference",
      "Lenses"
    ]
  },
  {
    "objectID": "reference/lenses.html#lens-laws",
    "href": "reference/lenses.html#lens-laws",
    "title": "Lenses",
    "section": "Lens Laws",
    "text": "Lens Laws\nWell-behaved lenses should satisfy these properties:\n\nGetPutPutGetPutPut\n\n\nIf you get a view and immediately put it back, the source is unchanged:\n\nview = lens.get(source)\nassert lens.put(view, source) == source\n\n\n\nIf you put a view, getting it back yields that view:\n\nupdated = lens.put(view, source)\nassert lens.get(updated) == view\n\n\n\nPutting twice is equivalent to putting once with the final value:\n\nresult1 = lens.put(v2, lens.put(v1, source))\nresult2 = lens.put(v2, source)\nassert result1 == result2",
    "crumbs": [
      "Reference",
      "Lenses"
    ]
  },
  {
    "objectID": "reference/lenses.html#trivial-putter",
    "href": "reference/lenses.html#trivial-putter",
    "title": "Lenses",
    "section": "Trivial Putter",
    "text": "Trivial Putter\nIf no putter is defined, a trivial putter is used that ignores view updates:\n\n@atdata.lens\ndef extract_label(src: FullSample) -&gt; SimpleSample:\n    return SimpleSample(label=src.label, confidence=src.confidence)\n\n# Without a putter, put() returns the original source unchanged\nview = SimpleSample(label=\"modified\", confidence=0.5)\nupdated = extract_label.put(view, original)\nassert updated == original  # No changes applied",
    "crumbs": [
      "Reference",
      "Lenses"
    ]
  },
  {
    "objectID": "reference/lenses.html#lensnetwork-registry",
    "href": "reference/lenses.html#lensnetwork-registry",
    "title": "Lenses",
    "section": "LensNetwork Registry",
    "text": "LensNetwork Registry\nThe LensNetwork is a singleton that stores all registered lenses:\n\nfrom atdata.lens import LensNetwork\n\nnetwork = LensNetwork()\n\n# Look up a specific lens\nlens = network.transform(FullSample, SimpleSample)\n\n# Raises ValueError if no lens exists\ntry:\n    lens = network.transform(TypeA, TypeB)\nexcept ValueError:\n    print(\"No lens registered for TypeA -&gt; TypeB\")",
    "crumbs": [
      "Reference",
      "Lenses"
    ]
  },
  {
    "objectID": "reference/lenses.html#example-feature-extraction",
    "href": "reference/lenses.html#example-feature-extraction",
    "title": "Lenses",
    "section": "Example: Feature Extraction",
    "text": "Example: Feature Extraction\n\n@atdata.packable\nclass RawSample:\n    audio: NDArray\n    text: str\n    speaker_id: int\n\n@atdata.packable\nclass TextFeatures:\n    text: str\n    word_count: int\n\n@atdata.lens\ndef extract_text(src: RawSample) -&gt; TextFeatures:\n    return TextFeatures(\n        text=src.text,\n        word_count=len(src.text.split())\n    )\n\n@extract_text.putter\ndef extract_text_put(view: TextFeatures, source: RawSample) -&gt; RawSample:\n    return RawSample(\n        audio=source.audio,\n        text=view.text,\n        speaker_id=source.speaker_id\n    )",
    "crumbs": [
      "Reference",
      "Lenses"
    ]
  },
  {
    "objectID": "reference/lenses.html#related",
    "href": "reference/lenses.html#related",
    "title": "Lenses",
    "section": "Related",
    "text": "Related\n\nDatasets - Using lenses with Dataset.as_type()\nPackable Samples - Defining sample types\nAtmosphere - Publishing lenses to ATProto federation",
    "crumbs": [
      "Reference",
      "Lenses"
    ]
  },
  {
    "objectID": "reference/packable-samples.html",
    "href": "reference/packable-samples.html",
    "title": "Packable Samples",
    "section": "",
    "text": "Packable samples are typed dataclasses that can be serialized with msgpack for storage in WebDataset tar files.",
    "crumbs": [
      "Reference",
      "Packable Samples"
    ]
  },
  {
    "objectID": "reference/packable-samples.html#the-packable-decorator",
    "href": "reference/packable-samples.html#the-packable-decorator",
    "title": "Packable Samples",
    "section": "The @packable Decorator",
    "text": "The @packable Decorator\nThe recommended way to define a sample type is with the @packable decorator:\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport atdata\n\n@atdata.packable\nclass ImageSample:\n    image: NDArray\n    label: str\n    confidence: float\n\nThis creates a dataclass that:\n\nInherits from PackableSample\nHas automatic msgpack serialization\nHandles NDArray conversion to/from bytes",
    "crumbs": [
      "Reference",
      "Packable Samples"
    ]
  },
  {
    "objectID": "reference/packable-samples.html#supported-field-types",
    "href": "reference/packable-samples.html#supported-field-types",
    "title": "Packable Samples",
    "section": "Supported Field Types",
    "text": "Supported Field Types\n\nPrimitives\n\n@atdata.packable\nclass PrimitiveSample:\n    name: str\n    count: int\n    score: float\n    active: bool\n    data: bytes\n\n\n\nNumPy Arrays\nFields annotated as NDArray are automatically converted:\n\n@atdata.packable\nclass ArraySample:\n    features: NDArray          # Required array\n    embeddings: NDArray | None  # Optional array\n\n\n\n\n\n\n\nNote\n\n\n\nBytes in NDArray-typed fields are always interpreted as serialized arrays. Don’t use NDArray for raw binary data—use bytes instead.\n\n\n\n\nLists\n\n@atdata.packable\nclass ListSample:\n    tags: list[str]\n    scores: list[float]",
    "crumbs": [
      "Reference",
      "Packable Samples"
    ]
  },
  {
    "objectID": "reference/packable-samples.html#serialization",
    "href": "reference/packable-samples.html#serialization",
    "title": "Packable Samples",
    "section": "Serialization",
    "text": "Serialization\n\nPacking to Bytes\n\nsample = ImageSample(\n    image=np.random.rand(224, 224, 3).astype(np.float32),\n    label=\"cat\",\n    confidence=0.95,\n)\n\n# Serialize to msgpack bytes\npacked_bytes = sample.packed\nprint(f\"Size: {len(packed_bytes)} bytes\")\n\n\n\nUnpacking from Bytes\n\n# Deserialize from bytes\nrestored = ImageSample.from_bytes(packed_bytes)\n\n# Arrays are automatically restored\nassert np.array_equal(sample.image, restored.image)\nassert sample.label == restored.label\n\n\n\nWebDataset Format\nThe as_wds property returns a dict ready for WebDataset:\n\nwds_dict = sample.as_wds\n# {'__key__': '1234...', 'msgpack': b'...'}\n\nWrite samples to a tar file:\n\nimport webdataset as wds\n\nwith wds.writer.TarWriter(\"data-000000.tar\") as sink:\n    for i, sample in enumerate(samples):\n        # Use custom key or let as_wds generate one\n        sink.write({**sample.as_wds, \"__key__\": f\"sample_{i:06d}\"})",
    "crumbs": [
      "Reference",
      "Packable Samples"
    ]
  },
  {
    "objectID": "reference/packable-samples.html#direct-inheritance-alternative",
    "href": "reference/packable-samples.html#direct-inheritance-alternative",
    "title": "Packable Samples",
    "section": "Direct Inheritance (Alternative)",
    "text": "Direct Inheritance (Alternative)\nYou can also inherit directly from PackableSample:\n\nfrom dataclasses import dataclass\n\n@dataclass\nclass DirectSample(atdata.PackableSample):\n    name: str\n    values: NDArray\n\nThis is equivalent to using @packable but more verbose.",
    "crumbs": [
      "Reference",
      "Packable Samples"
    ]
  },
  {
    "objectID": "reference/packable-samples.html#how-it-works",
    "href": "reference/packable-samples.html#how-it-works",
    "title": "Packable Samples",
    "section": "How It Works",
    "text": "How It Works\n\nSerialization Flow\n\nPackingUnpacking\n\n\n\nNDArray fields → converted to bytes via array_to_bytes()\nOther fields → passed through unchanged\nAll fields → packed with msgpack\n\n\n\n\nBytes → unpacked with ormsgpack\nDict → passed to __init__\n__post_init__ → calls _ensure_good()\nNDArray fields → bytes converted back to arrays\n\n\n\n\n\n\nThe _ensure_good() Method\nThis method runs automatically after construction and handles NDArray conversion:\n\ndef _ensure_good(self):\n    for field in dataclasses.fields(self):\n        if _is_possibly_ndarray_type(field.type):\n            value = getattr(self, field.name)\n            if isinstance(value, bytes):\n                setattr(self, field.name, bytes_to_array(value))",
    "crumbs": [
      "Reference",
      "Packable Samples"
    ]
  },
  {
    "objectID": "reference/packable-samples.html#best-practices",
    "href": "reference/packable-samples.html#best-practices",
    "title": "Packable Samples",
    "section": "Best Practices",
    "text": "Best Practices\n\nDoDon’t\n\n\n\n@atdata.packable\nclass GoodSample:\n    features: NDArray           # Clear type annotation\n    label: str                  # Simple primitives\n    metadata: dict              # Msgpack-compatible dicts\n    scores: list[float]         # Typed lists\n\n\n\n\n@atdata.packable\nclass BadSample:\n    # DON'T: Nested dataclasses not supported\n    nested: OtherSample\n\n    # DON'T: Complex objects that aren't msgpack-serializable\n    callback: Callable\n\n    # DON'T: Use NDArray for raw bytes\n    raw_data: NDArray  # Use 'bytes' type instead",
    "crumbs": [
      "Reference",
      "Packable Samples"
    ]
  },
  {
    "objectID": "reference/packable-samples.html#related",
    "href": "reference/packable-samples.html#related",
    "title": "Packable Samples",
    "section": "Related",
    "text": "Related\n\nDatasets - Loading and iterating samples\nLenses - Transforming between sample types",
    "crumbs": [
      "Reference",
      "Packable Samples"
    ]
  },
  {
    "objectID": "reference/deployment.html",
    "href": "reference/deployment.html",
    "title": "Deployment Guide",
    "section": "",
    "text": "This guide covers deploying atdata in production environments, including Redis setup for LocalIndex, S3 storage configuration, and ATProto publishing considerations.",
    "crumbs": [
      "Reference",
      "Deployment Guide"
    ]
  },
  {
    "objectID": "reference/deployment.html#local-storage-deployment",
    "href": "reference/deployment.html#local-storage-deployment",
    "title": "Deployment Guide",
    "section": "Local Storage Deployment",
    "text": "Local Storage Deployment\nThe local storage backend uses Redis for metadata indexing and S3-compatible storage for dataset files.\n\nRedis Setup\n\nRequirements\n\nRedis 6.0+ (for Redis-OM compatibility)\nSufficient memory for index metadata (typically &lt; 100MB for most deployments)\n\n\n\nDocker Deployment\n# Basic Redis\ndocker run -d \\\n  --name atdata-redis \\\n  -p 6379:6379 \\\n  -v redis-data:/data \\\n  redis:7-alpine \\\n  redis-server --appendonly yes\n\n# With password\ndocker run -d \\\n  --name atdata-redis \\\n  -p 6379:6379 \\\n  -v redis-data:/data \\\n  redis:7-alpine \\\n  redis-server --appendonly yes --requirepass yourpassword\n\n\nConfiguration\nfrom redis import Redis\nfrom atdata.local import LocalIndex\n\n# Basic connection\nredis = Redis(host=\"localhost\", port=6379)\nindex = LocalIndex(redis=redis)\n\n# With authentication\nredis = Redis(\n    host=\"redis.example.com\",\n    port=6379,\n    password=\"yourpassword\",\n    ssl=True,  # For production\n)\nindex = LocalIndex(redis=redis)\n\n\nRedis Clustering\nFor high-availability deployments:\nfrom redis.cluster import RedisCluster\n\n# Redis Cluster connection\nredis = RedisCluster(\n    host=\"redis-cluster.example.com\",\n    port=6379,\n    password=\"yourpassword\",\n)\nindex = LocalIndex(redis=redis)\n\n\n\n\n\n\nNote\n\n\n\nRedis-OM (used internally) supports Redis Cluster mode. Ensure all nodes have the same configuration.\n\n\n\n\n\nS3 Storage Setup\n\nAWS S3\nfrom atdata.local import S3DataStore\n\n# Using environment credentials (recommended for AWS)\n# Set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY\nstore = S3DataStore(\n    bucket=\"my-atdata-bucket\",\n    prefix=\"datasets/\",\n)\n\n# Explicit credentials\nstore = S3DataStore(\n    bucket=\"my-atdata-bucket\",\n    prefix=\"datasets/\",\n    credentials={\n        \"AWS_ACCESS_KEY_ID\": \"...\",\n        \"AWS_SECRET_ACCESS_KEY\": \"...\",\n        \"AWS_DEFAULT_REGION\": \"us-west-2\",\n    },\n)\n\n\nS3-Compatible Storage (MinIO, Cloudflare R2, etc.)\nstore = S3DataStore(\n    bucket=\"my-bucket\",\n    prefix=\"datasets/\",\n    endpoint_url=\"https://s3.example.com\",\n    credentials={\n        \"AWS_ACCESS_KEY_ID\": \"...\",\n        \"AWS_SECRET_ACCESS_KEY\": \"...\",\n    },\n)\n\n\nMinIO Deployment\n# Docker deployment\ndocker run -d \\\n  --name minio \\\n  -p 9000:9000 \\\n  -p 9001:9001 \\\n  -v minio-data:/data \\\n  -e MINIO_ROOT_USER=minioadmin \\\n  -e MINIO_ROOT_PASSWORD=minioadmin \\\n  minio/minio server /data --console-address \":9001\"\nstore = S3DataStore(\n    bucket=\"atdata\",\n    endpoint_url=\"http://localhost:9000\",\n    credentials={\n        \"AWS_ACCESS_KEY_ID\": \"minioadmin\",\n        \"AWS_SECRET_ACCESS_KEY\": \"minioadmin\",\n    },\n)\n\n\n\nProduction Checklist\n\nRedis persistence enabled (appendonly yes)\nRedis password authentication configured\nRedis TLS enabled for remote connections\nS3 bucket access policies configured (least privilege)\nS3 bucket versioning enabled (for data recovery)\nMonitoring for Redis memory usage\nBackup strategy for Redis data",
    "crumbs": [
      "Reference",
      "Deployment Guide"
    ]
  },
  {
    "objectID": "reference/deployment.html#atproto-deployment",
    "href": "reference/deployment.html#atproto-deployment",
    "title": "Deployment Guide",
    "section": "ATProto Deployment",
    "text": "ATProto Deployment\n\nAccount Setup\n\nCreate a Bluesky account or use your existing account\nGenerate an app-specific password at bsky.app/settings/app-passwords\nNever use your main account password in code\n\n\n\n\n\n\n\nWarning\n\n\n\nSecurity: Always use app passwords, never your main password. App passwords can be revoked without affecting your account.\n\n\n\n\nAuthentication Patterns\n\nEnvironment Variables (Recommended)\nimport os\nfrom atdata.atmosphere import AtmosphereClient\n\nclient = AtmosphereClient()\nclient.login(\n    os.environ[\"ATPROTO_HANDLE\"],\n    os.environ[\"ATPROTO_APP_PASSWORD\"],\n)\n\n\nSession Persistence\nFor long-running services, persist and reuse sessions:\nimport os\nfrom pathlib import Path\n\nSESSION_FILE = Path(\"~/.atdata/session\").expanduser()\n\nclient = AtmosphereClient()\n\nif SESSION_FILE.exists():\n    # Restore existing session\n    session_string = SESSION_FILE.read_text()\n    try:\n        client.login_with_session(session_string)\n    except Exception:\n        # Session expired, re-authenticate\n        client.login(handle, app_password)\n        SESSION_FILE.parent.mkdir(parents=True, exist_ok=True)\n        SESSION_FILE.write_text(client.export_session())\nelse:\n    # Initial login\n    client.login(handle, app_password)\n    SESSION_FILE.parent.mkdir(parents=True, exist_ok=True)\n    SESSION_FILE.write_text(client.export_session())\n\n\n\nCustom PDS Deployment\nFor self-hosted ATProto infrastructure:\nclient = AtmosphereClient(base_url=\"https://pds.example.com\")\nclient.login(\"handle.example.com\", \"app-password\")\nSee ATProto PDS documentation for self-hosting setup.\n\n\nRate Limiting Considerations\nATProto has rate limits. For bulk operations:\n\nSpace out record creation (1-2 per second for bulk uploads)\nUse batch operations where available\nImplement exponential backoff for retries\nConsider blob storage limits (~50MB per blob)\n\nimport time\n\nfor i, dataset in enumerate(datasets_to_publish):\n    index.insert_dataset(dataset, name=f\"dataset-{i}\", ...)\n    time.sleep(1)  # Rate limiting",
    "crumbs": [
      "Reference",
      "Deployment Guide"
    ]
  },
  {
    "objectID": "reference/deployment.html#docker-compose-example",
    "href": "reference/deployment.html#docker-compose-example",
    "title": "Deployment Guide",
    "section": "Docker Compose Example",
    "text": "Docker Compose Example\nComplete local deployment with Redis and MinIO:\n# docker-compose.yml\nversion: '3.8'\n\nservices:\n  redis:\n    image: redis:7-alpine\n    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD}\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - redis-data:/data\n\n  minio:\n    image: minio/minio\n    command: server /data --console-address \":9001\"\n    ports:\n      - \"9000:9000\"\n      - \"9001:9001\"\n    environment:\n      MINIO_ROOT_USER: ${MINIO_USER}\n      MINIO_ROOT_PASSWORD: ${MINIO_PASSWORD}\n    volumes:\n      - minio-data:/data\n\nvolumes:\n  redis-data:\n  minio-data:\n# .env\nREDIS_PASSWORD=your-redis-password\nMINIO_USER=minioadmin\nMINIO_PASSWORD=your-minio-password",
    "crumbs": [
      "Reference",
      "Deployment Guide"
    ]
  },
  {
    "objectID": "reference/deployment.html#monitoring",
    "href": "reference/deployment.html#monitoring",
    "title": "Deployment Guide",
    "section": "Monitoring",
    "text": "Monitoring\n\nRedis Metrics\nKey metrics to monitor:\n\nused_memory: Memory usage\nconnected_clients: Active connections\nkeyspace_hits/misses: Cache efficiency\naof_last_write_status: Persistence health\n\nredis-cli INFO | grep -E \"used_memory|connected_clients|keyspace\"\n\n\nS3 Metrics\n\nRequest counts and latency\nError rates (4xx, 5xx)\nStorage usage by prefix\nData transfer costs",
    "crumbs": [
      "Reference",
      "Deployment Guide"
    ]
  },
  {
    "objectID": "reference/deployment.html#security-best-practices",
    "href": "reference/deployment.html#security-best-practices",
    "title": "Deployment Guide",
    "section": "Security Best Practices",
    "text": "Security Best Practices\n\nNetwork Isolation: Run Redis and S3 in private networks\nTLS Everywhere: Encrypt connections to Redis and S3\nCredential Rotation: Rotate API keys and passwords regularly\nAccess Logging: Enable S3 access logging for audit trails\nLeast Privilege: Use minimal IAM permissions for S3 access\n\n\nS3 IAM Policy Example\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:ListBucket\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::my-atdata-bucket\",\n        \"arn:aws:s3:::my-atdata-bucket/*\"\n      ]\n    }\n  ]\n}",
    "crumbs": [
      "Reference",
      "Deployment Guide"
    ]
  },
  {
    "objectID": "reference/troubleshooting.html",
    "href": "reference/troubleshooting.html",
    "title": "Troubleshooting & FAQ",
    "section": "",
    "text": "This page covers common issues, error messages, and frequently asked questions when working with atdata.",
    "crumbs": [
      "Reference",
      "Troubleshooting & FAQ"
    ]
  },
  {
    "objectID": "reference/troubleshooting.html#common-errors",
    "href": "reference/troubleshooting.html#common-errors",
    "title": "Troubleshooting & FAQ",
    "section": "Common Errors",
    "text": "Common Errors\n\nTypeError: ‘type’ object is not subscriptable\nError:\nTypeError: 'type' object is not subscriptable\nCause: Using Dataset or SampleBatch without subscripting the type parameter on Python &lt; 3.9, or using an unsubscripted generic.\nSolution: Always use the subscripted form:\n# Correct\nds = Dataset[MySample](\"data.tar\")\nbatch = SampleBatch[MySample](samples)\n\n# Incorrect\nds = Dataset(\"data.tar\")  # Missing type parameter\n\n\nAttributeError: ‘NoneType’ object has no attribute…\nError:\nAttributeError: 'NoneType' object has no attribute '__args__'\nCause: Creating a Dataset or SampleBatch without using the subscripted syntax Class[Type](...).\nSolution: These classes use Python’s __orig_class__ mechanism to extract type parameters at runtime. You must use:\nds = Dataset[MySample](url)  # Correct\nNot:\nds = Dataset(url)  # Wrong - no type information\n\n\nRuntimeError: msgpack field not found in sample\nError:\nRuntimeError: Malformed sample: 'msgpack' field not found\nCause: The tar file contains samples that weren’t written with atdata’s serialization format.\nSolution: Ensure samples are written using sample.as_wds:\nwith wds.writer.TarWriter(\"data.tar\") as sink:\n    for sample in samples:\n        sink.write(sample.as_wds)  # Correct\n\n\nValueError: Field type not supported\nError:\nTypeError: Unsupported type for schema field: &lt;class 'SomeType'&gt;\nCause: Using an unsupported Python type in a PackableSample field.\nSupported types:\n\n\n\nPython Type\nNotes\n\n\n\n\nstr\nUnicode strings\n\n\nint\nIntegers\n\n\nfloat\nFloating point\n\n\nbool\nBoolean\n\n\nbytes\nBinary data\n\n\nNDArray\nNumpy arrays (any dtype)\n\n\nlist[T]\nLists of primitives\n\n\nT \\| None\nOptional fields\n\n\n\nNot supported: Nested dataclasses, dicts, custom classes.\n\n\nKeyError when iterating dataset\nError:\nKeyError: 'msgpack'\nCause: The WebDataset tar file structure doesn’t match expected format.\nSolution: Verify your tar file was created correctly:\n# Check tar contents\ntar -tvf data.tar | head -20\nEach sample should have a .msgpack extension in the tar file.",
    "crumbs": [
      "Reference",
      "Troubleshooting & FAQ"
    ]
  },
  {
    "objectID": "reference/troubleshooting.html#faq",
    "href": "reference/troubleshooting.html#faq",
    "title": "Troubleshooting & FAQ",
    "section": "FAQ",
    "text": "FAQ\n\nHow do I check the sample type of a dataset?\nds = Dataset[MySample](\"data.tar\")\nprint(ds.sample_type)  # &lt;class 'MySample'&gt;\n\n\nHow do I convert a dataset to a different type?\nUse the as_type() method with a registered lens:\n@atdata.lens\ndef my_lens(src: SourceType) -&gt; TargetType:\n    return TargetType(field=src.other_field)\n\nds_view = ds.as_type(TargetType)\n\n\nHow do I handle optional NDArray fields?\nUse NDArray | None annotation:\n@atdata.packable\nclass MySample:\n    required_array: NDArray\n    optional_array: NDArray | None = None\n\n\nWhy is my dataset iteration slow?\nCommon causes:\n\nNetwork latency: Use local caching for remote datasets\nSmall batch sizes: Increase batch_size in ordered() or shuffled()\nShuffle buffer: For shuffled(), the initial parameter controls buffer size\n\n# Larger batches = better throughput\nfor batch in ds.shuffled(batch_size=64, initial=1000):\n    ...\n\n\nHow do I export to parquet?\nds = Dataset[MySample](\"data.tar\")\nds.to_parquet(\"output.parquet\")\n\n# With sample limit (for large datasets)\nds.to_parquet(\"output.parquet\", maxcount=10000)\n\n\n\n\n\n\nWarning\n\n\n\nto_parquet() loads the dataset into memory. For very large datasets, use maxcount to limit samples or process in chunks.\n\n\n\n\nHow do I handle multiple shards?\nUse WebDataset brace notation:\n# Single shard\nds = Dataset[MySample](\"data-000000.tar\")\n\n# Multiple shards (range)\nds = Dataset[MySample](\"data-{000000..000009}.tar\")\n\n# Multiple shards (list)\nds = Dataset[MySample](\"data-{000000,000005,000009}.tar\")\n\n\nCan I use S3 or other cloud storage?\nYes, use S3Source for S3-compatible storage:\nfrom atdata import S3Source, Dataset\n\nsource = S3Source.from_urls(\n    [\"s3://bucket/data-000000.tar\", \"s3://bucket/data-000001.tar\"],\n    endpoint_url=\"https://s3.example.com\",  # Optional for non-AWS S3\n)\n\nds = Dataset[MySample](source)\n\n\nHow do I publish to ATProto/Atmosphere?\nfrom atdata.atmosphere import AtmosphereClient, AtmosphereIndex\n\nclient = AtmosphereClient()\nclient.login(\"handle.bsky.social\", \"app-password\")  # Use app password!\n\nindex = AtmosphereIndex(client)\n\n# Publish schema\nschema_uri = index.publish_schema(MySample, version=\"1.0.0\")\n\n# Publish dataset\nentry = index.insert_dataset(ds, name=\"my-dataset\", schema_ref=schema_uri)\n\n\nWhat’s the difference between LocalIndex and AtmosphereIndex?\n\n\n\nFeature\nLocalIndex\nAtmosphereIndex\n\n\n\n\nStorage\nRedis + S3\nATProto PDS\n\n\nDiscovery\nLocal only\nFederated network\n\n\nAuth\nNone required\nATProto account\n\n\nUse case\nDevelopment, private data\nPublic distribution\n\n\n\nBoth implement the AbstractIndex protocol, so code can work with either.",
    "crumbs": [
      "Reference",
      "Troubleshooting & FAQ"
    ]
  },
  {
    "objectID": "reference/troubleshooting.html#getting-help",
    "href": "reference/troubleshooting.html#getting-help",
    "title": "Troubleshooting & FAQ",
    "section": "Getting Help",
    "text": "Getting Help\n\nGitHub Issues: github.com/forecast-bio/atdata/issues\nDocumentation: Check the reference pages for detailed API documentation\nExamples: See the examples/ directory for working code samples",
    "crumbs": [
      "Reference",
      "Troubleshooting & FAQ"
    ]
  }
]
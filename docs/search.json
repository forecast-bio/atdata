[
  {
    "objectID": "reference/protocols.html",
    "href": "reference/protocols.html",
    "title": "Protocols",
    "section": "",
    "text": "The protocols module defines abstract interfaces that enable interchangeable index backends (local Redis vs ATProto), data stores (S3 vs PDS blobs), and data sources (URL, S3, etc.).",
    "crumbs": [
      "Guide",
      "Reference",
      "Protocols"
    ]
  },
  {
    "objectID": "reference/protocols.html#overview",
    "href": "reference/protocols.html#overview",
    "title": "Protocols",
    "section": "Overview",
    "text": "Overview\nBoth local and atmosphere implementations solve the same problem: indexed dataset storage with external data URLs. These protocols formalize that common interface:\n\nIndexEntry: Common interface for dataset index entries\nAbstractIndex: Protocol for index operations\nAbstractDataStore: Protocol for data storage operations\nDataSource: Protocol for streaming data from various backends",
    "crumbs": [
      "Guide",
      "Reference",
      "Protocols"
    ]
  },
  {
    "objectID": "reference/protocols.html#indexentry-protocol",
    "href": "reference/protocols.html#indexentry-protocol",
    "title": "Protocols",
    "section": "IndexEntry Protocol",
    "text": "IndexEntry Protocol\nRepresents a dataset entry in any index:\n\nfrom atdata._protocols import IndexEntry\n\ndef process_entry(entry: IndexEntry) -&gt; None:\n    print(f\"Name: {entry.name}\")\n    print(f\"Schema: {entry.schema_ref}\")\n    print(f\"URLs: {entry.data_urls}\")\n    print(f\"Metadata: {entry.metadata}\")\n\n\nProperties\n\n\n\nProperty\nType\nDescription\n\n\n\n\nname\nstr\nHuman-readable dataset name\n\n\nschema_ref\nstr\nSchema reference (local:// or at://)\n\n\ndata_urls\nlist[str]\nWebDataset URLs for the data\n\n\nmetadata\ndict \\| None\nArbitrary metadata dictionary\n\n\n\n\n\nImplementations\n\nLocalDatasetEntry (from atdata.local)\nAtmosphereIndexEntry (from atdata.atmosphere)",
    "crumbs": [
      "Guide",
      "Reference",
      "Protocols"
    ]
  },
  {
    "objectID": "reference/protocols.html#abstractindex-protocol",
    "href": "reference/protocols.html#abstractindex-protocol",
    "title": "Protocols",
    "section": "AbstractIndex Protocol",
    "text": "AbstractIndex Protocol\nDefines operations for managing schemas and datasets:\n\nfrom atdata._protocols import AbstractIndex\n\ndef list_all_datasets(index: AbstractIndex) -&gt; None:\n    \"\"\"Works with LocalIndex or AtmosphereIndex.\"\"\"\n    for entry in index.list_datasets():\n        print(f\"{entry.name}: {entry.schema_ref}\")\n\n\nDataset Operations\n\n# Insert a dataset\nentry = index.insert_dataset(\n    dataset,\n    name=\"my-dataset\",\n    schema_ref=\"local://schemas/MySample@1.0.0\",  # optional\n)\n\n# Get by name/reference\nentry = index.get_dataset(\"my-dataset\")\n\n# List all datasets\nfor entry in index.list_datasets():\n    print(entry.name)\n\n\n\nSchema Operations\n\n# Publish a schema\nschema_ref = index.publish_schema(\n    MySample,\n    version=\"1.0.0\",\n)\n\n# Get schema record\nschema = index.get_schema(schema_ref)\nprint(schema[\"name\"], schema[\"version\"])\n\n# List all schemas\nfor schema in index.list_schemas():\n    print(f\"{schema['name']}@{schema['version']}\")\n\n# Decode schema to Python type\nSampleType = index.decode_schema(schema_ref)\ndataset = atdata.Dataset[SampleType](entry.data_urls[0])\n\n\n\nImplementations\n\nLocalIndex / Index (from atdata.local)\nAtmosphereIndex (from atdata.atmosphere)",
    "crumbs": [
      "Guide",
      "Reference",
      "Protocols"
    ]
  },
  {
    "objectID": "reference/protocols.html#abstractdatastore-protocol",
    "href": "reference/protocols.html#abstractdatastore-protocol",
    "title": "Protocols",
    "section": "AbstractDataStore Protocol",
    "text": "AbstractDataStore Protocol\nAbstracts over different storage backends:\n\nfrom atdata._protocols import AbstractDataStore\n\ndef write_dataset(store: AbstractDataStore, dataset) -&gt; list[str]:\n    \"\"\"Works with S3DataStore or future PDS blob store.\"\"\"\n    urls = store.write_shards(dataset, prefix=\"datasets/v1\")\n    return urls\n\n\nMethods\n\n# Write dataset shards\nurls = store.write_shards(\n    dataset,\n    prefix=\"datasets/mnist/v1\",\n    maxcount=10000,  # samples per shard\n)\n\n# Resolve URL for reading\nreadable_url = store.read_url(\"s3://bucket/path.tar\")\n\n# Check streaming support\nif store.supports_streaming():\n    # Can stream directly\n    pass\n\n\n\nImplementations\n\nS3DataStore (from atdata.local)",
    "crumbs": [
      "Guide",
      "Reference",
      "Protocols"
    ]
  },
  {
    "objectID": "reference/protocols.html#datasource-protocol",
    "href": "reference/protocols.html#datasource-protocol",
    "title": "Protocols",
    "section": "DataSource Protocol",
    "text": "DataSource Protocol\nAbstracts over different data source backends for streaming dataset shards:\n\nfrom atdata._protocols import DataSource\n\ndef load_from_source(source: DataSource) -&gt; None:\n    \"\"\"Works with URLSource, S3Source, or custom implementations.\"\"\"\n    print(f\"Shards: {source.shard_list}\")\n\n    for shard_id, stream in source.shards():\n        print(f\"Reading {shard_id}\")\n        # stream is a file-like object\n\n\nMethods\n\n# Get list of shard identifiers\nshard_ids = source.shard_list  # ['data-000000.tar', 'data-000001.tar', ...]\n\n# Iterate over all shards with streams\nfor shard_id, stream in source.shards():\n    # stream is IO[bytes], can be passed to tar reader\n    process_shard(stream)\n\n# Open a specific shard\nstream = source.open_shard(\"data-000001.tar\")\n\n\n\nImplementations\n\nURLSource (from atdata) - WebDataset-compatible URLs (local, HTTP, etc.)\nS3Source (from atdata) - S3 and S3-compatible storage with boto3\n\n\n\nCreating Custom Data Sources\nImplement the DataSource protocol for custom backends:\n\nfrom typing import Iterator, IO\nfrom atdata._protocols import DataSource\n\nclass MyCustomSource:\n    \"\"\"Custom data source for proprietary storage.\"\"\"\n\n    def __init__(self, config: dict):\n        self._config = config\n        self._shards = [\"shard-001.tar\", \"shard-002.tar\"]\n\n    @property\n    def shard_list(self) -&gt; list[str]:\n        return self._shards\n\n    def shards(self) -&gt; Iterator[tuple[str, IO[bytes]]]:\n        for shard_id in self._shards:\n            stream = self._open(shard_id)\n            yield shard_id, stream\n\n    def open_shard(self, shard_id: str) -&gt; IO[bytes]:\n        if shard_id not in self._shards:\n            raise KeyError(f\"Shard not found: {shard_id}\")\n        return self._open(shard_id)\n\n    def _open(self, shard_id: str) -&gt; IO[bytes]:\n        # Implementation-specific logic\n        ...\n\n# Use with Dataset\nsource = MyCustomSource({\"endpoint\": \"...\"})\ndataset = atdata.Dataset[MySample](source)",
    "crumbs": [
      "Guide",
      "Reference",
      "Protocols"
    ]
  },
  {
    "objectID": "reference/protocols.html#using-protocols-for-polymorphism",
    "href": "reference/protocols.html#using-protocols-for-polymorphism",
    "title": "Protocols",
    "section": "Using Protocols for Polymorphism",
    "text": "Using Protocols for Polymorphism\nWrite code that works with any backend:\n\nfrom atdata._protocols import AbstractIndex, IndexEntry\nfrom atdata import Dataset\n\ndef backup_all_datasets(\n    source: AbstractIndex,\n    target: AbstractIndex,\n) -&gt; None:\n    \"\"\"Copy all datasets from source index to target.\"\"\"\n    for entry in source.list_datasets():\n        # Decode schema from source\n        SampleType = source.decode_schema(entry.schema_ref)\n\n        # Publish schema to target\n        target_schema = target.publish_schema(SampleType)\n\n        # Load and re-insert dataset\n        ds = Dataset[SampleType](entry.data_urls[0])\n        target.insert_dataset(\n            ds,\n            name=entry.name,\n            schema_ref=target_schema,\n        )",
    "crumbs": [
      "Guide",
      "Reference",
      "Protocols"
    ]
  },
  {
    "objectID": "reference/protocols.html#schema-reference-formats",
    "href": "reference/protocols.html#schema-reference-formats",
    "title": "Protocols",
    "section": "Schema Reference Formats",
    "text": "Schema Reference Formats\nSchema references vary by backend:\n\n\n\n\n\n\n\n\nBackend\nFormat\nExample\n\n\n\n\nLocal\natdata://local/sampleSchema/{Class}@{version}\natdata://local/sampleSchema/ImageSample@1.0.0\n\n\nAtmosphere\nat://{did}/{collection}/{rkey}\nat://did:plc:abc123/ac.foundation.dataset.sampleSchema/xyz\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLegacy local://schemas/ URIs are still supported for backward compatibility.",
    "crumbs": [
      "Guide",
      "Reference",
      "Protocols"
    ]
  },
  {
    "objectID": "reference/protocols.html#type-checking",
    "href": "reference/protocols.html#type-checking",
    "title": "Protocols",
    "section": "Type Checking",
    "text": "Type Checking\nProtocols are runtime-checkable:\n\nfrom atdata._protocols import IndexEntry, AbstractIndex\n\n# Check if object implements protocol\nentry = index.get_dataset(\"test\")\nassert isinstance(entry, IndexEntry)\n\n# Type hints work with protocols\ndef process(index: AbstractIndex) -&gt; None:\n    ...  # IDE provides autocomplete",
    "crumbs": [
      "Guide",
      "Reference",
      "Protocols"
    ]
  },
  {
    "objectID": "reference/protocols.html#complete-example",
    "href": "reference/protocols.html#complete-example",
    "title": "Protocols",
    "section": "Complete Example",
    "text": "Complete Example\n\nimport atdata\nfrom atdata.local import LocalIndex, S3DataStore\nfrom atdata.atmosphere import AtmosphereClient, AtmosphereIndex\nfrom atdata._protocols import AbstractIndex\nimport numpy as np\nfrom numpy.typing import NDArray\n\n# Define sample type\n@atdata.packable\nclass FeatureSample:\n    features: NDArray\n    label: int\n\n# Function works with any index\ndef count_datasets(index: AbstractIndex) -&gt; int:\n    return sum(1 for _ in index.list_datasets())\n\n# Use with local index\nlocal_index = LocalIndex()\nprint(f\"Local datasets: {count_datasets(local_index)}\")\n\n# Use with atmosphere index\nclient = AtmosphereClient()\nclient.login(\"handle.bsky.social\", \"app-password\")\natm_index = AtmosphereIndex(client)\nprint(f\"Atmosphere datasets: {count_datasets(atm_index)}\")\n\n# Migrate from local to atmosphere\ndef migrate_dataset(\n    name: str,\n    source: AbstractIndex,\n    target: AbstractIndex,\n) -&gt; None:\n    entry = source.get_dataset(name)\n    SampleType = source.decode_schema(entry.schema_ref)\n\n    # Publish schema\n    schema_ref = target.publish_schema(SampleType)\n\n    # Create dataset and insert\n    ds = atdata.Dataset[SampleType](entry.data_urls[0])\n    target.insert_dataset(ds, name=name, schema_ref=schema_ref)\n\nmigrate_dataset(\"my-features\", local_index, atm_index)",
    "crumbs": [
      "Guide",
      "Reference",
      "Protocols"
    ]
  },
  {
    "objectID": "reference/protocols.html#related",
    "href": "reference/protocols.html#related",
    "title": "Protocols",
    "section": "Related",
    "text": "Related\n\nLocal Storage - LocalIndex and S3DataStore\nAtmosphere - AtmosphereIndex\nPromotion - Local to atmosphere migration\nload_dataset - Using indexes with load_dataset()",
    "crumbs": [
      "Guide",
      "Reference",
      "Protocols"
    ]
  },
  {
    "objectID": "reference/datasets.html",
    "href": "reference/datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "The Dataset class provides typed iteration over WebDataset tar files with automatic batching and lens transformations.",
    "crumbs": [
      "Guide",
      "Reference",
      "Datasets"
    ]
  },
  {
    "objectID": "reference/datasets.html#creating-a-dataset",
    "href": "reference/datasets.html#creating-a-dataset",
    "title": "Datasets",
    "section": "Creating a Dataset",
    "text": "Creating a Dataset\n\nimport atdata\nfrom numpy.typing import NDArray\n\n@atdata.packable\nclass ImageSample:\n    image: NDArray\n    label: str\n\n# Single shard (string URL - most common)\ndataset = atdata.Dataset[ImageSample](\"data-000000.tar\")\n\n# Multiple shards with brace notation\ndataset = atdata.Dataset[ImageSample](\"data-{000000..000009}.tar\")\n\nThe type parameter [ImageSample] specifies what sample type the dataset contains. This enables type-safe iteration and automatic deserialization.",
    "crumbs": [
      "Guide",
      "Reference",
      "Datasets"
    ]
  },
  {
    "objectID": "reference/datasets.html#data-sources",
    "href": "reference/datasets.html#data-sources",
    "title": "Datasets",
    "section": "Data Sources",
    "text": "Data Sources\nDatasets can be created from different data sources using the DataSource protocol:\n\nURL Source (default)\nWhen you pass a string to Dataset, it automatically wraps it in a URLSource:\n\n# These are equivalent:\ndataset = atdata.Dataset[ImageSample](\"data-{000000..000009}.tar\")\ndataset = atdata.Dataset[ImageSample](atdata.URLSource(\"data-{000000..000009}.tar\"))\n\n\n\nS3 Source\nFor private S3 buckets or S3-compatible storage (Cloudflare R2, MinIO), use S3Source:\n\n# From explicit credentials\nsource = atdata.S3Source(\n    bucket=\"my-bucket\",\n    keys=[\"data-000000.tar\", \"data-000001.tar\"],\n    endpoint=\"https://my-r2-account.r2.cloudflarestorage.com\",\n    access_key=\"AKID...\",\n    secret_key=\"SECRET...\",\n)\ndataset = atdata.Dataset[ImageSample](source)\n\n# From S3 URLs\nsource = atdata.S3Source.from_urls([\n    \"s3://my-bucket/data-000000.tar\",\n    \"s3://my-bucket/data-000001.tar\",\n])\ndataset = atdata.Dataset[ImageSample](source)\n\n\n\n\n\n\n\nNote\n\n\n\nS3Source uses boto3 for streaming, enabling authentication with private buckets. For public S3 URLs, a string URL with URLSource works directly.",
    "crumbs": [
      "Guide",
      "Reference",
      "Datasets"
    ]
  },
  {
    "objectID": "reference/datasets.html#iteration-modes",
    "href": "reference/datasets.html#iteration-modes",
    "title": "Datasets",
    "section": "Iteration Modes",
    "text": "Iteration Modes\n\nOrdered Iteration\nIterate through samples in their original order:\n\n# With batching (default batch_size=1)\nfor batch in dataset.ordered(batch_size=32):\n    images = batch.image  # numpy array (32, H, W, C)\n    labels = batch.label  # list of 32 strings\n\n# Without batching (raw samples)\nfor sample in dataset.ordered(batch_size=None):\n    print(sample.label)\n\n\n\nShuffled Iteration\nIterate with randomized order at both shard and sample levels:\n\nfor batch in dataset.shuffled(batch_size=32):\n    # Samples are shuffled\n    process(batch)\n\n# Control shuffle buffer sizes\nfor batch in dataset.shuffled(\n    buffer_shards=100,    # Shards to buffer (default: 100)\n    buffer_samples=10000, # Samples to buffer (default: 10,000)\n    batch_size=32,\n):\n    process(batch)\n\n\n\n\n\n\n\nTip\n\n\n\nLarger buffer sizes increase randomness but use more memory. For training, buffer_samples=10000 is usually a good balance.",
    "crumbs": [
      "Guide",
      "Reference",
      "Datasets"
    ]
  },
  {
    "objectID": "reference/datasets.html#samplebatch",
    "href": "reference/datasets.html#samplebatch",
    "title": "Datasets",
    "section": "SampleBatch",
    "text": "SampleBatch\nWhen iterating with a batch_size, each iteration yields a SampleBatch with automatic attribute aggregation.\n\n@atdata.packable\nclass Sample:\n    features: NDArray  # shape (256,)\n    label: str\n    score: float\n\nfor batch in dataset.ordered(batch_size=16):\n    # NDArray fields are stacked with a batch dimension\n    features = batch.features  # numpy array (16, 256)\n\n    # Other fields become lists\n    labels = batch.label       # list of 16 strings\n    scores = batch.score       # list of 16 floats\n\nResults are cached, so accessing the same attribute multiple times is efficient.",
    "crumbs": [
      "Guide",
      "Reference",
      "Datasets"
    ]
  },
  {
    "objectID": "reference/datasets.html#type-transformations-with-lenses",
    "href": "reference/datasets.html#type-transformations-with-lenses",
    "title": "Datasets",
    "section": "Type Transformations with Lenses",
    "text": "Type Transformations with Lenses\nView a dataset through a different sample type using registered lenses:\n\n@atdata.packable\nclass SimplifiedSample:\n    label: str\n\n@atdata.lens\ndef simplify(src: ImageSample) -&gt; SimplifiedSample:\n    return SimplifiedSample(label=src.label)\n\n# Transform dataset to different type\nsimple_ds = dataset.as_type(SimplifiedSample)\n\nfor batch in simple_ds.ordered(batch_size=16):\n    print(batch.label)  # Only label field available\n\nSee Lenses for details on defining transformations.",
    "crumbs": [
      "Guide",
      "Reference",
      "Datasets"
    ]
  },
  {
    "objectID": "reference/datasets.html#dataset-properties",
    "href": "reference/datasets.html#dataset-properties",
    "title": "Datasets",
    "section": "Dataset Properties",
    "text": "Dataset Properties\n\nShard List\nGet the list of individual tar files:\n\ndataset = atdata.Dataset[Sample](\"data-{000000..000009}.tar\")\nshards = dataset.shard_list\n# ['data-000000.tar', 'data-000001.tar', ..., 'data-000009.tar']\n\n\n\nMetadata\nDatasets can have associated metadata from a URL:\n\ndataset = atdata.Dataset[Sample](\n    \"data-{000000..000009}.tar\",\n    metadata_url=\"https://example.com/metadata.msgpack\"\n)\n\n# Fetched and cached on first access\nmetadata = dataset.metadata  # dict or None",
    "crumbs": [
      "Guide",
      "Reference",
      "Datasets"
    ]
  },
  {
    "objectID": "reference/datasets.html#writing-datasets",
    "href": "reference/datasets.html#writing-datasets",
    "title": "Datasets",
    "section": "Writing Datasets",
    "text": "Writing Datasets\nUse WebDataset’s TarWriter or ShardWriter to create datasets:\n\nimport webdataset as wds\nimport numpy as np\n\nsamples = [\n    ImageSample(image=np.random.rand(224, 224, 3).astype(np.float32), label=\"cat\")\n    for _ in range(100)\n]\n\n# Single tar file\nwith wds.writer.TarWriter(\"data-000000.tar\") as sink:\n    for i, sample in enumerate(samples):\n        sink.write({**sample.as_wds, \"__key__\": f\"sample_{i:06d}\"})\n\n# Multiple shards with automatic splitting\nwith wds.writer.ShardWriter(\"data-%06d.tar\", maxcount=1000) as sink:\n    for i, sample in enumerate(samples):\n        sink.write({**sample.as_wds, \"__key__\": f\"sample_{i:06d}\"})",
    "crumbs": [
      "Guide",
      "Reference",
      "Datasets"
    ]
  },
  {
    "objectID": "reference/datasets.html#parquet-export",
    "href": "reference/datasets.html#parquet-export",
    "title": "Datasets",
    "section": "Parquet Export",
    "text": "Parquet Export\nExport dataset contents to parquet format:\n\n# Export entire dataset\ndataset.to_parquet(\"output.parquet\")\n\n# Export with custom field mapping\ndef extract_fields(sample):\n    return {\"label\": sample.label, \"score\": sample.confidence}\n\ndataset.to_parquet(\"output.parquet\", sample_map=extract_fields)\n\n# Export in segments\ndataset.to_parquet(\"output.parquet\", maxcount=10000)\n# Creates output-000000.parquet, output-000001.parquet, etc.",
    "crumbs": [
      "Guide",
      "Reference",
      "Datasets"
    ]
  },
  {
    "objectID": "reference/datasets.html#url-formats",
    "href": "reference/datasets.html#url-formats",
    "title": "Datasets",
    "section": "URL Formats",
    "text": "URL Formats\nWhen using string URLs (via URLSource), WebDataset supports various formats:\n\n\n\n\n\n\n\nFormat\nExample\n\n\n\n\nLocal files\n./data/file.tar, /absolute/path/file-{000000..000009}.tar\n\n\nHTTP/HTTPS\nhttps://example.com/data-{000000..000009}.tar\n\n\nGoogle Cloud\ngs://bucket/path/file.tar\n\n\n\nFor S3 with authentication, use S3Source instead of s3:// URLs.",
    "crumbs": [
      "Guide",
      "Reference",
      "Datasets"
    ]
  },
  {
    "objectID": "reference/datasets.html#dataset-properties-1",
    "href": "reference/datasets.html#dataset-properties-1",
    "title": "Datasets",
    "section": "Dataset Properties",
    "text": "Dataset Properties\n\nSource\nAccess the underlying DataSource:\n\ndataset = atdata.Dataset[Sample](\"data.tar\")\nsource = dataset.source  # URLSource instance\nprint(source.shard_list)  # ['data.tar']\n\n\n\nSample Type\nGet the type parameter used to create the dataset:\n\ndataset = atdata.Dataset[ImageSample](\"data.tar\")\nprint(dataset.sample_type)  # &lt;class 'ImageSample'&gt;\nprint(dataset.batch_type)   # SampleBatch[ImageSample]",
    "crumbs": [
      "Guide",
      "Reference",
      "Datasets"
    ]
  },
  {
    "objectID": "reference/datasets.html#related",
    "href": "reference/datasets.html#related",
    "title": "Datasets",
    "section": "Related",
    "text": "Related\n\nPackable Samples - Defining typed samples\nLenses - Type transformations\nload_dataset - HuggingFace-style loading API\nProtocols - DataSource protocol details",
    "crumbs": [
      "Guide",
      "Reference",
      "Datasets"
    ]
  },
  {
    "objectID": "reference/packable-samples.html",
    "href": "reference/packable-samples.html",
    "title": "Packable Samples",
    "section": "",
    "text": "Packable samples are typed dataclasses that can be serialized with msgpack for storage in WebDataset tar files.",
    "crumbs": [
      "Guide",
      "Reference",
      "Packable Samples"
    ]
  },
  {
    "objectID": "reference/packable-samples.html#the-packable-decorator",
    "href": "reference/packable-samples.html#the-packable-decorator",
    "title": "Packable Samples",
    "section": "The @packable Decorator",
    "text": "The @packable Decorator\nThe recommended way to define a sample type is with the @packable decorator:\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport atdata\n\n@atdata.packable\nclass ImageSample:\n    image: NDArray\n    label: str\n    confidence: float\n\nThis creates a dataclass that:\n\nInherits from PackableSample\nHas automatic msgpack serialization\nHandles NDArray conversion to/from bytes",
    "crumbs": [
      "Guide",
      "Reference",
      "Packable Samples"
    ]
  },
  {
    "objectID": "reference/packable-samples.html#supported-field-types",
    "href": "reference/packable-samples.html#supported-field-types",
    "title": "Packable Samples",
    "section": "Supported Field Types",
    "text": "Supported Field Types\n\nPrimitives\n\n@atdata.packable\nclass PrimitiveSample:\n    name: str\n    count: int\n    score: float\n    active: bool\n    data: bytes\n\n\n\nNumPy Arrays\nFields annotated as NDArray are automatically converted:\n\n@atdata.packable\nclass ArraySample:\n    features: NDArray          # Required array\n    embeddings: NDArray | None  # Optional array\n\n\n\n\n\n\n\nNote\n\n\n\nBytes in NDArray-typed fields are always interpreted as serialized arrays. Don’t use NDArray for raw binary data—use bytes instead.\n\n\n\n\nLists\n\n@atdata.packable\nclass ListSample:\n    tags: list[str]\n    scores: list[float]",
    "crumbs": [
      "Guide",
      "Reference",
      "Packable Samples"
    ]
  },
  {
    "objectID": "reference/packable-samples.html#serialization",
    "href": "reference/packable-samples.html#serialization",
    "title": "Packable Samples",
    "section": "Serialization",
    "text": "Serialization\n\nPacking to Bytes\n\nsample = ImageSample(\n    image=np.random.rand(224, 224, 3).astype(np.float32),\n    label=\"cat\",\n    confidence=0.95,\n)\n\n# Serialize to msgpack bytes\npacked_bytes = sample.packed\nprint(f\"Size: {len(packed_bytes)} bytes\")\n\n\n\nUnpacking from Bytes\n\n# Deserialize from bytes\nrestored = ImageSample.from_bytes(packed_bytes)\n\n# Arrays are automatically restored\nassert np.array_equal(sample.image, restored.image)\nassert sample.label == restored.label\n\n\n\nWebDataset Format\nThe as_wds property returns a dict ready for WebDataset:\n\nwds_dict = sample.as_wds\n# {'__key__': '1234...', 'msgpack': b'...'}\n\nWrite samples to a tar file:\n\nimport webdataset as wds\n\nwith wds.writer.TarWriter(\"data-000000.tar\") as sink:\n    for i, sample in enumerate(samples):\n        # Use custom key or let as_wds generate one\n        sink.write({**sample.as_wds, \"__key__\": f\"sample_{i:06d}\"})",
    "crumbs": [
      "Guide",
      "Reference",
      "Packable Samples"
    ]
  },
  {
    "objectID": "reference/packable-samples.html#direct-inheritance-alternative",
    "href": "reference/packable-samples.html#direct-inheritance-alternative",
    "title": "Packable Samples",
    "section": "Direct Inheritance (Alternative)",
    "text": "Direct Inheritance (Alternative)\nYou can also inherit directly from PackableSample:\n\nfrom dataclasses import dataclass\n\n@dataclass\nclass DirectSample(atdata.PackableSample):\n    name: str\n    values: NDArray\n\nThis is equivalent to using @packable but more verbose.",
    "crumbs": [
      "Guide",
      "Reference",
      "Packable Samples"
    ]
  },
  {
    "objectID": "reference/packable-samples.html#how-it-works",
    "href": "reference/packable-samples.html#how-it-works",
    "title": "Packable Samples",
    "section": "How It Works",
    "text": "How It Works\n\nSerialization Flow\n\nPackingUnpacking\n\n\n\nNDArray fields → converted to bytes via array_to_bytes()\nOther fields → passed through unchanged\nAll fields → packed with msgpack\n\n\n\n\nBytes → unpacked with ormsgpack\nDict → passed to __init__\n__post_init__ → calls _ensure_good()\nNDArray fields → bytes converted back to arrays\n\n\n\n\n\n\nThe _ensure_good() Method\nThis method runs automatically after construction and handles NDArray conversion:\n\ndef _ensure_good(self):\n    for field in dataclasses.fields(self):\n        if _is_possibly_ndarray_type(field.type):\n            value = getattr(self, field.name)\n            if isinstance(value, bytes):\n                setattr(self, field.name, bytes_to_array(value))",
    "crumbs": [
      "Guide",
      "Reference",
      "Packable Samples"
    ]
  },
  {
    "objectID": "reference/packable-samples.html#best-practices",
    "href": "reference/packable-samples.html#best-practices",
    "title": "Packable Samples",
    "section": "Best Practices",
    "text": "Best Practices\n\nDoDon’t\n\n\n\n@atdata.packable\nclass GoodSample:\n    features: NDArray           # Clear type annotation\n    label: str                  # Simple primitives\n    metadata: dict              # Msgpack-compatible dicts\n    scores: list[float]         # Typed lists\n\n\n\n\n@atdata.packable\nclass BadSample:\n    # DON'T: Nested dataclasses not supported\n    nested: OtherSample\n\n    # DON'T: Complex objects that aren't msgpack-serializable\n    callback: Callable\n\n    # DON'T: Use NDArray for raw bytes\n    raw_data: NDArray  # Use 'bytes' type instead",
    "crumbs": [
      "Guide",
      "Reference",
      "Packable Samples"
    ]
  },
  {
    "objectID": "reference/packable-samples.html#related",
    "href": "reference/packable-samples.html#related",
    "title": "Packable Samples",
    "section": "Related",
    "text": "Related\n\nDatasets - Loading and iterating samples\nLenses - Transforming between sample types",
    "crumbs": [
      "Guide",
      "Reference",
      "Packable Samples"
    ]
  },
  {
    "objectID": "reference/lenses.html",
    "href": "reference/lenses.html",
    "title": "Lenses",
    "section": "",
    "text": "Lenses provide bidirectional transformations between sample types, enabling datasets to be viewed through different schemas without duplicating data.",
    "crumbs": [
      "Guide",
      "Reference",
      "Lenses"
    ]
  },
  {
    "objectID": "reference/lenses.html#overview",
    "href": "reference/lenses.html#overview",
    "title": "Lenses",
    "section": "Overview",
    "text": "Overview\nA lens consists of:\n\nGetter: Transforms source type S to view type V\nPutter: Updates source based on a modified view (optional)",
    "crumbs": [
      "Guide",
      "Reference",
      "Lenses"
    ]
  },
  {
    "objectID": "reference/lenses.html#creating-a-lens",
    "href": "reference/lenses.html#creating-a-lens",
    "title": "Lenses",
    "section": "Creating a Lens",
    "text": "Creating a Lens\nUse the @lens decorator to define a getter:\n\nimport atdata\nfrom numpy.typing import NDArray\n\n@atdata.packable\nclass FullSample:\n    image: NDArray\n    label: str\n    confidence: float\n    metadata: dict\n\n@atdata.packable\nclass SimpleSample:\n    label: str\n    confidence: float\n\n@atdata.lens\ndef simplify(src: FullSample) -&gt; SimpleSample:\n    return SimpleSample(label=src.label, confidence=src.confidence)\n\nThe decorator:\n\nCreates a Lens object from the getter function\nRegisters it in the global LensNetwork registry\nExtracts source/view types from annotations",
    "crumbs": [
      "Guide",
      "Reference",
      "Lenses"
    ]
  },
  {
    "objectID": "reference/lenses.html#adding-a-putter",
    "href": "reference/lenses.html#adding-a-putter",
    "title": "Lenses",
    "section": "Adding a Putter",
    "text": "Adding a Putter\nTo enable bidirectional updates, add a putter:\n\n@simplify.putter\ndef simplify_put(view: SimpleSample, source: FullSample) -&gt; FullSample:\n    return FullSample(\n        image=source.image,\n        label=view.label,\n        confidence=view.confidence,\n        metadata=source.metadata,\n    )\n\nThe putter receives:\n\nview: The modified view value\nsource: The original source value\n\nIt returns an updated source that reflects changes from the view.",
    "crumbs": [
      "Guide",
      "Reference",
      "Lenses"
    ]
  },
  {
    "objectID": "reference/lenses.html#using-lenses-with-datasets",
    "href": "reference/lenses.html#using-lenses-with-datasets",
    "title": "Lenses",
    "section": "Using Lenses with Datasets",
    "text": "Using Lenses with Datasets\nLenses integrate with Dataset.as_type():\n\ndataset = atdata.Dataset[FullSample](\"data-{000000..000009}.tar\")\n\n# View through a different type\nsimple_ds = dataset.as_type(SimpleSample)\n\nfor batch in simple_ds.ordered(batch_size=32):\n    # Only SimpleSample fields available\n    labels = batch.label\n    scores = batch.confidence",
    "crumbs": [
      "Guide",
      "Reference",
      "Lenses"
    ]
  },
  {
    "objectID": "reference/lenses.html#direct-lens-usage",
    "href": "reference/lenses.html#direct-lens-usage",
    "title": "Lenses",
    "section": "Direct Lens Usage",
    "text": "Direct Lens Usage\nLenses can also be called directly:\n\nimport numpy as np\n\nfull = FullSample(\n    image=np.zeros((224, 224, 3)),\n    label=\"cat\",\n    confidence=0.95,\n    metadata={\"source\": \"training\"}\n)\n\n# Apply getter\nsimple = simplify(full)\n# Or: simple = simplify.get(full)\n\n# Apply putter\nmodified_simple = SimpleSample(label=\"dog\", confidence=0.87)\nupdated_full = simplify.put(modified_simple, full)\n# updated_full has label=\"dog\", confidence=0.87, but retains\n# original image and metadata",
    "crumbs": [
      "Guide",
      "Reference",
      "Lenses"
    ]
  },
  {
    "objectID": "reference/lenses.html#lens-laws",
    "href": "reference/lenses.html#lens-laws",
    "title": "Lenses",
    "section": "Lens Laws",
    "text": "Lens Laws\nWell-behaved lenses should satisfy these properties:\n\nGetPutPutGetPutPut\n\n\nIf you get a view and immediately put it back, the source is unchanged:\n\nview = lens.get(source)\nassert lens.put(view, source) == source\n\n\n\nIf you put a view, getting it back yields that view:\n\nupdated = lens.put(view, source)\nassert lens.get(updated) == view\n\n\n\nPutting twice is equivalent to putting once with the final value:\n\nresult1 = lens.put(v2, lens.put(v1, source))\nresult2 = lens.put(v2, source)\nassert result1 == result2",
    "crumbs": [
      "Guide",
      "Reference",
      "Lenses"
    ]
  },
  {
    "objectID": "reference/lenses.html#trivial-putter",
    "href": "reference/lenses.html#trivial-putter",
    "title": "Lenses",
    "section": "Trivial Putter",
    "text": "Trivial Putter\nIf no putter is defined, a trivial putter is used that ignores view updates:\n\n@atdata.lens\ndef extract_label(src: FullSample) -&gt; SimpleSample:\n    return SimpleSample(label=src.label, confidence=src.confidence)\n\n# Without a putter, put() returns the original source unchanged\nview = SimpleSample(label=\"modified\", confidence=0.5)\nupdated = extract_label.put(view, original)\nassert updated == original  # No changes applied",
    "crumbs": [
      "Guide",
      "Reference",
      "Lenses"
    ]
  },
  {
    "objectID": "reference/lenses.html#lensnetwork-registry",
    "href": "reference/lenses.html#lensnetwork-registry",
    "title": "Lenses",
    "section": "LensNetwork Registry",
    "text": "LensNetwork Registry\nThe LensNetwork is a singleton that stores all registered lenses:\n\nfrom atdata.lens import LensNetwork\n\nnetwork = LensNetwork()\n\n# Look up a specific lens\nlens = network.transform(FullSample, SimpleSample)\n\n# Raises ValueError if no lens exists\ntry:\n    lens = network.transform(TypeA, TypeB)\nexcept ValueError:\n    print(\"No lens registered for TypeA -&gt; TypeB\")",
    "crumbs": [
      "Guide",
      "Reference",
      "Lenses"
    ]
  },
  {
    "objectID": "reference/lenses.html#example-feature-extraction",
    "href": "reference/lenses.html#example-feature-extraction",
    "title": "Lenses",
    "section": "Example: Feature Extraction",
    "text": "Example: Feature Extraction\n\n@atdata.packable\nclass RawSample:\n    audio: NDArray\n    text: str\n    speaker_id: int\n\n@atdata.packable\nclass TextFeatures:\n    text: str\n    word_count: int\n\n@atdata.lens\ndef extract_text(src: RawSample) -&gt; TextFeatures:\n    return TextFeatures(\n        text=src.text,\n        word_count=len(src.text.split())\n    )\n\n@extract_text.putter\ndef extract_text_put(view: TextFeatures, source: RawSample) -&gt; RawSample:\n    return RawSample(\n        audio=source.audio,\n        text=view.text,\n        speaker_id=source.speaker_id\n    )",
    "crumbs": [
      "Guide",
      "Reference",
      "Lenses"
    ]
  },
  {
    "objectID": "reference/lenses.html#related",
    "href": "reference/lenses.html#related",
    "title": "Lenses",
    "section": "Related",
    "text": "Related\n\nDatasets - Using lenses with Dataset.as_type()\nPackable Samples - Defining sample types\nAtmosphere - Publishing lenses to ATProto federation",
    "crumbs": [
      "Guide",
      "Reference",
      "Lenses"
    ]
  },
  {
    "objectID": "reference/load-dataset.html",
    "href": "reference/load-dataset.html",
    "title": "load_dataset API",
    "section": "",
    "text": "The load_dataset() function provides a HuggingFace Datasets-style interface for loading typed datasets.",
    "crumbs": [
      "Guide",
      "Reference",
      "load_dataset API"
    ]
  },
  {
    "objectID": "reference/load-dataset.html#overview",
    "href": "reference/load-dataset.html#overview",
    "title": "load_dataset API",
    "section": "Overview",
    "text": "Overview\nKey differences from HuggingFace Datasets:\n\nRequires explicit sample_type parameter (typed dataclass) unless using index\nReturns atdata.Dataset[ST] instead of HF Dataset\nBuilt on WebDataset for efficient streaming\nNo Arrow caching layer",
    "crumbs": [
      "Guide",
      "Reference",
      "load_dataset API"
    ]
  },
  {
    "objectID": "reference/load-dataset.html#basic-usage",
    "href": "reference/load-dataset.html#basic-usage",
    "title": "load_dataset API",
    "section": "Basic Usage",
    "text": "Basic Usage\n\nimport atdata\nfrom atdata import load_dataset\nfrom numpy.typing import NDArray\n\n@atdata.packable\nclass TextSample:\n    text: str\n    label: int\n\n# Load a specific split\ntrain_ds = load_dataset(\"path/to/data.tar\", TextSample, split=\"train\")\n\n# Load all splits (returns DatasetDict)\nds_dict = load_dataset(\"path/to/data/\", TextSample)\ntrain_ds = ds_dict[\"train\"]\ntest_ds = ds_dict[\"test\"]",
    "crumbs": [
      "Guide",
      "Reference",
      "load_dataset API"
    ]
  },
  {
    "objectID": "reference/load-dataset.html#path-formats",
    "href": "reference/load-dataset.html#path-formats",
    "title": "load_dataset API",
    "section": "Path Formats",
    "text": "Path Formats\n\nWebDataset Brace Notation\n\n# Range notation\nds = load_dataset(\"data-{000000..000099}.tar\", MySample, split=\"train\")\n\n# List notation\nds = load_dataset(\"data-{train,test,val}.tar\", MySample, split=\"train\")\n\n\n\nGlob Patterns\n\n# Match all tar files\nds = load_dataset(\"path/to/*.tar\", MySample)\n\n# Match pattern\nds = load_dataset(\"path/to/train-*.tar\", MySample, split=\"train\")\n\n\n\nLocal Directory\n\n# Scans for .tar files\nds = load_dataset(\"./my-dataset/\", MySample)\n\n\n\nRemote URLs\n\n# S3 (public buckets)\nds = load_dataset(\"s3://bucket/data-{000..099}.tar\", MySample, split=\"train\")\n\n# HTTP/HTTPS\nds = load_dataset(\"https://example.com/data.tar\", MySample, split=\"train\")\n\n# Google Cloud Storage\nds = load_dataset(\"gs://bucket/data.tar\", MySample, split=\"train\")\n\n\n\n\n\n\n\nNote\n\n\n\nFor private S3 buckets or S3-compatible storage with authentication, use atdata.S3Source with Dataset directly. See Datasets for details.\n\n\n\n\nIndex Lookup\n\nfrom atdata.local import LocalIndex\n\nindex = LocalIndex()\n\n# Load from local index (auto-resolves type from schema)\nds = load_dataset(\"@local/my-dataset\", index=index, split=\"train\")\n\n# With explicit type\nds = load_dataset(\"@local/my-dataset\", MySample, index=index, split=\"train\")",
    "crumbs": [
      "Guide",
      "Reference",
      "load_dataset API"
    ]
  },
  {
    "objectID": "reference/load-dataset.html#split-detection",
    "href": "reference/load-dataset.html#split-detection",
    "title": "load_dataset API",
    "section": "Split Detection",
    "text": "Split Detection\nSplits are automatically detected from filenames and directories:\n\n\n\nPattern\nDetected Split\n\n\n\n\ntrain-*.tar, training-*.tar\ntrain\n\n\ntest-*.tar, testing-*.tar\ntest\n\n\nval-*.tar, valid-*.tar, validation-*.tar\nvalidation\n\n\ndev-*.tar, development-*.tar\nvalidation\n\n\ntrain/*.tar (directory)\ntrain\n\n\ntest/*.tar (directory)\ntest\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFiles without a detected split default to “train”.",
    "crumbs": [
      "Guide",
      "Reference",
      "load_dataset API"
    ]
  },
  {
    "objectID": "reference/load-dataset.html#datasetdict",
    "href": "reference/load-dataset.html#datasetdict",
    "title": "load_dataset API",
    "section": "DatasetDict",
    "text": "DatasetDict\nWhen loading without split=, returns a DatasetDict:\n\nds_dict = load_dataset(\"path/to/data/\", MySample)\n\n# Access splits\ntrain_ds = ds_dict[\"train\"]\ntest_ds = ds_dict[\"test\"]\n\n# Iterate splits\nfor name, dataset in ds_dict.items():\n    print(f\"{name}: {len(dataset.shard_list)} shards\")\n\n# Properties\nprint(ds_dict.num_shards)    # {'train': 10, 'test': 2}\nprint(ds_dict.sample_type)   # &lt;class 'MySample'&gt;\nprint(ds_dict.streaming)     # False",
    "crumbs": [
      "Guide",
      "Reference",
      "load_dataset API"
    ]
  },
  {
    "objectID": "reference/load-dataset.html#explicit-data-files",
    "href": "reference/load-dataset.html#explicit-data-files",
    "title": "load_dataset API",
    "section": "Explicit Data Files",
    "text": "Explicit Data Files\nOverride automatic detection with data_files:\n\n# Single pattern\nds = load_dataset(\n    \"path/to/\",\n    MySample,\n    data_files=\"custom-*.tar\",\n)\n\n# List of patterns\nds = load_dataset(\n    \"path/to/\",\n    MySample,\n    data_files=[\"shard-000.tar\", \"shard-001.tar\"],\n)\n\n# Explicit split mapping\nds = load_dataset(\n    \"path/to/\",\n    MySample,\n    data_files={\n        \"train\": \"training-shards-*.tar\",\n        \"test\": \"eval-data.tar\",\n    },\n)",
    "crumbs": [
      "Guide",
      "Reference",
      "load_dataset API"
    ]
  },
  {
    "objectID": "reference/load-dataset.html#streaming-mode",
    "href": "reference/load-dataset.html#streaming-mode",
    "title": "load_dataset API",
    "section": "Streaming Mode",
    "text": "Streaming Mode\nThe streaming parameter signals intent for streaming mode:\n\n# Mark as streaming\nds_dict = load_dataset(\"path/to/data.tar\", MySample, streaming=True)\n\n# Check streaming status\nif ds_dict.streaming:\n    print(\"Streaming mode\")\n\n\n\n\n\n\n\nTip\n\n\n\natdata datasets are always lazy/streaming via WebDataset pipelines. This parameter primarily signals intent.",
    "crumbs": [
      "Guide",
      "Reference",
      "load_dataset API"
    ]
  },
  {
    "objectID": "reference/load-dataset.html#auto-type-resolution",
    "href": "reference/load-dataset.html#auto-type-resolution",
    "title": "load_dataset API",
    "section": "Auto Type Resolution",
    "text": "Auto Type Resolution\nWhen using index lookup, the sample type can be resolved automatically:\n\nfrom atdata.local import LocalIndex\n\nindex = LocalIndex()\n\n# No sample_type needed - resolved from schema\nds = load_dataset(\"@local/my-dataset\", index=index, split=\"train\")\n\n# Type is inferred from the stored schema\nsample_type = ds.sample_type",
    "crumbs": [
      "Guide",
      "Reference",
      "load_dataset API"
    ]
  },
  {
    "objectID": "reference/load-dataset.html#error-handling",
    "href": "reference/load-dataset.html#error-handling",
    "title": "load_dataset API",
    "section": "Error Handling",
    "text": "Error Handling\n\ntry:\n    ds = load_dataset(\"path/to/data.tar\", MySample, split=\"train\")\nexcept FileNotFoundError:\n    print(\"No data files found\")\nexcept ValueError as e:\n    if \"Split\" in str(e):\n        print(\"Requested split not found\")\n    else:\n        print(f\"Invalid configuration: {e}\")\nexcept KeyError:\n    print(\"Dataset not found in index\")",
    "crumbs": [
      "Guide",
      "Reference",
      "load_dataset API"
    ]
  },
  {
    "objectID": "reference/load-dataset.html#complete-example",
    "href": "reference/load-dataset.html#complete-example",
    "title": "load_dataset API",
    "section": "Complete Example",
    "text": "Complete Example\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport atdata\nfrom atdata import load_dataset\nimport webdataset as wds\n\n# 1. Define sample type\n@atdata.packable\nclass ImageSample:\n    image: NDArray\n    label: str\n\n# 2. Create dataset files\nfor split in [\"train\", \"test\"]:\n    with wds.writer.TarWriter(f\"{split}-000.tar\") as sink:\n        for i in range(100):\n            sample = ImageSample(\n                image=np.random.rand(64, 64, 3).astype(np.float32),\n                label=f\"sample_{i}\",\n            )\n            sink.write({**sample.as_wds, \"__key__\": f\"{i:06d}\"})\n\n# 3. Load with split detection\nds_dict = load_dataset(\"./\", ImageSample)\nprint(ds_dict.keys())  # dict_keys(['train', 'test'])\n\n# 4. Iterate\nfor batch in ds_dict[\"train\"].ordered(batch_size=16):\n    print(batch.image.shape)  # (16, 64, 64, 3)\n    print(batch.label)        # ['sample_0', 'sample_1', ...]\n    break\n\n# 5. Load specific split\ntrain_ds = load_dataset(\"./\", ImageSample, split=\"train\")\nfor batch in train_ds.ordered(batch_size=32):\n    process(batch)",
    "crumbs": [
      "Guide",
      "Reference",
      "load_dataset API"
    ]
  },
  {
    "objectID": "reference/load-dataset.html#related",
    "href": "reference/load-dataset.html#related",
    "title": "load_dataset API",
    "section": "Related",
    "text": "Related\n\nDatasets - Dataset iteration and batching\nPackable Samples - Defining sample types\nLocal Storage - LocalIndex for index lookup\nProtocols - AbstractIndex interface",
    "crumbs": [
      "Guide",
      "Reference",
      "load_dataset API"
    ]
  },
  {
    "objectID": "reference/promotion.html",
    "href": "reference/promotion.html",
    "title": "Promotion Workflow",
    "section": "",
    "text": "The promotion workflow migrates datasets from local storage (Redis + S3) to the ATProto atmosphere network, enabling federation and discovery.",
    "crumbs": [
      "Guide",
      "Reference",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "reference/promotion.html#overview",
    "href": "reference/promotion.html#overview",
    "title": "Promotion Workflow",
    "section": "Overview",
    "text": "Overview\nPromotion handles:\n\nSchema deduplication: Avoids publishing duplicate schemas\nData URL preservation: Keeps existing S3 URLs or copies to new storage\nMetadata transfer: Preserves tags, descriptions, and custom metadata",
    "crumbs": [
      "Guide",
      "Reference",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "reference/promotion.html#basic-usage",
    "href": "reference/promotion.html#basic-usage",
    "title": "Promotion Workflow",
    "section": "Basic Usage",
    "text": "Basic Usage\n\nfrom atdata.local import LocalIndex\nfrom atdata.atmosphere import AtmosphereClient\nfrom atdata.promote import promote_to_atmosphere\n\n# Setup\nlocal_index = LocalIndex()\nclient = AtmosphereClient()\nclient.login(\"handle.bsky.social\", \"app-password\")\n\n# Get local entry\nentry = local_index.get_entry_by_name(\"my-dataset\")\n\n# Promote to atmosphere\nat_uri = promote_to_atmosphere(entry, local_index, client)\nprint(f\"Published: {at_uri}\")",
    "crumbs": [
      "Guide",
      "Reference",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "reference/promotion.html#with-metadata",
    "href": "reference/promotion.html#with-metadata",
    "title": "Promotion Workflow",
    "section": "With Metadata",
    "text": "With Metadata\n\nat_uri = promote_to_atmosphere(\n    entry,\n    local_index,\n    client,\n    name=\"my-dataset-v2\",           # Override name\n    description=\"Training images\",  # Add description\n    tags=[\"images\", \"training\"],    # Add discovery tags\n    license=\"MIT\",                  # Specify license\n)",
    "crumbs": [
      "Guide",
      "Reference",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "reference/promotion.html#schema-deduplication",
    "href": "reference/promotion.html#schema-deduplication",
    "title": "Promotion Workflow",
    "section": "Schema Deduplication",
    "text": "Schema Deduplication\nThe promotion workflow automatically checks for existing schemas:\n\n# First promotion: publishes schema\nuri1 = promote_to_atmosphere(entry1, local_index, client)\n\n# Second promotion with same schema type + version: reuses existing schema\nuri2 = promote_to_atmosphere(entry2, local_index, client)\n\nSchema matching is based on:\n\n{module}.{class_name} (e.g., mymodule.ImageSample)\nVersion string (e.g., 1.0.0)",
    "crumbs": [
      "Guide",
      "Reference",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "reference/promotion.html#data-storage-options",
    "href": "reference/promotion.html#data-storage-options",
    "title": "Promotion Workflow",
    "section": "Data Storage Options",
    "text": "Data Storage Options\n\nKeep Existing URLs (Default)Copy to New Storage\n\n\nBy default, promotion keeps the original data URLs:\n\n# Data stays in original S3 location\nat_uri = promote_to_atmosphere(entry, local_index, client)\n\n\nData stays in original S3 location\nDataset record points to existing URLs\nFastest option, no data copying\nRequires original storage to remain accessible\n\n\n\nTo copy data to a different storage location:\n\nfrom atdata.local import S3DataStore\n\n# Create new data store\nnew_store = S3DataStore(\n    credentials=\"new-s3-creds.env\",\n    bucket=\"public-datasets\",\n)\n\n# Promote with data copy\nat_uri = promote_to_atmosphere(\n    entry,\n    local_index,\n    client,\n    data_store=new_store,  # Copy data to new storage\n)\n\n\nData is copied to new bucket\nDataset record points to new URLs\nGood for moving from private to public storage",
    "crumbs": [
      "Guide",
      "Reference",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "reference/promotion.html#complete-workflow-example",
    "href": "reference/promotion.html#complete-workflow-example",
    "title": "Promotion Workflow",
    "section": "Complete Workflow Example",
    "text": "Complete Workflow Example\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport atdata\nfrom atdata.local import LocalIndex, S3DataStore\nfrom atdata.atmosphere import AtmosphereClient\nfrom atdata.promote import promote_to_atmosphere\nimport webdataset as wds\n\n# 1. Define sample type\n@atdata.packable\nclass FeatureSample:\n    features: NDArray\n    label: int\n\n# 2. Create local dataset\nsamples = [\n    FeatureSample(\n        features=np.random.randn(128).astype(np.float32),\n        label=i % 10,\n    )\n    for i in range(1000)\n]\n\nwith wds.writer.TarWriter(\"features.tar\") as sink:\n    for i, s in enumerate(samples):\n        sink.write({**s.as_wds, \"__key__\": f\"{i:06d}\"})\n\n# 3. Set up index with S3 data store\nstore = S3DataStore(\n    credentials={\n        \"AWS_ENDPOINT\": \"http://localhost:9000\",\n        \"AWS_ACCESS_KEY_ID\": \"minioadmin\",\n        \"AWS_SECRET_ACCESS_KEY\": \"minioadmin\",\n    },\n    bucket=\"datasets-bucket\",\n)\nlocal_index = LocalIndex(data_store=store)\n\n# 4. Publish schema and insert dataset\nlocal_index.publish_schema(FeatureSample, version=\"1.0.0\")\ndataset = atdata.Dataset[FeatureSample](\"features.tar\")\nlocal_entry = local_index.insert_dataset(dataset, name=\"feature-vectors-v1\", prefix=\"features\")\n\n# 5. Promote to atmosphere\nclient = AtmosphereClient()\nclient.login(\"myhandle.bsky.social\", \"app-password\")\n\nat_uri = promote_to_atmosphere(\n    local_entry,\n    local_index,\n    client,\n    description=\"Feature vectors for classification\",\n    tags=[\"features\", \"embeddings\"],\n    license=\"MIT\",\n)\n\nprint(f\"Dataset published: {at_uri}\")\n\n# 6. Verify on atmosphere\nfrom atdata.atmosphere import AtmosphereIndex\n\natm_index = AtmosphereIndex(client)\nentry = atm_index.get_dataset(at_uri)\nprint(f\"Name: {entry.name}\")\nprint(f\"Schema: {entry.schema_ref}\")\nprint(f\"URLs: {entry.data_urls}\")",
    "crumbs": [
      "Guide",
      "Reference",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "reference/promotion.html#error-handling",
    "href": "reference/promotion.html#error-handling",
    "title": "Promotion Workflow",
    "section": "Error Handling",
    "text": "Error Handling\n\ntry:\n    at_uri = promote_to_atmosphere(entry, local_index, client)\nexcept KeyError as e:\n    # Schema not found in local index\n    print(f\"Missing schema: {e}\")\nexcept ValueError as e:\n    # Entry has no data URLs\n    print(f\"Invalid entry: {e}\")",
    "crumbs": [
      "Guide",
      "Reference",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "reference/promotion.html#requirements",
    "href": "reference/promotion.html#requirements",
    "title": "Promotion Workflow",
    "section": "Requirements",
    "text": "Requirements\nBefore promotion:\n\nDataset must be in local index (via Index.insert_dataset() or Index.add_entry())\nSchema must be published to local index (via Index.publish_schema())\nAtmosphereClient must be authenticated",
    "crumbs": [
      "Guide",
      "Reference",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "reference/promotion.html#related",
    "href": "reference/promotion.html#related",
    "title": "Promotion Workflow",
    "section": "Related",
    "text": "Related\n\nLocal Storage - Setting up local datasets\nAtmosphere - ATProto integration\nProtocols - AbstractIndex and AbstractDataStore",
    "crumbs": [
      "Guide",
      "Reference",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/local-workflow.html",
    "href": "tutorials/local-workflow.html",
    "title": "Local Workflow",
    "section": "",
    "text": "This tutorial demonstrates how to use the local storage module to store and index datasets using Redis and S3-compatible storage.",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Local Workflow"
    ]
  },
  {
    "objectID": "tutorials/local-workflow.html#prerequisites",
    "href": "tutorials/local-workflow.html#prerequisites",
    "title": "Local Workflow",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRedis server running (default: localhost:6379)\nS3-compatible storage (MinIO, AWS S3, etc.)\n\n\n\n\n\n\n\nTip\n\n\n\nFor local development, you can use MinIO:\ndocker run -p 9000:9000 minio/minio server /data",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Local Workflow"
    ]
  },
  {
    "objectID": "tutorials/local-workflow.html#setup",
    "href": "tutorials/local-workflow.html#setup",
    "title": "Local Workflow",
    "section": "Setup",
    "text": "Setup\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport atdata\nfrom atdata.local import LocalIndex, LocalDatasetEntry, S3DataStore\nimport webdataset as wds",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Local Workflow"
    ]
  },
  {
    "objectID": "tutorials/local-workflow.html#define-sample-types",
    "href": "tutorials/local-workflow.html#define-sample-types",
    "title": "Local Workflow",
    "section": "Define Sample Types",
    "text": "Define Sample Types\n\n@atdata.packable\nclass TrainingSample:\n    \"\"\"A sample containing features and label for training.\"\"\"\n    features: NDArray\n    label: int\n\n@atdata.packable\nclass TextSample:\n    \"\"\"A sample containing text data.\"\"\"\n    text: str\n    category: str",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Local Workflow"
    ]
  },
  {
    "objectID": "tutorials/local-workflow.html#localdatasetentry",
    "href": "tutorials/local-workflow.html#localdatasetentry",
    "title": "Local Workflow",
    "section": "LocalDatasetEntry",
    "text": "LocalDatasetEntry\nCreate entries with content-addressable CIDs:\n\n# Create an entry manually\nentry = LocalDatasetEntry(\n    _name=\"my-dataset\",\n    _schema_ref=\"local://schemas/examples.TrainingSample@1.0.0\",\n    _data_urls=[\"s3://bucket/data-000000.tar\", \"s3://bucket/data-000001.tar\"],\n    _metadata={\"source\": \"example\", \"samples\": 10000},\n)\n\nprint(f\"Entry name: {entry.name}\")\nprint(f\"Schema ref: {entry.schema_ref}\")\nprint(f\"Data URLs: {entry.data_urls}\")\nprint(f\"Metadata: {entry.metadata}\")\nprint(f\"CID: {entry.cid}\")\n\n\n\n\n\n\n\nNote\n\n\n\nCIDs are generated from content (schema_ref + data_urls), so identical data produces identical CIDs.",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Local Workflow"
    ]
  },
  {
    "objectID": "tutorials/local-workflow.html#localindex",
    "href": "tutorials/local-workflow.html#localindex",
    "title": "Local Workflow",
    "section": "LocalIndex",
    "text": "LocalIndex\nThe index tracks datasets in Redis:\n\nfrom redis import Redis\n\n# Connect to Redis\nredis = Redis(host=\"localhost\", port=6379)\nindex = LocalIndex(redis=redis)\n\nprint(\"LocalIndex connected\")\n\n\nSchema Management\n\n# Publish a schema\nschema_ref = index.publish_schema(TrainingSample, version=\"1.0.0\")\nprint(f\"Published schema: {schema_ref}\")\n\n# List all schemas\nfor schema in index.list_schemas():\n    print(f\"  - {schema.get('name', 'Unknown')} v{schema.get('version', '?')}\")\n\n# Get schema record\nschema_record = index.get_schema(schema_ref)\nprint(f\"Schema fields: {[f['name'] for f in schema_record.get('fields', [])]}\")\n\n# Decode schema back to a PackableSample class\ndecoded_type = index.decode_schema(schema_ref)\nprint(f\"Decoded type: {decoded_type.__name__}\")",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Local Workflow"
    ]
  },
  {
    "objectID": "tutorials/local-workflow.html#s3datastore",
    "href": "tutorials/local-workflow.html#s3datastore",
    "title": "Local Workflow",
    "section": "S3DataStore",
    "text": "S3DataStore\nFor direct S3 operations:\n\ncreds = {\n    \"AWS_ENDPOINT\": \"http://localhost:9000\",\n    \"AWS_ACCESS_KEY_ID\": \"minioadmin\",\n    \"AWS_SECRET_ACCESS_KEY\": \"minioadmin\",\n}\n\nstore = S3DataStore(creds, bucket=\"my-bucket\")\n\nprint(f\"Bucket: {store.bucket}\")\nprint(f\"Supports streaming: {store.supports_streaming()}\")",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Local Workflow"
    ]
  },
  {
    "objectID": "tutorials/local-workflow.html#complete-index-workflow",
    "href": "tutorials/local-workflow.html#complete-index-workflow",
    "title": "Local Workflow",
    "section": "Complete Index Workflow",
    "text": "Complete Index Workflow\nUse LocalIndex with S3DataStore to store datasets with S3 storage and Redis indexing:\n\n# 1. Create sample data\nsamples = [\n    TrainingSample(\n        features=np.random.randn(128).astype(np.float32),\n        label=i % 10\n    )\n    for i in range(1000)\n]\nprint(f\"Created {len(samples)} training samples\")\n\n# 2. Write to local tar file\nwith wds.writer.TarWriter(\"local-data-000000.tar\") as sink:\n    for i, sample in enumerate(samples):\n        sink.write({**sample.as_wds, \"__key__\": f\"sample_{i:06d}\"})\nprint(\"Wrote samples to local tar file\")\n\n# 3. Create Dataset\nds = atdata.Dataset[TrainingSample](\"local-data-000000.tar\")\n\n# 4. Set up index with S3 data store and insert\nstore = S3DataStore(\n    credentials={\n        \"AWS_ENDPOINT\": \"http://localhost:9000\",\n        \"AWS_ACCESS_KEY_ID\": \"minioadmin\",\n        \"AWS_SECRET_ACCESS_KEY\": \"minioadmin\",\n    },\n    bucket=\"my-bucket\",\n)\nindex = LocalIndex(redis=redis, data_store=store)\n\n# Publish schema and insert dataset\nindex.publish_schema(TrainingSample, version=\"1.0.0\")\nentry = index.insert_dataset(ds, name=\"training-v1\", prefix=\"datasets\")\nprint(f\"Stored at: {entry.data_urls}\")\nprint(f\"CID: {entry.cid}\")\n\n# 5. Retrieve later\nretrieved_entry = index.get_entry_by_name(\"training-v1\")\ndataset = atdata.Dataset[TrainingSample](retrieved_entry.data_urls[0])\n\nfor batch in dataset.ordered(batch_size=32):\n    print(f\"Batch features shape: {batch.features.shape}\")\n    break",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Local Workflow"
    ]
  },
  {
    "objectID": "tutorials/local-workflow.html#using-load_dataset-with-index",
    "href": "tutorials/local-workflow.html#using-load_dataset-with-index",
    "title": "Local Workflow",
    "section": "Using load_dataset with Index",
    "text": "Using load_dataset with Index\nThe load_dataset() function supports index lookup:\n\nfrom atdata import load_dataset\n\n# Load from local index\nds = load_dataset(\"@local/my-dataset\", index=index, split=\"train\")\n\n# The index resolves the dataset name to URLs and schema\nfor batch in ds.shuffled(batch_size=32):\n    process(batch)\n    break",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Local Workflow"
    ]
  },
  {
    "objectID": "tutorials/local-workflow.html#next-steps",
    "href": "tutorials/local-workflow.html#next-steps",
    "title": "Local Workflow",
    "section": "Next Steps",
    "text": "Next Steps\n\nAtmosphere Publishing - Publish to ATProto federation\nPromotion Workflow - Migrate from local to atmosphere\nLocal Storage Reference - Complete API reference",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Local Workflow"
    ]
  },
  {
    "objectID": "tutorials/promotion.html",
    "href": "tutorials/promotion.html",
    "title": "Promotion Workflow",
    "section": "",
    "text": "This tutorial demonstrates the workflow for migrating datasets from local Redis/S3 storage to the federated ATProto atmosphere network.",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/promotion.html#overview",
    "href": "tutorials/promotion.html#overview",
    "title": "Promotion Workflow",
    "section": "Overview",
    "text": "Overview\nThe promotion workflow moves datasets from local storage to the atmosphere:\nLOCAL                           ATMOSPHERE\n-----                           ----------\nRedis Index                     ATProto PDS\nS3 Storage            --&gt;       (same S3 or new location)\nlocal://schemas/...             at://did:plc:.../schema/...\nKey features:\n\nSchema deduplication: Won’t republish identical schemas\nFlexible data handling: Keep existing URLs or copy to new storage\nMetadata preservation: Local metadata carries over to atmosphere",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/promotion.html#setup",
    "href": "tutorials/promotion.html#setup",
    "title": "Promotion Workflow",
    "section": "Setup",
    "text": "Setup\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport atdata\nfrom atdata.local import LocalIndex, LocalDatasetEntry, S3DataStore\nfrom atdata.atmosphere import AtmosphereClient\nfrom atdata.promote import promote_to_atmosphere\nimport webdataset as wds",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/promotion.html#prepare-a-local-dataset",
    "href": "tutorials/promotion.html#prepare-a-local-dataset",
    "title": "Promotion Workflow",
    "section": "Prepare a Local Dataset",
    "text": "Prepare a Local Dataset\nFirst, set up a dataset in local storage:\n\n# 1. Define sample type\n@atdata.packable\nclass ExperimentSample:\n    \"\"\"A sample from a scientific experiment.\"\"\"\n    measurement: NDArray\n    timestamp: float\n    sensor_id: str\n\n# 2. Create samples\nsamples = [\n    ExperimentSample(\n        measurement=np.random.randn(64).astype(np.float32),\n        timestamp=float(i),\n        sensor_id=f\"sensor_{i % 4}\",\n    )\n    for i in range(1000)\n]\n\n# 3. Write to tar\nwith wds.writer.TarWriter(\"experiment.tar\") as sink:\n    for i, s in enumerate(samples):\n        sink.write({**s.as_wds, \"__key__\": f\"{i:06d}\"})\n\n# 4. Set up local index with S3 storage\nstore = S3DataStore(\n    credentials={\n        \"AWS_ENDPOINT\": \"http://localhost:9000\",\n        \"AWS_ACCESS_KEY_ID\": \"minioadmin\",\n        \"AWS_SECRET_ACCESS_KEY\": \"minioadmin\",\n    },\n    bucket=\"datasets-bucket\",\n)\nlocal_index = LocalIndex(data_store=store)\n\n# 5. Insert dataset into index\ndataset = atdata.Dataset[ExperimentSample](\"experiment.tar\")\nlocal_entry = local_index.insert_dataset(dataset, name=\"experiment-2024-001\", prefix=\"experiments\")\n\n# 6. Publish schema to local index\nlocal_index.publish_schema(ExperimentSample, version=\"1.0.0\")\n\nprint(f\"Local entry name: {local_entry.name}\")\nprint(f\"Local entry CID: {local_entry.cid}\")\nprint(f\"Data URLs: {local_entry.data_urls}\")",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/promotion.html#basic-promotion",
    "href": "tutorials/promotion.html#basic-promotion",
    "title": "Promotion Workflow",
    "section": "Basic Promotion",
    "text": "Basic Promotion\nPromote the dataset to ATProto:\n\n# Connect to atmosphere\nclient = AtmosphereClient()\nclient.login(\"myhandle.bsky.social\", \"app-password\")\n\n# Promote to atmosphere\nat_uri = promote_to_atmosphere(local_entry, local_index, client)\nprint(f\"Published: {at_uri}\")",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/promotion.html#promotion-with-metadata",
    "href": "tutorials/promotion.html#promotion-with-metadata",
    "title": "Promotion Workflow",
    "section": "Promotion with Metadata",
    "text": "Promotion with Metadata\nAdd description, tags, and license:\n\nat_uri = promote_to_atmosphere(\n    local_entry,\n    local_index,\n    client,\n    name=\"experiment-2024-001-v2\",   # Override name\n    description=\"Sensor measurements from Lab 302\",\n    tags=[\"experiment\", \"physics\", \"2024\"],\n    license=\"CC-BY-4.0\",\n)\nprint(f\"Published with metadata: {at_uri}\")",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/promotion.html#schema-deduplication",
    "href": "tutorials/promotion.html#schema-deduplication",
    "title": "Promotion Workflow",
    "section": "Schema Deduplication",
    "text": "Schema Deduplication\nThe promotion workflow automatically checks for existing schemas:\n\nfrom atdata.promote import _find_existing_schema\n\n# Check if schema already exists\nexisting = _find_existing_schema(client, \"ExperimentSample\", \"1.0.0\")\nif existing:\n    print(f\"Found existing schema: {existing}\")\n    print(\"Will reuse instead of republishing\")\nelse:\n    print(\"No existing schema found, will publish new one\")\n\nWhen you promote multiple datasets with the same sample type:\n\n# First promotion: publishes schema\nuri1 = promote_to_atmosphere(entry1, local_index, client)\n\n# Second promotion with same schema type + version: reuses existing schema\nuri2 = promote_to_atmosphere(entry2, local_index, client)",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/promotion.html#data-migration-options",
    "href": "tutorials/promotion.html#data-migration-options",
    "title": "Promotion Workflow",
    "section": "Data Migration Options",
    "text": "Data Migration Options\n\nKeep Existing URLsCopy to New Storage\n\n\nBy default, promotion keeps the original data URLs:\n\n# Data stays in original S3 location\nat_uri = promote_to_atmosphere(local_entry, local_index, client)\n\nBenefits:\n\nFastest option, no data copying\nDataset record points to existing URLs\nRequires original storage to remain accessible\n\n\n\nTo copy data to a different storage location:\n\nfrom atdata.local import S3DataStore\n\n# Create new data store\nnew_store = S3DataStore(\n    credentials=\"new-s3-creds.env\",\n    bucket=\"public-datasets\",\n)\n\n# Promote with data copy\nat_uri = promote_to_atmosphere(\n    local_entry,\n    local_index,\n    client,\n    data_store=new_store,  # Copy data to new storage\n)\n\nBenefits:\n\nData is copied to new bucket\nGood for moving from private to public storage\nOriginal storage can be retired",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/promotion.html#verify-on-atmosphere",
    "href": "tutorials/promotion.html#verify-on-atmosphere",
    "title": "Promotion Workflow",
    "section": "Verify on Atmosphere",
    "text": "Verify on Atmosphere\nAfter promotion, verify the dataset is accessible:\n\nfrom atdata.atmosphere import AtmosphereIndex\n\natm_index = AtmosphereIndex(client)\nentry = atm_index.get_dataset(at_uri)\n\nprint(f\"Name: {entry.name}\")\nprint(f\"Schema: {entry.schema_ref}\")\nprint(f\"URLs: {entry.data_urls}\")\n\n# Load and iterate\nSampleType = atm_index.decode_schema(entry.schema_ref)\nds = atdata.Dataset[SampleType](entry.data_urls[0])\n\nfor batch in ds.ordered(batch_size=32):\n    print(f\"Measurement shape: {batch.measurement.shape}\")\n    break",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/promotion.html#error-handling",
    "href": "tutorials/promotion.html#error-handling",
    "title": "Promotion Workflow",
    "section": "Error Handling",
    "text": "Error Handling\n\ntry:\n    at_uri = promote_to_atmosphere(local_entry, local_index, client)\nexcept KeyError as e:\n    # Schema not found in local index\n    print(f\"Missing schema: {e}\")\n    print(\"Publish schema first: local_index.publish_schema(SampleType)\")\nexcept ValueError as e:\n    # Entry has no data URLs\n    print(f\"Invalid entry: {e}\")",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/promotion.html#requirements-checklist",
    "href": "tutorials/promotion.html#requirements-checklist",
    "title": "Promotion Workflow",
    "section": "Requirements Checklist",
    "text": "Requirements Checklist\nBefore promotion:\n\nDataset is in local index (via LocalIndex.insert_dataset() or LocalIndex.add_entry())\nSchema is published to local index (via LocalIndex.publish_schema())\nAtmosphereClient is authenticated\nData URLs are publicly accessible (or will be copied)",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/promotion.html#complete-workflow",
    "href": "tutorials/promotion.html#complete-workflow",
    "title": "Promotion Workflow",
    "section": "Complete Workflow",
    "text": "Complete Workflow\n\n# Complete local-to-atmosphere workflow\nimport numpy as np\nfrom numpy.typing import NDArray\nimport atdata\nfrom atdata.local import LocalIndex, S3DataStore\nfrom atdata.atmosphere import AtmosphereClient, AtmosphereIndex\nfrom atdata.promote import promote_to_atmosphere\nimport webdataset as wds\n\n# 1. Define sample type\n@atdata.packable\nclass FeatureSample:\n    features: NDArray\n    label: int\n\n# 2. Create dataset tar\nsamples = [\n    FeatureSample(\n        features=np.random.randn(128).astype(np.float32),\n        label=i % 10,\n    )\n    for i in range(1000)\n]\n\nwith wds.writer.TarWriter(\"features.tar\") as sink:\n    for i, s in enumerate(samples):\n        sink.write({**s.as_wds, \"__key__\": f\"{i:06d}\"})\n\n# 3. Store in local index with S3 backend\nstore = S3DataStore(credentials=\"creds.env\", bucket=\"bucket\")\nlocal_index = LocalIndex(data_store=store)\n\ndataset = atdata.Dataset[FeatureSample](\"features.tar\")\nlocal_entry = local_index.insert_dataset(dataset, name=\"feature-vectors-v1\", prefix=\"features\")\n\n# 4. Publish schema locally\nlocal_index.publish_schema(FeatureSample, version=\"1.0.0\")\n\n# 5. Promote to atmosphere\nclient = AtmosphereClient()\nclient.login(\"myhandle.bsky.social\", \"app-password\")\n\nat_uri = promote_to_atmosphere(\n    local_entry,\n    local_index,\n    client,\n    description=\"Feature vectors for classification\",\n    tags=[\"features\", \"embeddings\"],\n    license=\"MIT\",\n)\n\nprint(f\"Dataset published: {at_uri}\")\n\n# 6. Others can now discover and load\n# ds = atdata.load_dataset(\"@myhandle.bsky.social/feature-vectors-v1\")",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "tutorials/promotion.html#next-steps",
    "href": "tutorials/promotion.html#next-steps",
    "title": "Promotion Workflow",
    "section": "Next Steps",
    "text": "Next Steps\n\nAtmosphere Reference - Complete atmosphere API\nProtocols - Abstract interfaces\nLocal Storage - Local storage reference",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Promotion Workflow"
    ]
  },
  {
    "objectID": "api/DatasetDict.html",
    "href": "api/DatasetDict.html",
    "title": "DatasetDict",
    "section": "",
    "text": "DatasetDict(splits=None, sample_type=None, streaming=False)\nA dictionary of split names to Dataset instances.\nSimilar to HuggingFace’s DatasetDict, this provides a container for multiple dataset splits (train, test, validation, etc.) with convenience methods that operate across all splits.\nType Parameters: ST: The sample type for all datasets in this dict.\nExample: &gt;&gt;&gt; ds_dict = load_dataset(“path/to/data”, MyData) &gt;&gt;&gt; train = ds_dict[“train”] &gt;&gt;&gt; test = ds_dict[“test”] &gt;&gt;&gt; &gt;&gt;&gt; # Iterate over all splits &gt;&gt;&gt; for split_name, dataset in ds_dict.items(): … print(f”{split_name}: {len(dataset.shard_list)} shards”)\n\n\n\n\n\nName\nDescription\n\n\n\n\nnum_shards\nNumber of shards in each split.\n\n\nsample_type\nThe sample type for datasets in this dict.\n\n\nstreaming\nWhether this DatasetDict was loaded in streaming mode."
  },
  {
    "objectID": "api/DatasetDict.html#attributes",
    "href": "api/DatasetDict.html#attributes",
    "title": "DatasetDict",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nnum_shards\nNumber of shards in each split.\n\n\nsample_type\nThe sample type for datasets in this dict.\n\n\nstreaming\nWhether this DatasetDict was loaded in streaming mode."
  },
  {
    "objectID": "api/AtmosphereClient.html",
    "href": "api/AtmosphereClient.html",
    "title": "AtmosphereClient",
    "section": "",
    "text": "atmosphere.AtmosphereClient(base_url=None, *, _client=None)\nATProto client wrapper for atdata operations.\nThis class wraps the atproto SDK client and provides higher-level methods for working with atdata records (schemas, datasets, lenses).\nExample: &gt;&gt;&gt; client = AtmosphereClient() &gt;&gt;&gt; client.login(“alice.bsky.social”, “app-password”) &gt;&gt;&gt; print(client.did) ‘did:plc:…’\nNote: The password should be an app-specific password, not your main account password. Create app passwords in your Bluesky account settings.\n\n\n\n\n\nName\nDescription\n\n\n\n\ndid\nGet the DID of the authenticated user.\n\n\nhandle\nGet the handle of the authenticated user.\n\n\nis_authenticated\nCheck if the client has a valid session.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncreate_record\nCreate a record in the user’s repository.\n\n\ndelete_record\nDelete a record.\n\n\nexport_session\nExport the current session for later reuse.\n\n\nget_blob\nDownload a blob from a PDS.\n\n\nget_blob_url\nGet the direct URL for fetching a blob.\n\n\nget_record\nFetch a record by AT URI.\n\n\nlist_datasets\nList dataset records.\n\n\nlist_lenses\nList lens records.\n\n\nlist_records\nList records in a collection.\n\n\nlist_schemas\nList schema records.\n\n\nlogin\nAuthenticate with the ATProto PDS.\n\n\nlogin_with_session\nAuthenticate using an exported session string.\n\n\nput_record\nCreate or update a record at a specific key.\n\n\nupload_blob\nUpload binary data as a blob to the PDS.\n\n\n\n\n\natmosphere.AtmosphereClient.create_record(\n    collection,\n    record,\n    *,\n    rkey=None,\n    validate=False,\n)\nCreate a record in the user’s repository.\nArgs: collection: The NSID of the record collection (e.g., ‘ac.foundation.dataset.sampleSchema’). record: The record data. Must include a ‘$type’ field. rkey: Optional explicit record key. If not provided, a TID is generated. validate: Whether to validate against the Lexicon schema. Set to False for custom lexicons that the PDS doesn’t know about.\nReturns: The AT URI of the created record.\nRaises: ValueError: If not authenticated. atproto.exceptions.AtProtocolError: If record creation fails.\n\n\n\natmosphere.AtmosphereClient.delete_record(uri, *, swap_commit=None)\nDelete a record.\nArgs: uri: The AT URI of the record to delete. swap_commit: Optional CID for compare-and-swap delete.\nRaises: ValueError: If not authenticated. atproto.exceptions.AtProtocolError: If deletion fails.\n\n\n\natmosphere.AtmosphereClient.export_session()\nExport the current session for later reuse.\nReturns: Session string that can be passed to login_with_session().\nRaises: ValueError: If not authenticated.\n\n\n\natmosphere.AtmosphereClient.get_blob(did, cid)\nDownload a blob from a PDS.\nThis resolves the PDS endpoint from the DID document and fetches the blob directly from the PDS.\nArgs: did: The DID of the repository containing the blob. cid: The CID of the blob.\nReturns: The blob data as bytes.\nRaises: ValueError: If PDS endpoint cannot be resolved. requests.HTTPError: If blob fetch fails.\n\n\n\natmosphere.AtmosphereClient.get_blob_url(did, cid)\nGet the direct URL for fetching a blob.\nThis is useful for passing to WebDataset or other HTTP clients.\nArgs: did: The DID of the repository containing the blob. cid: The CID of the blob.\nReturns: The full URL for fetching the blob.\nRaises: ValueError: If PDS endpoint cannot be resolved.\n\n\n\natmosphere.AtmosphereClient.get_record(uri)\nFetch a record by AT URI.\nArgs: uri: The AT URI of the record.\nReturns: The record data as a dictionary.\nRaises: atproto.exceptions.AtProtocolError: If record not found.\n\n\n\natmosphere.AtmosphereClient.list_datasets(repo=None, limit=100)\nList dataset records.\nArgs: repo: The DID to query. Defaults to authenticated user. limit: Maximum number to return.\nReturns: List of dataset records.\n\n\n\natmosphere.AtmosphereClient.list_lenses(repo=None, limit=100)\nList lens records.\nArgs: repo: The DID to query. Defaults to authenticated user. limit: Maximum number to return.\nReturns: List of lens records.\n\n\n\natmosphere.AtmosphereClient.list_records(\n    collection,\n    *,\n    repo=None,\n    limit=100,\n    cursor=None,\n)\nList records in a collection.\nArgs: collection: The NSID of the record collection. repo: The DID of the repository to query. Defaults to the authenticated user’s repository. limit: Maximum number of records to return (default 100). cursor: Pagination cursor from a previous call.\nReturns: A tuple of (records, next_cursor). The cursor is None if there are no more records.\nRaises: ValueError: If repo is None and not authenticated.\n\n\n\natmosphere.AtmosphereClient.list_schemas(repo=None, limit=100)\nList schema records.\nArgs: repo: The DID to query. Defaults to authenticated user. limit: Maximum number to return.\nReturns: List of schema records.\n\n\n\natmosphere.AtmosphereClient.login(handle, password)\nAuthenticate with the ATProto PDS.\nArgs: handle: Your Bluesky handle (e.g., ‘alice.bsky.social’). password: App-specific password (not your main password).\nRaises: atproto.exceptions.AtProtocolError: If authentication fails.\n\n\n\natmosphere.AtmosphereClient.login_with_session(session_string)\nAuthenticate using an exported session string.\nThis allows reusing a session without re-authenticating, which helps avoid rate limits on session creation.\nArgs: session_string: Session string from export_session().\n\n\n\natmosphere.AtmosphereClient.put_record(\n    collection,\n    rkey,\n    record,\n    *,\n    validate=False,\n    swap_commit=None,\n)\nCreate or update a record at a specific key.\nArgs: collection: The NSID of the record collection. rkey: The record key. record: The record data. Must include a ‘$type’ field. validate: Whether to validate against the Lexicon schema. swap_commit: Optional CID for compare-and-swap update.\nReturns: The AT URI of the record.\nRaises: ValueError: If not authenticated. atproto.exceptions.AtProtocolError: If operation fails.\n\n\n\natmosphere.AtmosphereClient.upload_blob(\n    data,\n    mime_type='application/octet-stream',\n)\nUpload binary data as a blob to the PDS.\nArgs: data: Binary data to upload. mime_type: MIME type of the data (for reference, not enforced by PDS).\nReturns: A blob reference dict with keys: ‘$type’, ‘ref’, ‘mimeType’, ‘size’. This can be embedded directly in record fields.\nRaises: ValueError: If not authenticated. atproto.exceptions.AtProtocolError: If upload fails."
  },
  {
    "objectID": "api/AtmosphereClient.html#attributes",
    "href": "api/AtmosphereClient.html#attributes",
    "title": "AtmosphereClient",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndid\nGet the DID of the authenticated user.\n\n\nhandle\nGet the handle of the authenticated user.\n\n\nis_authenticated\nCheck if the client has a valid session."
  },
  {
    "objectID": "api/AtmosphereClient.html#methods",
    "href": "api/AtmosphereClient.html#methods",
    "title": "AtmosphereClient",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncreate_record\nCreate a record in the user’s repository.\n\n\ndelete_record\nDelete a record.\n\n\nexport_session\nExport the current session for later reuse.\n\n\nget_blob\nDownload a blob from a PDS.\n\n\nget_blob_url\nGet the direct URL for fetching a blob.\n\n\nget_record\nFetch a record by AT URI.\n\n\nlist_datasets\nList dataset records.\n\n\nlist_lenses\nList lens records.\n\n\nlist_records\nList records in a collection.\n\n\nlist_schemas\nList schema records.\n\n\nlogin\nAuthenticate with the ATProto PDS.\n\n\nlogin_with_session\nAuthenticate using an exported session string.\n\n\nput_record\nCreate or update a record at a specific key.\n\n\nupload_blob\nUpload binary data as a blob to the PDS.\n\n\n\n\n\natmosphere.AtmosphereClient.create_record(\n    collection,\n    record,\n    *,\n    rkey=None,\n    validate=False,\n)\nCreate a record in the user’s repository.\nArgs: collection: The NSID of the record collection (e.g., ‘ac.foundation.dataset.sampleSchema’). record: The record data. Must include a ‘$type’ field. rkey: Optional explicit record key. If not provided, a TID is generated. validate: Whether to validate against the Lexicon schema. Set to False for custom lexicons that the PDS doesn’t know about.\nReturns: The AT URI of the created record.\nRaises: ValueError: If not authenticated. atproto.exceptions.AtProtocolError: If record creation fails.\n\n\n\natmosphere.AtmosphereClient.delete_record(uri, *, swap_commit=None)\nDelete a record.\nArgs: uri: The AT URI of the record to delete. swap_commit: Optional CID for compare-and-swap delete.\nRaises: ValueError: If not authenticated. atproto.exceptions.AtProtocolError: If deletion fails.\n\n\n\natmosphere.AtmosphereClient.export_session()\nExport the current session for later reuse.\nReturns: Session string that can be passed to login_with_session().\nRaises: ValueError: If not authenticated.\n\n\n\natmosphere.AtmosphereClient.get_blob(did, cid)\nDownload a blob from a PDS.\nThis resolves the PDS endpoint from the DID document and fetches the blob directly from the PDS.\nArgs: did: The DID of the repository containing the blob. cid: The CID of the blob.\nReturns: The blob data as bytes.\nRaises: ValueError: If PDS endpoint cannot be resolved. requests.HTTPError: If blob fetch fails.\n\n\n\natmosphere.AtmosphereClient.get_blob_url(did, cid)\nGet the direct URL for fetching a blob.\nThis is useful for passing to WebDataset or other HTTP clients.\nArgs: did: The DID of the repository containing the blob. cid: The CID of the blob.\nReturns: The full URL for fetching the blob.\nRaises: ValueError: If PDS endpoint cannot be resolved.\n\n\n\natmosphere.AtmosphereClient.get_record(uri)\nFetch a record by AT URI.\nArgs: uri: The AT URI of the record.\nReturns: The record data as a dictionary.\nRaises: atproto.exceptions.AtProtocolError: If record not found.\n\n\n\natmosphere.AtmosphereClient.list_datasets(repo=None, limit=100)\nList dataset records.\nArgs: repo: The DID to query. Defaults to authenticated user. limit: Maximum number to return.\nReturns: List of dataset records.\n\n\n\natmosphere.AtmosphereClient.list_lenses(repo=None, limit=100)\nList lens records.\nArgs: repo: The DID to query. Defaults to authenticated user. limit: Maximum number to return.\nReturns: List of lens records.\n\n\n\natmosphere.AtmosphereClient.list_records(\n    collection,\n    *,\n    repo=None,\n    limit=100,\n    cursor=None,\n)\nList records in a collection.\nArgs: collection: The NSID of the record collection. repo: The DID of the repository to query. Defaults to the authenticated user’s repository. limit: Maximum number of records to return (default 100). cursor: Pagination cursor from a previous call.\nReturns: A tuple of (records, next_cursor). The cursor is None if there are no more records.\nRaises: ValueError: If repo is None and not authenticated.\n\n\n\natmosphere.AtmosphereClient.list_schemas(repo=None, limit=100)\nList schema records.\nArgs: repo: The DID to query. Defaults to authenticated user. limit: Maximum number to return.\nReturns: List of schema records.\n\n\n\natmosphere.AtmosphereClient.login(handle, password)\nAuthenticate with the ATProto PDS.\nArgs: handle: Your Bluesky handle (e.g., ‘alice.bsky.social’). password: App-specific password (not your main password).\nRaises: atproto.exceptions.AtProtocolError: If authentication fails.\n\n\n\natmosphere.AtmosphereClient.login_with_session(session_string)\nAuthenticate using an exported session string.\nThis allows reusing a session without re-authenticating, which helps avoid rate limits on session creation.\nArgs: session_string: Session string from export_session().\n\n\n\natmosphere.AtmosphereClient.put_record(\n    collection,\n    rkey,\n    record,\n    *,\n    validate=False,\n    swap_commit=None,\n)\nCreate or update a record at a specific key.\nArgs: collection: The NSID of the record collection. rkey: The record key. record: The record data. Must include a ‘$type’ field. validate: Whether to validate against the Lexicon schema. swap_commit: Optional CID for compare-and-swap update.\nReturns: The AT URI of the record.\nRaises: ValueError: If not authenticated. atproto.exceptions.AtProtocolError: If operation fails.\n\n\n\natmosphere.AtmosphereClient.upload_blob(\n    data,\n    mime_type='application/octet-stream',\n)\nUpload binary data as a blob to the PDS.\nArgs: data: Binary data to upload. mime_type: MIME type of the data (for reference, not enforced by PDS).\nReturns: A blob reference dict with keys: ‘$type’, ‘ref’, ‘mimeType’, ‘size’. This can be embedded directly in record fields.\nRaises: ValueError: If not authenticated. atproto.exceptions.AtProtocolError: If upload fails."
  },
  {
    "objectID": "api/DictSample.html",
    "href": "api/DictSample.html",
    "title": "DictSample",
    "section": "",
    "text": "DictSample(_data=None, **kwargs)\nDynamic sample type providing dict-like access to raw msgpack data.\nThis class is the default sample type for datasets when no explicit type is specified. It stores the raw unpacked msgpack data and provides both attribute-style (sample.field) and dict-style (sample[\"field\"]) access to fields.\nDictSample is useful for: - Exploring datasets without defining a schema first - Working with datasets that have variable schemas - Prototyping before committing to a typed schema\nTo convert to a typed schema, use Dataset.as_type() with a @packable-decorated class. Every @packable class automatically registers a lens from DictSample, making this conversion seamless.\nExample: &gt;&gt;&gt; ds = load_dataset(“path/to/data.tar”) # Returns DatasetDictSample &gt;&gt;&gt; for sample in ds.ordered(): … print(sample.some_field) # Attribute access … print(sample[“other_field”]) # Dict access … print(sample.keys()) # Inspect available fields … &gt;&gt;&gt; # Convert to typed schema &gt;&gt;&gt; typed_ds = ds.as_type(MyTypedSample)\nNote: NDArray fields are stored as raw bytes in DictSample. They are only converted to numpy arrays when accessed through a typed sample class.\n\n\n\n\n\nName\nDescription\n\n\n\n\nas_wds\nPack this sample’s data for writing to WebDataset.\n\n\npacked\nPack this sample’s data into msgpack bytes.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfrom_bytes\nCreate a DictSample from raw msgpack bytes.\n\n\nfrom_data\nCreate a DictSample from unpacked msgpack data.\n\n\nget\nGet a field value with optional default.\n\n\nitems\nReturn list of (field_name, value) tuples.\n\n\nkeys\nReturn list of field names.\n\n\nto_dict\nReturn a copy of the underlying data dictionary.\n\n\nvalues\nReturn list of field values.\n\n\n\n\n\nDictSample.from_bytes(bs)\nCreate a DictSample from raw msgpack bytes.\nArgs: bs: Raw bytes from a msgpack-serialized sample.\nReturns: New DictSample instance with the unpacked data.\n\n\n\nDictSample.from_data(data)\nCreate a DictSample from unpacked msgpack data.\nArgs: data: Dictionary with field names as keys.\nReturns: New DictSample instance wrapping the data.\n\n\n\nDictSample.get(key, default=None)\nGet a field value with optional default.\nArgs: key: Field name to access. default: Value to return if field doesn’t exist.\nReturns: The field value or default.\n\n\n\nDictSample.items()\nReturn list of (field_name, value) tuples.\n\n\n\nDictSample.keys()\nReturn list of field names.\n\n\n\nDictSample.to_dict()\nReturn a copy of the underlying data dictionary.\n\n\n\nDictSample.values()\nReturn list of field values."
  },
  {
    "objectID": "api/DictSample.html#attributes",
    "href": "api/DictSample.html#attributes",
    "title": "DictSample",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nas_wds\nPack this sample’s data for writing to WebDataset.\n\n\npacked\nPack this sample’s data into msgpack bytes."
  },
  {
    "objectID": "api/DictSample.html#methods",
    "href": "api/DictSample.html#methods",
    "title": "DictSample",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfrom_bytes\nCreate a DictSample from raw msgpack bytes.\n\n\nfrom_data\nCreate a DictSample from unpacked msgpack data.\n\n\nget\nGet a field value with optional default.\n\n\nitems\nReturn list of (field_name, value) tuples.\n\n\nkeys\nReturn list of field names.\n\n\nto_dict\nReturn a copy of the underlying data dictionary.\n\n\nvalues\nReturn list of field values.\n\n\n\n\n\nDictSample.from_bytes(bs)\nCreate a DictSample from raw msgpack bytes.\nArgs: bs: Raw bytes from a msgpack-serialized sample.\nReturns: New DictSample instance with the unpacked data.\n\n\n\nDictSample.from_data(data)\nCreate a DictSample from unpacked msgpack data.\nArgs: data: Dictionary with field names as keys.\nReturns: New DictSample instance wrapping the data.\n\n\n\nDictSample.get(key, default=None)\nGet a field value with optional default.\nArgs: key: Field name to access. default: Value to return if field doesn’t exist.\nReturns: The field value or default.\n\n\n\nDictSample.items()\nReturn list of (field_name, value) tuples.\n\n\n\nDictSample.keys()\nReturn list of field names.\n\n\n\nDictSample.to_dict()\nReturn a copy of the underlying data dictionary.\n\n\n\nDictSample.values()\nReturn list of field values."
  },
  {
    "objectID": "api/LensLoader.html",
    "href": "api/LensLoader.html",
    "title": "LensLoader",
    "section": "",
    "text": "atmosphere.LensLoader(client)\nLoads lens records from ATProto.\nThis class fetches lens transformation records. Note that actually using a lens requires installing the referenced code and importing it manually.\nExample: &gt;&gt;&gt; client = AtmosphereClient() &gt;&gt;&gt; loader = LensLoader(client) &gt;&gt;&gt; &gt;&gt;&gt; record = loader.get(“at://did:plc:abc/ac.foundation.dataset.lens/xyz”) &gt;&gt;&gt; print(record[“name”]) &gt;&gt;&gt; print(record[“sourceSchema”]) &gt;&gt;&gt; print(record.get(“getterCode”, {}).get(“repository”))\n\n\n\n\n\nName\nDescription\n\n\n\n\nfind_by_schemas\nFind lenses that transform between specific schemas.\n\n\nget\nFetch a lens record by AT URI.\n\n\nlist_all\nList lens records from a repository.\n\n\n\n\n\natmosphere.LensLoader.find_by_schemas(\n    source_schema_uri,\n    target_schema_uri=None,\n    repo=None,\n)\nFind lenses that transform between specific schemas.\nArgs: source_schema_uri: AT URI of the source schema. target_schema_uri: Optional AT URI of the target schema. If not provided, returns all lenses from the source. repo: The DID of the repository to search.\nReturns: List of matching lens records.\n\n\n\natmosphere.LensLoader.get(uri)\nFetch a lens record by AT URI.\nArgs: uri: The AT URI of the lens record.\nReturns: The lens record as a dictionary.\nRaises: ValueError: If the record is not a lens record.\n\n\n\natmosphere.LensLoader.list_all(repo=None, limit=100)\nList lens records from a repository.\nArgs: repo: The DID of the repository. Defaults to authenticated user. limit: Maximum number of records to return.\nReturns: List of lens records."
  },
  {
    "objectID": "api/LensLoader.html#methods",
    "href": "api/LensLoader.html#methods",
    "title": "LensLoader",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfind_by_schemas\nFind lenses that transform between specific schemas.\n\n\nget\nFetch a lens record by AT URI.\n\n\nlist_all\nList lens records from a repository.\n\n\n\n\n\natmosphere.LensLoader.find_by_schemas(\n    source_schema_uri,\n    target_schema_uri=None,\n    repo=None,\n)\nFind lenses that transform between specific schemas.\nArgs: source_schema_uri: AT URI of the source schema. target_schema_uri: Optional AT URI of the target schema. If not provided, returns all lenses from the source. repo: The DID of the repository to search.\nReturns: List of matching lens records.\n\n\n\natmosphere.LensLoader.get(uri)\nFetch a lens record by AT URI.\nArgs: uri: The AT URI of the lens record.\nReturns: The lens record as a dictionary.\nRaises: ValueError: If the record is not a lens record.\n\n\n\natmosphere.LensLoader.list_all(repo=None, limit=100)\nList lens records from a repository.\nArgs: repo: The DID of the repository. Defaults to authenticated user. limit: Maximum number of records to return.\nReturns: List of lens records."
  },
  {
    "objectID": "api/AtmosphereIndex.html",
    "href": "api/AtmosphereIndex.html",
    "title": "AtmosphereIndex",
    "section": "",
    "text": "atmosphere.AtmosphereIndex(client)\nATProto index implementing AbstractIndex protocol.\nWraps SchemaPublisher/Loader and DatasetPublisher/Loader to provide a unified interface compatible with LocalIndex.\nExample: &gt;&gt;&gt; client = AtmosphereClient() &gt;&gt;&gt; client.login(“handle.bsky.social”, “app-password”) &gt;&gt;&gt; &gt;&gt;&gt; index = AtmosphereIndex(client) &gt;&gt;&gt; schema_ref = index.publish_schema(MySample, version=“1.0.0”) &gt;&gt;&gt; entry = index.insert_dataset(dataset, name=“my-data”)\n\n\n\n\n\nName\nDescription\n\n\n\n\ndatasets\nLazily iterate over all dataset entries (AbstractIndex protocol).\n\n\nschemas\nLazily iterate over all schema records (AbstractIndex protocol).\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ndecode_schema\nReconstruct a Python type from a schema record.\n\n\nget_dataset\nGet a dataset by AT URI.\n\n\nget_schema\nGet a schema record by AT URI.\n\n\ninsert_dataset\nInsert a dataset into ATProto.\n\n\nlist_datasets\nGet all dataset entries as a materialized list (AbstractIndex protocol).\n\n\nlist_schemas\nGet all schema records as a materialized list (AbstractIndex protocol).\n\n\npublish_schema\nPublish a schema to ATProto.\n\n\n\n\n\natmosphere.AtmosphereIndex.decode_schema(ref)\nReconstruct a Python type from a schema record.\nArgs: ref: AT URI of the schema record.\nReturns: Dynamically generated Packable type.\nRaises: ValueError: If schema cannot be decoded.\n\n\n\natmosphere.AtmosphereIndex.get_dataset(ref)\nGet a dataset by AT URI.\nArgs: ref: AT URI of the dataset record.\nReturns: AtmosphereIndexEntry for the dataset.\nRaises: ValueError: If record is not a dataset.\n\n\n\natmosphere.AtmosphereIndex.get_schema(ref)\nGet a schema record by AT URI.\nArgs: ref: AT URI of the schema record.\nReturns: Schema record dictionary.\nRaises: ValueError: If record is not a schema.\n\n\n\natmosphere.AtmosphereIndex.insert_dataset(\n    ds,\n    *,\n    name,\n    schema_ref=None,\n    **kwargs,\n)\nInsert a dataset into ATProto.\nArgs: ds: The Dataset to publish. name: Human-readable name. schema_ref: Optional schema AT URI. If None, auto-publishes schema. **kwargs: Additional options (description, tags, license).\nReturns: AtmosphereIndexEntry for the inserted dataset.\n\n\n\natmosphere.AtmosphereIndex.list_datasets(repo=None)\nGet all dataset entries as a materialized list (AbstractIndex protocol).\nArgs: repo: DID of repository. Defaults to authenticated user.\nReturns: List of AtmosphereIndexEntry for each dataset.\n\n\n\natmosphere.AtmosphereIndex.list_schemas(repo=None)\nGet all schema records as a materialized list (AbstractIndex protocol).\nArgs: repo: DID of repository. Defaults to authenticated user.\nReturns: List of schema records as dictionaries.\n\n\n\natmosphere.AtmosphereIndex.publish_schema(\n    sample_type,\n    *,\n    version='1.0.0',\n    **kwargs,\n)\nPublish a schema to ATProto.\nArgs: sample_type: A Packable type (PackableSample subclass or @packable-decorated). version: Semantic version string. **kwargs: Additional options (description, metadata).\nReturns: AT URI of the schema record."
  },
  {
    "objectID": "api/AtmosphereIndex.html#attributes",
    "href": "api/AtmosphereIndex.html#attributes",
    "title": "AtmosphereIndex",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndatasets\nLazily iterate over all dataset entries (AbstractIndex protocol).\n\n\nschemas\nLazily iterate over all schema records (AbstractIndex protocol)."
  },
  {
    "objectID": "api/AtmosphereIndex.html#methods",
    "href": "api/AtmosphereIndex.html#methods",
    "title": "AtmosphereIndex",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndecode_schema\nReconstruct a Python type from a schema record.\n\n\nget_dataset\nGet a dataset by AT URI.\n\n\nget_schema\nGet a schema record by AT URI.\n\n\ninsert_dataset\nInsert a dataset into ATProto.\n\n\nlist_datasets\nGet all dataset entries as a materialized list (AbstractIndex protocol).\n\n\nlist_schemas\nGet all schema records as a materialized list (AbstractIndex protocol).\n\n\npublish_schema\nPublish a schema to ATProto.\n\n\n\n\n\natmosphere.AtmosphereIndex.decode_schema(ref)\nReconstruct a Python type from a schema record.\nArgs: ref: AT URI of the schema record.\nReturns: Dynamically generated Packable type.\nRaises: ValueError: If schema cannot be decoded.\n\n\n\natmosphere.AtmosphereIndex.get_dataset(ref)\nGet a dataset by AT URI.\nArgs: ref: AT URI of the dataset record.\nReturns: AtmosphereIndexEntry for the dataset.\nRaises: ValueError: If record is not a dataset.\n\n\n\natmosphere.AtmosphereIndex.get_schema(ref)\nGet a schema record by AT URI.\nArgs: ref: AT URI of the schema record.\nReturns: Schema record dictionary.\nRaises: ValueError: If record is not a schema.\n\n\n\natmosphere.AtmosphereIndex.insert_dataset(\n    ds,\n    *,\n    name,\n    schema_ref=None,\n    **kwargs,\n)\nInsert a dataset into ATProto.\nArgs: ds: The Dataset to publish. name: Human-readable name. schema_ref: Optional schema AT URI. If None, auto-publishes schema. **kwargs: Additional options (description, tags, license).\nReturns: AtmosphereIndexEntry for the inserted dataset.\n\n\n\natmosphere.AtmosphereIndex.list_datasets(repo=None)\nGet all dataset entries as a materialized list (AbstractIndex protocol).\nArgs: repo: DID of repository. Defaults to authenticated user.\nReturns: List of AtmosphereIndexEntry for each dataset.\n\n\n\natmosphere.AtmosphereIndex.list_schemas(repo=None)\nGet all schema records as a materialized list (AbstractIndex protocol).\nArgs: repo: DID of repository. Defaults to authenticated user.\nReturns: List of schema records as dictionaries.\n\n\n\natmosphere.AtmosphereIndex.publish_schema(\n    sample_type,\n    *,\n    version='1.0.0',\n    **kwargs,\n)\nPublish a schema to ATProto.\nArgs: sample_type: A Packable type (PackableSample subclass or @packable-decorated). version: Semantic version string. **kwargs: Additional options (description, metadata).\nReturns: AT URI of the schema record."
  },
  {
    "objectID": "api/DataSource.html",
    "href": "api/DataSource.html",
    "title": "DataSource",
    "section": "",
    "text": "DataSource()\nProtocol for data sources that provide streams to Dataset.\nA DataSource abstracts over different ways of accessing dataset shards: - URLSource: Standard WebDataset-compatible URLs (http, https, pipe, gs, etc.) - S3Source: S3-compatible storage with explicit credentials - BlobSource: ATProto blob references (future)\nThe key method is shards(), which yields (identifier, stream) pairs. These are fed directly to WebDataset’s tar_file_expander, bypassing URL resolution entirely. This enables: - Private S3 repos with credentials - Custom endpoints (Cloudflare R2, MinIO) - ATProto blob streaming - Any other source that can provide file-like objects\nExample: &gt;&gt;&gt; source = S3Source( … bucket=“my-bucket”, … keys=[“data-000.tar”, “data-001.tar”], … endpoint=“https://r2.example.com”, … credentials=creds, … ) &gt;&gt;&gt; ds = DatasetMySample &gt;&gt;&gt; for sample in ds.ordered(): … print(sample)\n\n\n\n\n\nName\nDescription\n\n\n\n\nshards\nLazily yield (identifier, stream) pairs for each shard.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nlist_shards\nGet list of shard identifiers without opening streams.\n\n\nopen_shard\nOpen a single shard by its identifier.\n\n\n\n\n\nDataSource.list_shards()\nGet list of shard identifiers without opening streams.\nUsed for metadata queries like counting shards without actually streaming data. Implementations should return identifiers that match what shards would yield.\nReturns: List of shard identifier strings.\n\n\n\nDataSource.open_shard(shard_id)\nOpen a single shard by its identifier.\nThis method enables random access to individual shards, which is required for PyTorch DataLoader worker splitting. Each worker opens only its assigned shards rather than iterating all shards.\nArgs: shard_id: Shard identifier from shard_list.\nReturns: File-like stream for reading the shard.\nRaises: KeyError: If shard_id is not in shard_list."
  },
  {
    "objectID": "api/DataSource.html#attributes",
    "href": "api/DataSource.html#attributes",
    "title": "DataSource",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nshards\nLazily yield (identifier, stream) pairs for each shard."
  },
  {
    "objectID": "api/DataSource.html#methods",
    "href": "api/DataSource.html#methods",
    "title": "DataSource",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nlist_shards\nGet list of shard identifiers without opening streams.\n\n\nopen_shard\nOpen a single shard by its identifier.\n\n\n\n\n\nDataSource.list_shards()\nGet list of shard identifiers without opening streams.\nUsed for metadata queries like counting shards without actually streaming data. Implementations should return identifiers that match what shards would yield.\nReturns: List of shard identifier strings.\n\n\n\nDataSource.open_shard(shard_id)\nOpen a single shard by its identifier.\nThis method enables random access to individual shards, which is required for PyTorch DataLoader worker splitting. Each worker opens only its assigned shards rather than iterating all shards.\nArgs: shard_id: Shard identifier from shard_list.\nReturns: File-like stream for reading the shard.\nRaises: KeyError: If shard_id is not in shard_list."
  },
  {
    "objectID": "api/DatasetLoader.html",
    "href": "api/DatasetLoader.html",
    "title": "DatasetLoader",
    "section": "",
    "text": "atmosphere.DatasetLoader(client)\nLoads dataset records from ATProto.\nThis class fetches dataset index records and can create Dataset objects from them. Note that loading a dataset requires having the corresponding Python class for the sample type.\nExample: &gt;&gt;&gt; client = AtmosphereClient() &gt;&gt;&gt; loader = DatasetLoader(client) &gt;&gt;&gt; &gt;&gt;&gt; # List available datasets &gt;&gt;&gt; datasets = loader.list() &gt;&gt;&gt; for ds in datasets: … print(ds[“name”], ds[“schemaRef”]) &gt;&gt;&gt; &gt;&gt;&gt; # Get a specific dataset record &gt;&gt;&gt; record = loader.get(“at://did:plc:abc/ac.foundation.dataset.record/xyz”)\n\n\n\n\n\nName\nDescription\n\n\n\n\nget\nFetch a dataset record by AT URI.\n\n\nget_blob_urls\nGet fetchable URLs for blob-stored dataset shards.\n\n\nget_blobs\nGet the blob references from a dataset record.\n\n\nget_metadata\nGet the metadata from a dataset record.\n\n\nget_storage_type\nGet the storage type of a dataset record.\n\n\nget_urls\nGet the WebDataset URLs from a dataset record.\n\n\nlist_all\nList dataset records from a repository.\n\n\nto_dataset\nCreate a Dataset object from an ATProto record.\n\n\n\n\n\natmosphere.DatasetLoader.get(uri)\nFetch a dataset record by AT URI.\nArgs: uri: The AT URI of the dataset record.\nReturns: The dataset record as a dictionary.\nRaises: ValueError: If the record is not a dataset record.\n\n\n\natmosphere.DatasetLoader.get_blob_urls(uri)\nGet fetchable URLs for blob-stored dataset shards.\nThis resolves the PDS endpoint and constructs URLs that can be used to fetch the blob data directly.\nArgs: uri: The AT URI of the dataset record.\nReturns: List of URLs for fetching the blob data.\nRaises: ValueError: If storage type is not blobs or PDS cannot be resolved.\n\n\n\natmosphere.DatasetLoader.get_blobs(uri)\nGet the blob references from a dataset record.\nArgs: uri: The AT URI of the dataset record.\nReturns: List of blob reference dicts with keys: $type, ref, mimeType, size.\nRaises: ValueError: If the storage type is not blobs.\n\n\n\natmosphere.DatasetLoader.get_metadata(uri)\nGet the metadata from a dataset record.\nArgs: uri: The AT URI of the dataset record.\nReturns: The metadata dictionary, or None if no metadata.\n\n\n\natmosphere.DatasetLoader.get_storage_type(uri)\nGet the storage type of a dataset record.\nArgs: uri: The AT URI of the dataset record.\nReturns: Either “external” or “blobs”.\nRaises: ValueError: If storage type is unknown.\n\n\n\natmosphere.DatasetLoader.get_urls(uri)\nGet the WebDataset URLs from a dataset record.\nArgs: uri: The AT URI of the dataset record.\nReturns: List of WebDataset URLs.\nRaises: ValueError: If the storage type is not external URLs.\n\n\n\natmosphere.DatasetLoader.list_all(repo=None, limit=100)\nList dataset records from a repository.\nArgs: repo: The DID of the repository. Defaults to authenticated user. limit: Maximum number of records to return.\nReturns: List of dataset records.\n\n\n\natmosphere.DatasetLoader.to_dataset(uri, sample_type)\nCreate a Dataset object from an ATProto record.\nThis method creates a Dataset instance from a published record. You must provide the sample type class, which should match the schema referenced by the record.\nSupports both external URL storage and ATProto blob storage.\nArgs: uri: The AT URI of the dataset record. sample_type: The Python class for the sample type.\nReturns: A Dataset instance configured from the record.\nRaises: ValueError: If no storage URLs can be resolved.\nExample: &gt;&gt;&gt; loader = DatasetLoader(client) &gt;&gt;&gt; dataset = loader.to_dataset(uri, MySampleType) &gt;&gt;&gt; for batch in dataset.shuffled(batch_size=32): … process(batch)"
  },
  {
    "objectID": "api/DatasetLoader.html#methods",
    "href": "api/DatasetLoader.html#methods",
    "title": "DatasetLoader",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget\nFetch a dataset record by AT URI.\n\n\nget_blob_urls\nGet fetchable URLs for blob-stored dataset shards.\n\n\nget_blobs\nGet the blob references from a dataset record.\n\n\nget_metadata\nGet the metadata from a dataset record.\n\n\nget_storage_type\nGet the storage type of a dataset record.\n\n\nget_urls\nGet the WebDataset URLs from a dataset record.\n\n\nlist_all\nList dataset records from a repository.\n\n\nto_dataset\nCreate a Dataset object from an ATProto record.\n\n\n\n\n\natmosphere.DatasetLoader.get(uri)\nFetch a dataset record by AT URI.\nArgs: uri: The AT URI of the dataset record.\nReturns: The dataset record as a dictionary.\nRaises: ValueError: If the record is not a dataset record.\n\n\n\natmosphere.DatasetLoader.get_blob_urls(uri)\nGet fetchable URLs for blob-stored dataset shards.\nThis resolves the PDS endpoint and constructs URLs that can be used to fetch the blob data directly.\nArgs: uri: The AT URI of the dataset record.\nReturns: List of URLs for fetching the blob data.\nRaises: ValueError: If storage type is not blobs or PDS cannot be resolved.\n\n\n\natmosphere.DatasetLoader.get_blobs(uri)\nGet the blob references from a dataset record.\nArgs: uri: The AT URI of the dataset record.\nReturns: List of blob reference dicts with keys: $type, ref, mimeType, size.\nRaises: ValueError: If the storage type is not blobs.\n\n\n\natmosphere.DatasetLoader.get_metadata(uri)\nGet the metadata from a dataset record.\nArgs: uri: The AT URI of the dataset record.\nReturns: The metadata dictionary, or None if no metadata.\n\n\n\natmosphere.DatasetLoader.get_storage_type(uri)\nGet the storage type of a dataset record.\nArgs: uri: The AT URI of the dataset record.\nReturns: Either “external” or “blobs”.\nRaises: ValueError: If storage type is unknown.\n\n\n\natmosphere.DatasetLoader.get_urls(uri)\nGet the WebDataset URLs from a dataset record.\nArgs: uri: The AT URI of the dataset record.\nReturns: List of WebDataset URLs.\nRaises: ValueError: If the storage type is not external URLs.\n\n\n\natmosphere.DatasetLoader.list_all(repo=None, limit=100)\nList dataset records from a repository.\nArgs: repo: The DID of the repository. Defaults to authenticated user. limit: Maximum number of records to return.\nReturns: List of dataset records.\n\n\n\natmosphere.DatasetLoader.to_dataset(uri, sample_type)\nCreate a Dataset object from an ATProto record.\nThis method creates a Dataset instance from a published record. You must provide the sample type class, which should match the schema referenced by the record.\nSupports both external URL storage and ATProto blob storage.\nArgs: uri: The AT URI of the dataset record. sample_type: The Python class for the sample type.\nReturns: A Dataset instance configured from the record.\nRaises: ValueError: If no storage URLs can be resolved.\nExample: &gt;&gt;&gt; loader = DatasetLoader(client) &gt;&gt;&gt; dataset = loader.to_dataset(uri, MySampleType) &gt;&gt;&gt; for batch in dataset.shuffled(batch_size=32): … process(batch)"
  },
  {
    "objectID": "api/Lens.html",
    "href": "api/Lens.html",
    "title": "lens",
    "section": "",
    "text": "lens\nLens-based type transformations for datasets.\nThis module implements a lens system for bidirectional transformations between different sample types. Lenses enable viewing a dataset through different type schemas without duplicating the underlying data.\nKey components:\n\nLens: Bidirectional transformation with getter (S -&gt; V) and optional putter (V, S -&gt; S)\nLensNetwork: Global singleton registry for lens transformations\n@lens: Decorator to create and register lens transformations\n\nLenses support the functional programming concept of composable, well-behaved transformations that satisfy lens laws (GetPut and PutGet).\nExample: &gt;&gt;&gt; @packable … class FullData: … name: str … age: int … embedding: NDArray … &gt;&gt;&gt; @packable … class NameOnly: … name: str … &gt;&gt;&gt; @lens … def name_view(full: FullData) -&gt; NameOnly: … return NameOnly(name=full.name) … &gt;&gt;&gt; @name_view.putter … def name_view_put(view: NameOnly, source: FullData) -&gt; FullData: … return FullData(name=view.name, age=source.age, … embedding=source.embedding) … &gt;&gt;&gt; ds = DatasetFullData &gt;&gt;&gt; ds_names = ds.as_type(NameOnly) # Uses registered lens\n\n\n\n\n\nName\nDescription\n\n\n\n\nLens\nA bidirectional transformation between two sample types.\n\n\nLensNetwork\nGlobal registry for lens transformations between sample types.\n\n\n\n\n\nlens.Lens(get, put=None)\nA bidirectional transformation between two sample types.\nA lens provides a way to view and update data of type S (source) as if it were type V (view). It consists of a getter that transforms S -&gt; V and an optional putter that transforms (V, S) -&gt; S, enabling updates to the view to be reflected back in the source.\nType Parameters: S: The source type, must derive from PackableSample. V: The view type, must derive from PackableSample.\nExample: &gt;&gt;&gt; @lens … def name_lens(full: FullData) -&gt; NameOnly: … return NameOnly(name=full.name) … &gt;&gt;&gt; @name_lens.putter … def name_lens_put(view: NameOnly, source: FullData) -&gt; FullData: … return FullData(name=view.name, age=source.age)\n\n\n\n\n\nName\nDescription\n\n\n\n\nget\nTransform the source into the view type.\n\n\nput\nUpdate the source based on a modified view.\n\n\nputter\nDecorator to register a putter function for this lens.\n\n\n\n\n\nlens.Lens.get(s)\nTransform the source into the view type.\nArgs: s: The source sample of type S.\nReturns: A view of the source as type V.\n\n\n\nlens.Lens.put(v, s)\nUpdate the source based on a modified view.\nArgs: v: The modified view of type V. s: The original source of type S.\nReturns: An updated source of type S that reflects changes from the view.\n\n\n\nlens.Lens.putter(put)\nDecorator to register a putter function for this lens.\nArgs: put: A function that takes a view of type V and source of type S, and returns an updated source of type S.\nReturns: The putter function, allowing this to be used as a decorator.\nExample: &gt;&gt;&gt; @my_lens.putter … def my_lens_put(view: ViewType, source: SourceType) -&gt; SourceType: … return SourceType(…)\n\n\n\n\n\nlens.LensNetwork()\nGlobal registry for lens transformations between sample types.\nThis class implements a singleton pattern to maintain a global registry of all lenses decorated with @lens. It enables looking up transformations between different PackableSample types.\nAttributes: _instance: The singleton instance of this class. _registry: Dictionary mapping (source_type, view_type) tuples to their corresponding Lens objects.\n\n\n\n\n\nName\nDescription\n\n\n\n\nregister\nRegister a lens as the canonical transformation between two types.\n\n\ntransform\nLook up the lens transformation between two sample types.\n\n\n\n\n\nlens.LensNetwork.register(_lens)\nRegister a lens as the canonical transformation between two types.\nArgs: _lens: The lens to register. Will be stored in the registry under the key (_lens.source_type, _lens.view_type).\nNote: If a lens already exists for the same type pair, it will be overwritten.\n\n\n\nlens.LensNetwork.transform(source, view)\nLook up the lens transformation between two sample types.\nArgs: source: The source sample type (must derive from PackableSample). view: The target view type (must derive from PackableSample).\nReturns: The registered Lens that transforms from source to view.\nRaises: ValueError: If no lens has been registered for the given type pair.\nNote: Currently only supports direct transformations. Compositional transformations (chaining multiple lenses) are not yet implemented.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nlens\nDecorator to create and register a lens transformation.\n\n\n\n\n\nlens.lens(f)\nDecorator to create and register a lens transformation.\nThis decorator converts a getter function into a Lens object and automatically registers it in the global LensNetwork registry.\nArgs: f: A getter function that transforms from source type S to view type V. Must have exactly one parameter with a type annotation.\nReturns: A Lens[S, V] object that can be called to apply the transformation or decorated with @lens_name.putter to add a putter function.\nExample: &gt;&gt;&gt; @lens … def extract_name(full: FullData) -&gt; NameOnly: … return NameOnly(name=full.name) … &gt;&gt;&gt; @extract_name.putter … def extract_name_put(view: NameOnly, source: FullData) -&gt; FullData: … return FullData(name=view.name, age=source.age)"
  },
  {
    "objectID": "api/Lens.html#classes",
    "href": "api/Lens.html#classes",
    "title": "lens",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nLens\nA bidirectional transformation between two sample types.\n\n\nLensNetwork\nGlobal registry for lens transformations between sample types.\n\n\n\n\n\nlens.Lens(get, put=None)\nA bidirectional transformation between two sample types.\nA lens provides a way to view and update data of type S (source) as if it were type V (view). It consists of a getter that transforms S -&gt; V and an optional putter that transforms (V, S) -&gt; S, enabling updates to the view to be reflected back in the source.\nType Parameters: S: The source type, must derive from PackableSample. V: The view type, must derive from PackableSample.\nExample: &gt;&gt;&gt; @lens … def name_lens(full: FullData) -&gt; NameOnly: … return NameOnly(name=full.name) … &gt;&gt;&gt; @name_lens.putter … def name_lens_put(view: NameOnly, source: FullData) -&gt; FullData: … return FullData(name=view.name, age=source.age)\n\n\n\n\n\nName\nDescription\n\n\n\n\nget\nTransform the source into the view type.\n\n\nput\nUpdate the source based on a modified view.\n\n\nputter\nDecorator to register a putter function for this lens.\n\n\n\n\n\nlens.Lens.get(s)\nTransform the source into the view type.\nArgs: s: The source sample of type S.\nReturns: A view of the source as type V.\n\n\n\nlens.Lens.put(v, s)\nUpdate the source based on a modified view.\nArgs: v: The modified view of type V. s: The original source of type S.\nReturns: An updated source of type S that reflects changes from the view.\n\n\n\nlens.Lens.putter(put)\nDecorator to register a putter function for this lens.\nArgs: put: A function that takes a view of type V and source of type S, and returns an updated source of type S.\nReturns: The putter function, allowing this to be used as a decorator.\nExample: &gt;&gt;&gt; @my_lens.putter … def my_lens_put(view: ViewType, source: SourceType) -&gt; SourceType: … return SourceType(…)\n\n\n\n\n\nlens.LensNetwork()\nGlobal registry for lens transformations between sample types.\nThis class implements a singleton pattern to maintain a global registry of all lenses decorated with @lens. It enables looking up transformations between different PackableSample types.\nAttributes: _instance: The singleton instance of this class. _registry: Dictionary mapping (source_type, view_type) tuples to their corresponding Lens objects.\n\n\n\n\n\nName\nDescription\n\n\n\n\nregister\nRegister a lens as the canonical transformation between two types.\n\n\ntransform\nLook up the lens transformation between two sample types.\n\n\n\n\n\nlens.LensNetwork.register(_lens)\nRegister a lens as the canonical transformation between two types.\nArgs: _lens: The lens to register. Will be stored in the registry under the key (_lens.source_type, _lens.view_type).\nNote: If a lens already exists for the same type pair, it will be overwritten.\n\n\n\nlens.LensNetwork.transform(source, view)\nLook up the lens transformation between two sample types.\nArgs: source: The source sample type (must derive from PackableSample). view: The target view type (must derive from PackableSample).\nReturns: The registered Lens that transforms from source to view.\nRaises: ValueError: If no lens has been registered for the given type pair.\nNote: Currently only supports direct transformations. Compositional transformations (chaining multiple lenses) are not yet implemented."
  },
  {
    "objectID": "api/Lens.html#functions",
    "href": "api/Lens.html#functions",
    "title": "lens",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nlens\nDecorator to create and register a lens transformation.\n\n\n\n\n\nlens.lens(f)\nDecorator to create and register a lens transformation.\nThis decorator converts a getter function into a Lens object and automatically registers it in the global LensNetwork registry.\nArgs: f: A getter function that transforms from source type S to view type V. Must have exactly one parameter with a type annotation.\nReturns: A Lens[S, V] object that can be called to apply the transformation or decorated with @lens_name.putter to add a putter function.\nExample: &gt;&gt;&gt; @lens … def extract_name(full: FullData) -&gt; NameOnly: … return NameOnly(name=full.name) … &gt;&gt;&gt; @extract_name.putter … def extract_name_put(view: NameOnly, source: FullData) -&gt; FullData: … return FullData(name=view.name, age=source.age)"
  },
  {
    "objectID": "api/local.Index.html",
    "href": "api/local.Index.html",
    "title": "local.Index",
    "section": "",
    "text": "local.Index(\n    redis=None,\n    data_store=None,\n    auto_stubs=False,\n    stub_dir=None,\n    **kwargs,\n)\nRedis-backed index for tracking datasets in a repository.\nImplements the AbstractIndex protocol. Maintains a registry of LocalDatasetEntry objects in Redis, allowing enumeration and lookup of stored datasets.\nWhen initialized with a data_store, insert_dataset() will write dataset shards to storage before indexing. Without a data_store, insert_dataset() only indexes existing URLs.\nAttributes: _redis: Redis connection for index storage. _data_store: Optional AbstractDataStore for writing dataset shards.\n\n\n\n\n\nName\nDescription\n\n\n\n\nall_entries\nGet all index entries as a list (deprecated, use list_entries()).\n\n\ndata_store\nThe data store for writing shards, or None if index-only.\n\n\ndatasets\nLazily iterate over all dataset entries (AbstractIndex protocol).\n\n\nentries\nIterate over all index entries.\n\n\nschemas\nIterate over all schema records in this index.\n\n\nstub_dir\nDirectory where stub files are written, or None if auto-stubs disabled.\n\n\ntypes\nNamespace for accessing loaded schema types.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nadd_entry\nAdd a dataset to the index.\n\n\nclear_stubs\nRemove all auto-generated stub files.\n\n\ndecode_schema\nReconstruct a Python PackableSample type from a stored schema.\n\n\ndecode_schema_as\nDecode a schema with explicit type hint for IDE support.\n\n\nget_dataset\nGet a dataset entry by name (AbstractIndex protocol).\n\n\nget_entry\nGet an entry by its CID.\n\n\nget_entry_by_name\nGet an entry by its human-readable name.\n\n\nget_import_path\nGet the import path for a schema’s generated module.\n\n\nget_schema\nGet a schema record by reference (AbstractIndex protocol).\n\n\nget_schema_record\nGet a schema record as LocalSchemaRecord object.\n\n\ninsert_dataset\nInsert a dataset into the index (AbstractIndex protocol).\n\n\nlist_datasets\nGet all dataset entries as a materialized list (AbstractIndex protocol).\n\n\nlist_entries\nGet all index entries as a materialized list.\n\n\nlist_schemas\nGet all schema records as a materialized list (AbstractIndex protocol).\n\n\nload_schema\nLoad a schema and make it available in the types namespace.\n\n\npublish_schema\nPublish a schema for a sample type to Redis.\n\n\n\n\n\nlocal.Index.add_entry(ds, *, name, schema_ref=None, metadata=None)\nAdd a dataset to the index.\nCreates a LocalDatasetEntry for the dataset and persists it to Redis.\nArgs: ds: The dataset to add to the index. name: Human-readable name for the dataset. schema_ref: Optional schema reference. If None, generates from sample type. metadata: Optional metadata dictionary. If None, uses ds._metadata if available.\nReturns: The created LocalDatasetEntry object.\n\n\n\nlocal.Index.clear_stubs()\nRemove all auto-generated stub files.\nOnly works if auto_stubs was enabled when creating the Index.\nReturns: Number of stub files removed, or 0 if auto_stubs is disabled.\n\n\n\nlocal.Index.decode_schema(ref)\nReconstruct a Python PackableSample type from a stored schema.\nThis method enables loading datasets without knowing the sample type ahead of time. The index retrieves the schema record and dynamically generates a PackableSample subclass matching the schema definition.\nIf auto_stubs is enabled, a Python module will be generated and the class will be imported from it, providing full IDE autocomplete support. The returned class has proper type information that IDEs can understand.\nArgs: ref: Schema reference string (atdata://local/sampleSchema/… or legacy local://schemas/…).\nReturns: A PackableSample subclass - either imported from a generated module (if auto_stubs is enabled) or dynamically created.\nRaises: KeyError: If schema not found. ValueError: If schema cannot be decoded.\n\n\n\nlocal.Index.decode_schema_as(ref, type_hint)\nDecode a schema with explicit type hint for IDE support.\nThis is a typed wrapper around decode_schema() that preserves the type information for IDE autocomplete. Use this when you have a stub file for the schema and want full IDE support.\nArgs: ref: Schema reference string. type_hint: The stub type to use for type hints. Import this from the generated stub file.\nReturns: The decoded type, cast to match the type_hint for IDE support.\nExample: &gt;&gt;&gt; # After enabling auto_stubs and configuring IDE extraPaths: &gt;&gt;&gt; from local.MySample_1_0_0 import MySample &gt;&gt;&gt; &gt;&gt;&gt; # This gives full IDE autocomplete: &gt;&gt;&gt; DecodedType = index.decode_schema_as(ref, MySample) &gt;&gt;&gt; sample = DecodedType(text=“hello”, value=42) # IDE knows signature!\nNote: The type_hint is only used for static type checking - at runtime, the actual decoded type from the schema is returned. Ensure the stub matches the schema to avoid runtime surprises.\n\n\n\nlocal.Index.get_dataset(ref)\nGet a dataset entry by name (AbstractIndex protocol).\nArgs: ref: Dataset name.\nReturns: IndexEntry for the dataset.\nRaises: KeyError: If dataset not found.\n\n\n\nlocal.Index.get_entry(cid)\nGet an entry by its CID.\nArgs: cid: Content identifier of the entry.\nReturns: LocalDatasetEntry for the given CID.\nRaises: KeyError: If entry not found.\n\n\n\nlocal.Index.get_entry_by_name(name)\nGet an entry by its human-readable name.\nArgs: name: Human-readable name of the entry.\nReturns: LocalDatasetEntry with the given name.\nRaises: KeyError: If no entry with that name exists.\n\n\n\nlocal.Index.get_import_path(ref)\nGet the import path for a schema’s generated module.\nWhen auto_stubs is enabled, this returns the import path that can be used to import the schema type with full IDE support.\nArgs: ref: Schema reference string.\nReturns: Import path like “local.MySample_1_0_0”, or None if auto_stubs is disabled.\nExample: &gt;&gt;&gt; index = LocalIndex(auto_stubs=True) &gt;&gt;&gt; ref = index.publish_schema(MySample, version=“1.0.0”) &gt;&gt;&gt; index.load_schema(ref) &gt;&gt;&gt; print(index.get_import_path(ref)) local.MySample_1_0_0 &gt;&gt;&gt; # Then in your code: &gt;&gt;&gt; # from local.MySample_1_0_0 import MySample\n\n\n\nlocal.Index.get_schema(ref)\nGet a schema record by reference (AbstractIndex protocol).\nArgs: ref: Schema reference string. Supports both new format (atdata://local/sampleSchema/{name}@version) and legacy format (local://schemas/{module.Class}@version).\nReturns: Schema record as a dictionary with keys ‘name’, ‘version’, ‘fields’, ‘$ref’, etc.\nRaises: KeyError: If schema not found. ValueError: If reference format is invalid.\n\n\n\nlocal.Index.get_schema_record(ref)\nGet a schema record as LocalSchemaRecord object.\nUse this when you need the full LocalSchemaRecord with typed properties. For Protocol-compliant dict access, use get_schema() instead.\nArgs: ref: Schema reference string.\nReturns: LocalSchemaRecord with schema details.\nRaises: KeyError: If schema not found. ValueError: If reference format is invalid.\n\n\n\nlocal.Index.insert_dataset(ds, *, name, schema_ref=None, **kwargs)\nInsert a dataset into the index (AbstractIndex protocol).\nIf a data_store was provided at initialization, writes dataset shards to storage first, then indexes the new URLs. Otherwise, indexes the dataset’s existing URL.\nArgs: ds: The Dataset to register. name: Human-readable name for the dataset. schema_ref: Optional schema reference. **kwargs: Additional options: - metadata: Optional metadata dict - prefix: Storage prefix (default: dataset name) - cache_local: If True, cache writes locally first\nReturns: IndexEntry for the inserted dataset.\n\n\n\nlocal.Index.list_datasets()\nGet all dataset entries as a materialized list (AbstractIndex protocol).\nReturns: List of IndexEntry for each dataset.\n\n\n\nlocal.Index.list_entries()\nGet all index entries as a materialized list.\nReturns: List of all LocalDatasetEntry objects in the index.\n\n\n\nlocal.Index.list_schemas()\nGet all schema records as a materialized list (AbstractIndex protocol).\nReturns: List of schema records as dictionaries.\n\n\n\nlocal.Index.load_schema(ref)\nLoad a schema and make it available in the types namespace.\nThis method decodes the schema, optionally generates a Python module for IDE support (if auto_stubs is enabled), and registers the type in the :attr:types namespace for easy access.\nArgs: ref: Schema reference string (atdata://local/sampleSchema/… or legacy local://schemas/…).\nReturns: The decoded PackableSample subclass. Also available via index.types.&lt;ClassName&gt; after this call.\nRaises: KeyError: If schema not found. ValueError: If schema cannot be decoded.\nExample: &gt;&gt;&gt; # Load and use immediately &gt;&gt;&gt; MyType = index.load_schema(“atdata://local/sampleSchema/MySample@1.0.0”) &gt;&gt;&gt; sample = MyType(name=“hello”, value=42) &gt;&gt;&gt; &gt;&gt;&gt; # Or access later via namespace &gt;&gt;&gt; index.load_schema(“atdata://local/sampleSchema/OtherType@1.0.0”) &gt;&gt;&gt; other = index.types.OtherType(data=“test”)\n\n\n\nlocal.Index.publish_schema(sample_type, *, version=None, description=None)\nPublish a schema for a sample type to Redis.\nArgs: sample_type: The PackableSample subclass to publish. version: Semantic version string (e.g., ‘1.0.0’). If None, auto-increments from the latest published version (patch bump), or starts at ‘1.0.0’ if no previous version exists. description: Optional human-readable description. If None, uses the class docstring.\nReturns: Schema reference string: ‘atdata://local/sampleSchema/{name}@version’.\nRaises: ValueError: If sample_type is not a dataclass. TypeError: If a field type is not supported."
  },
  {
    "objectID": "api/local.Index.html#attributes",
    "href": "api/local.Index.html#attributes",
    "title": "local.Index",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nall_entries\nGet all index entries as a list (deprecated, use list_entries()).\n\n\ndata_store\nThe data store for writing shards, or None if index-only.\n\n\ndatasets\nLazily iterate over all dataset entries (AbstractIndex protocol).\n\n\nentries\nIterate over all index entries.\n\n\nschemas\nIterate over all schema records in this index.\n\n\nstub_dir\nDirectory where stub files are written, or None if auto-stubs disabled.\n\n\ntypes\nNamespace for accessing loaded schema types."
  },
  {
    "objectID": "api/local.Index.html#methods",
    "href": "api/local.Index.html#methods",
    "title": "local.Index",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nadd_entry\nAdd a dataset to the index.\n\n\nclear_stubs\nRemove all auto-generated stub files.\n\n\ndecode_schema\nReconstruct a Python PackableSample type from a stored schema.\n\n\ndecode_schema_as\nDecode a schema with explicit type hint for IDE support.\n\n\nget_dataset\nGet a dataset entry by name (AbstractIndex protocol).\n\n\nget_entry\nGet an entry by its CID.\n\n\nget_entry_by_name\nGet an entry by its human-readable name.\n\n\nget_import_path\nGet the import path for a schema’s generated module.\n\n\nget_schema\nGet a schema record by reference (AbstractIndex protocol).\n\n\nget_schema_record\nGet a schema record as LocalSchemaRecord object.\n\n\ninsert_dataset\nInsert a dataset into the index (AbstractIndex protocol).\n\n\nlist_datasets\nGet all dataset entries as a materialized list (AbstractIndex protocol).\n\n\nlist_entries\nGet all index entries as a materialized list.\n\n\nlist_schemas\nGet all schema records as a materialized list (AbstractIndex protocol).\n\n\nload_schema\nLoad a schema and make it available in the types namespace.\n\n\npublish_schema\nPublish a schema for a sample type to Redis.\n\n\n\n\n\nlocal.Index.add_entry(ds, *, name, schema_ref=None, metadata=None)\nAdd a dataset to the index.\nCreates a LocalDatasetEntry for the dataset and persists it to Redis.\nArgs: ds: The dataset to add to the index. name: Human-readable name for the dataset. schema_ref: Optional schema reference. If None, generates from sample type. metadata: Optional metadata dictionary. If None, uses ds._metadata if available.\nReturns: The created LocalDatasetEntry object.\n\n\n\nlocal.Index.clear_stubs()\nRemove all auto-generated stub files.\nOnly works if auto_stubs was enabled when creating the Index.\nReturns: Number of stub files removed, or 0 if auto_stubs is disabled.\n\n\n\nlocal.Index.decode_schema(ref)\nReconstruct a Python PackableSample type from a stored schema.\nThis method enables loading datasets without knowing the sample type ahead of time. The index retrieves the schema record and dynamically generates a PackableSample subclass matching the schema definition.\nIf auto_stubs is enabled, a Python module will be generated and the class will be imported from it, providing full IDE autocomplete support. The returned class has proper type information that IDEs can understand.\nArgs: ref: Schema reference string (atdata://local/sampleSchema/… or legacy local://schemas/…).\nReturns: A PackableSample subclass - either imported from a generated module (if auto_stubs is enabled) or dynamically created.\nRaises: KeyError: If schema not found. ValueError: If schema cannot be decoded.\n\n\n\nlocal.Index.decode_schema_as(ref, type_hint)\nDecode a schema with explicit type hint for IDE support.\nThis is a typed wrapper around decode_schema() that preserves the type information for IDE autocomplete. Use this when you have a stub file for the schema and want full IDE support.\nArgs: ref: Schema reference string. type_hint: The stub type to use for type hints. Import this from the generated stub file.\nReturns: The decoded type, cast to match the type_hint for IDE support.\nExample: &gt;&gt;&gt; # After enabling auto_stubs and configuring IDE extraPaths: &gt;&gt;&gt; from local.MySample_1_0_0 import MySample &gt;&gt;&gt; &gt;&gt;&gt; # This gives full IDE autocomplete: &gt;&gt;&gt; DecodedType = index.decode_schema_as(ref, MySample) &gt;&gt;&gt; sample = DecodedType(text=“hello”, value=42) # IDE knows signature!\nNote: The type_hint is only used for static type checking - at runtime, the actual decoded type from the schema is returned. Ensure the stub matches the schema to avoid runtime surprises.\n\n\n\nlocal.Index.get_dataset(ref)\nGet a dataset entry by name (AbstractIndex protocol).\nArgs: ref: Dataset name.\nReturns: IndexEntry for the dataset.\nRaises: KeyError: If dataset not found.\n\n\n\nlocal.Index.get_entry(cid)\nGet an entry by its CID.\nArgs: cid: Content identifier of the entry.\nReturns: LocalDatasetEntry for the given CID.\nRaises: KeyError: If entry not found.\n\n\n\nlocal.Index.get_entry_by_name(name)\nGet an entry by its human-readable name.\nArgs: name: Human-readable name of the entry.\nReturns: LocalDatasetEntry with the given name.\nRaises: KeyError: If no entry with that name exists.\n\n\n\nlocal.Index.get_import_path(ref)\nGet the import path for a schema’s generated module.\nWhen auto_stubs is enabled, this returns the import path that can be used to import the schema type with full IDE support.\nArgs: ref: Schema reference string.\nReturns: Import path like “local.MySample_1_0_0”, or None if auto_stubs is disabled.\nExample: &gt;&gt;&gt; index = LocalIndex(auto_stubs=True) &gt;&gt;&gt; ref = index.publish_schema(MySample, version=“1.0.0”) &gt;&gt;&gt; index.load_schema(ref) &gt;&gt;&gt; print(index.get_import_path(ref)) local.MySample_1_0_0 &gt;&gt;&gt; # Then in your code: &gt;&gt;&gt; # from local.MySample_1_0_0 import MySample\n\n\n\nlocal.Index.get_schema(ref)\nGet a schema record by reference (AbstractIndex protocol).\nArgs: ref: Schema reference string. Supports both new format (atdata://local/sampleSchema/{name}@version) and legacy format (local://schemas/{module.Class}@version).\nReturns: Schema record as a dictionary with keys ‘name’, ‘version’, ‘fields’, ‘$ref’, etc.\nRaises: KeyError: If schema not found. ValueError: If reference format is invalid.\n\n\n\nlocal.Index.get_schema_record(ref)\nGet a schema record as LocalSchemaRecord object.\nUse this when you need the full LocalSchemaRecord with typed properties. For Protocol-compliant dict access, use get_schema() instead.\nArgs: ref: Schema reference string.\nReturns: LocalSchemaRecord with schema details.\nRaises: KeyError: If schema not found. ValueError: If reference format is invalid.\n\n\n\nlocal.Index.insert_dataset(ds, *, name, schema_ref=None, **kwargs)\nInsert a dataset into the index (AbstractIndex protocol).\nIf a data_store was provided at initialization, writes dataset shards to storage first, then indexes the new URLs. Otherwise, indexes the dataset’s existing URL.\nArgs: ds: The Dataset to register. name: Human-readable name for the dataset. schema_ref: Optional schema reference. **kwargs: Additional options: - metadata: Optional metadata dict - prefix: Storage prefix (default: dataset name) - cache_local: If True, cache writes locally first\nReturns: IndexEntry for the inserted dataset.\n\n\n\nlocal.Index.list_datasets()\nGet all dataset entries as a materialized list (AbstractIndex protocol).\nReturns: List of IndexEntry for each dataset.\n\n\n\nlocal.Index.list_entries()\nGet all index entries as a materialized list.\nReturns: List of all LocalDatasetEntry objects in the index.\n\n\n\nlocal.Index.list_schemas()\nGet all schema records as a materialized list (AbstractIndex protocol).\nReturns: List of schema records as dictionaries.\n\n\n\nlocal.Index.load_schema(ref)\nLoad a schema and make it available in the types namespace.\nThis method decodes the schema, optionally generates a Python module for IDE support (if auto_stubs is enabled), and registers the type in the :attr:types namespace for easy access.\nArgs: ref: Schema reference string (atdata://local/sampleSchema/… or legacy local://schemas/…).\nReturns: The decoded PackableSample subclass. Also available via index.types.&lt;ClassName&gt; after this call.\nRaises: KeyError: If schema not found. ValueError: If schema cannot be decoded.\nExample: &gt;&gt;&gt; # Load and use immediately &gt;&gt;&gt; MyType = index.load_schema(“atdata://local/sampleSchema/MySample@1.0.0”) &gt;&gt;&gt; sample = MyType(name=“hello”, value=42) &gt;&gt;&gt; &gt;&gt;&gt; # Or access later via namespace &gt;&gt;&gt; index.load_schema(“atdata://local/sampleSchema/OtherType@1.0.0”) &gt;&gt;&gt; other = index.types.OtherType(data=“test”)\n\n\n\nlocal.Index.publish_schema(sample_type, *, version=None, description=None)\nPublish a schema for a sample type to Redis.\nArgs: sample_type: The PackableSample subclass to publish. version: Semantic version string (e.g., ‘1.0.0’). If None, auto-increments from the latest published version (patch bump), or starts at ‘1.0.0’ if no previous version exists. description: Optional human-readable description. If None, uses the class docstring.\nReturns: Schema reference string: ‘atdata://local/sampleSchema/{name}@version’.\nRaises: ValueError: If sample_type is not a dataclass. TypeError: If a field type is not supported."
  },
  {
    "objectID": "api/Dataset.html",
    "href": "api/Dataset.html",
    "title": "Dataset",
    "section": "",
    "text": "Dataset(source=None, metadata_url=None, *, url=None)\nA typed dataset built on WebDataset with lens transformations.\nThis class wraps WebDataset tar archives and provides type-safe iteration over samples of a specific PackableSample type. Samples are stored as msgpack-serialized data within WebDataset shards.\nThe dataset supports: - Ordered and shuffled iteration - Automatic batching with SampleBatch - Type transformations via the lens system (as_type()) - Export to parquet format\nType Parameters: ST: The sample type for this dataset, must derive from PackableSample.\nAttributes: url: WebDataset brace-notation URL for the tar file(s).\nExample: &gt;&gt;&gt; ds = DatasetMyData &gt;&gt;&gt; for sample in ds.ordered(batch_size=32): … # sample is SampleBatch[MyData] with batch_size samples … embeddings = sample.embeddings # shape: (32, …) … &gt;&gt;&gt; # Transform to a different view &gt;&gt;&gt; ds_view = ds.as_type(MyDataView)\nNote: This class uses Python’s __orig_class__ mechanism to extract the type parameter at runtime. Instances must be created using the subscripted syntax Dataset[MyType](url) rather than calling the constructor directly with an unsubscripted class.\n\n\n\n\n\nName\nDescription\n\n\n\n\nbatch_type\nThe type of batches produced by this dataset.\n\n\nmetadata\nFetch and cache metadata from metadata_url.\n\n\nmetadata_url\nOptional URL to msgpack-encoded metadata for this dataset.\n\n\nsample_type\nThe type of each returned sample from this dataset’s iterator.\n\n\nshard_list\nList of individual dataset shards (deprecated, use list_shards()).\n\n\nsource\nThe underlying data source for this dataset.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nas_type\nView this dataset through a different sample type using a registered lens.\n\n\nlist_shards\nGet list of individual dataset shards.\n\n\nordered\nIterate over the dataset in order\n\n\nshuffled\nIterate over the dataset in random order.\n\n\nto_parquet\nExport dataset contents to parquet format.\n\n\nwrap\nWrap a raw msgpack sample into the appropriate dataset-specific type.\n\n\nwrap_batch\nWrap a batch of raw msgpack samples into a typed SampleBatch.\n\n\n\n\n\nDataset.as_type(other)\nView this dataset through a different sample type using a registered lens.\nArgs: other: The target sample type to transform into. Must be a type derived from PackableSample.\nReturns: A new Dataset instance that yields samples of type other by applying the appropriate lens transformation from the global LensNetwork registry.\nRaises: ValueError: If no registered lens exists between the current sample type and the target type.\n\n\n\nDataset.list_shards()\nGet list of individual dataset shards.\nReturns: A full (non-lazy) list of the individual tar files within the source WebDataset.\n\n\n\nDataset.ordered(batch_size=None)\nIterate over the dataset in order\nArgs: batch_size (:obj:int, optional): The size of iterated batches. Default: None (unbatched). If None, iterates over one sample at a time with no batch dimension.\nReturns: :obj:webdataset.DataPipeline A data pipeline that iterates over the dataset in its original sample order\n\n\n\nDataset.shuffled(buffer_shards=100, buffer_samples=10000, batch_size=None)\nIterate over the dataset in random order.\nArgs: buffer_shards: Number of shards to buffer for shuffling at the shard level. Larger values increase randomness but use more memory. Default: 100. buffer_samples: Number of samples to buffer for shuffling within shards. Larger values increase randomness but use more memory. Default: 10,000. batch_size: The size of iterated batches. Default: None (unbatched). If None, iterates over one sample at a time with no batch dimension.\nReturns: A WebDataset data pipeline that iterates over the dataset in randomized order. If batch_size is not None, yields SampleBatch[ST] instances; otherwise yields individual ST samples.\n\n\n\nDataset.to_parquet(path, sample_map=None, maxcount=None, **kwargs)\nExport dataset contents to parquet format.\nConverts all samples to a pandas DataFrame and saves to parquet file(s). Useful for interoperability with data analysis tools.\nArgs: path: Output path for the parquet file. If maxcount is specified, files are named {stem}-{segment:06d}.parquet. sample_map: Optional function to convert samples to dictionaries. Defaults to dataclasses.asdict. maxcount: If specified, split output into multiple files with at most this many samples each. Recommended for large datasets. **kwargs: Additional arguments passed to pandas.DataFrame.to_parquet(). Common options include compression, index, engine.\nWarning: Memory Usage: When maxcount=None (default), this method loads the entire dataset into memory as a pandas DataFrame before writing. For large datasets, this can cause memory exhaustion.\nFor datasets larger than available RAM, always specify ``maxcount``::\n\n    # Safe for large datasets - processes in chunks\n    ds.to_parquet(\"output.parquet\", maxcount=10000)\n\nThis creates multiple parquet files: ``output-000000.parquet``,\n``output-000001.parquet``, etc.\nExample: &gt;&gt;&gt; ds = DatasetMySample &gt;&gt;&gt; # Small dataset - load all at once &gt;&gt;&gt; ds.to_parquet(“output.parquet”) &gt;&gt;&gt; &gt;&gt;&gt; # Large dataset - process in chunks &gt;&gt;&gt; ds.to_parquet(“output.parquet”, maxcount=50000)\n\n\n\nDataset.wrap(sample)\nWrap a raw msgpack sample into the appropriate dataset-specific type.\nArgs: sample: A dictionary containing at minimum a 'msgpack' key with serialized sample bytes.\nReturns: A deserialized sample of type ST, optionally transformed through a lens if as_type() was called.\n\n\n\nDataset.wrap_batch(batch)\nWrap a batch of raw msgpack samples into a typed SampleBatch.\nArgs: batch: A dictionary containing a 'msgpack' key with a list of serialized sample bytes.\nReturns: A SampleBatch[ST] containing deserialized samples, optionally transformed through a lens if as_type() was called.\nNote: This implementation deserializes samples one at a time, then aggregates them into a batch."
  },
  {
    "objectID": "api/Dataset.html#attributes",
    "href": "api/Dataset.html#attributes",
    "title": "Dataset",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nbatch_type\nThe type of batches produced by this dataset.\n\n\nmetadata\nFetch and cache metadata from metadata_url.\n\n\nmetadata_url\nOptional URL to msgpack-encoded metadata for this dataset.\n\n\nsample_type\nThe type of each returned sample from this dataset’s iterator.\n\n\nshard_list\nList of individual dataset shards (deprecated, use list_shards()).\n\n\nsource\nThe underlying data source for this dataset."
  },
  {
    "objectID": "api/Dataset.html#methods",
    "href": "api/Dataset.html#methods",
    "title": "Dataset",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nas_type\nView this dataset through a different sample type using a registered lens.\n\n\nlist_shards\nGet list of individual dataset shards.\n\n\nordered\nIterate over the dataset in order\n\n\nshuffled\nIterate over the dataset in random order.\n\n\nto_parquet\nExport dataset contents to parquet format.\n\n\nwrap\nWrap a raw msgpack sample into the appropriate dataset-specific type.\n\n\nwrap_batch\nWrap a batch of raw msgpack samples into a typed SampleBatch.\n\n\n\n\n\nDataset.as_type(other)\nView this dataset through a different sample type using a registered lens.\nArgs: other: The target sample type to transform into. Must be a type derived from PackableSample.\nReturns: A new Dataset instance that yields samples of type other by applying the appropriate lens transformation from the global LensNetwork registry.\nRaises: ValueError: If no registered lens exists between the current sample type and the target type.\n\n\n\nDataset.list_shards()\nGet list of individual dataset shards.\nReturns: A full (non-lazy) list of the individual tar files within the source WebDataset.\n\n\n\nDataset.ordered(batch_size=None)\nIterate over the dataset in order\nArgs: batch_size (:obj:int, optional): The size of iterated batches. Default: None (unbatched). If None, iterates over one sample at a time with no batch dimension.\nReturns: :obj:webdataset.DataPipeline A data pipeline that iterates over the dataset in its original sample order\n\n\n\nDataset.shuffled(buffer_shards=100, buffer_samples=10000, batch_size=None)\nIterate over the dataset in random order.\nArgs: buffer_shards: Number of shards to buffer for shuffling at the shard level. Larger values increase randomness but use more memory. Default: 100. buffer_samples: Number of samples to buffer for shuffling within shards. Larger values increase randomness but use more memory. Default: 10,000. batch_size: The size of iterated batches. Default: None (unbatched). If None, iterates over one sample at a time with no batch dimension.\nReturns: A WebDataset data pipeline that iterates over the dataset in randomized order. If batch_size is not None, yields SampleBatch[ST] instances; otherwise yields individual ST samples.\n\n\n\nDataset.to_parquet(path, sample_map=None, maxcount=None, **kwargs)\nExport dataset contents to parquet format.\nConverts all samples to a pandas DataFrame and saves to parquet file(s). Useful for interoperability with data analysis tools.\nArgs: path: Output path for the parquet file. If maxcount is specified, files are named {stem}-{segment:06d}.parquet. sample_map: Optional function to convert samples to dictionaries. Defaults to dataclasses.asdict. maxcount: If specified, split output into multiple files with at most this many samples each. Recommended for large datasets. **kwargs: Additional arguments passed to pandas.DataFrame.to_parquet(). Common options include compression, index, engine.\nWarning: Memory Usage: When maxcount=None (default), this method loads the entire dataset into memory as a pandas DataFrame before writing. For large datasets, this can cause memory exhaustion.\nFor datasets larger than available RAM, always specify ``maxcount``::\n\n    # Safe for large datasets - processes in chunks\n    ds.to_parquet(\"output.parquet\", maxcount=10000)\n\nThis creates multiple parquet files: ``output-000000.parquet``,\n``output-000001.parquet``, etc.\nExample: &gt;&gt;&gt; ds = DatasetMySample &gt;&gt;&gt; # Small dataset - load all at once &gt;&gt;&gt; ds.to_parquet(“output.parquet”) &gt;&gt;&gt; &gt;&gt;&gt; # Large dataset - process in chunks &gt;&gt;&gt; ds.to_parquet(“output.parquet”, maxcount=50000)\n\n\n\nDataset.wrap(sample)\nWrap a raw msgpack sample into the appropriate dataset-specific type.\nArgs: sample: A dictionary containing at minimum a 'msgpack' key with serialized sample bytes.\nReturns: A deserialized sample of type ST, optionally transformed through a lens if as_type() was called.\n\n\n\nDataset.wrap_batch(batch)\nWrap a batch of raw msgpack samples into a typed SampleBatch.\nArgs: batch: A dictionary containing a 'msgpack' key with a list of serialized sample bytes.\nReturns: A SampleBatch[ST] containing deserialized samples, optionally transformed through a lens if as_type() was called.\nNote: This implementation deserializes samples one at a time, then aggregates them into a batch."
  },
  {
    "objectID": "api/AbstractDataStore.html",
    "href": "api/AbstractDataStore.html",
    "title": "AbstractDataStore",
    "section": "",
    "text": "AbstractDataStore()\nProtocol for data storage operations.\nThis protocol abstracts over different storage backends for dataset data: - S3DataStore: S3-compatible object storage - PDSBlobStore: ATProto PDS blob storage (future)\nThe separation of index (metadata) from data store (actual files) allows flexible deployment: local index with S3 storage, atmosphere index with S3 storage, or atmosphere index with PDS blobs.\nExample: &gt;&gt;&gt; store = S3DataStore(credentials, bucket=“my-bucket”) &gt;&gt;&gt; urls = store.write_shards(dataset, prefix=“training/v1”) &gt;&gt;&gt; print(urls) [‘s3://my-bucket/training/v1/shard-000000.tar’, …]\n\n\n\n\n\nName\nDescription\n\n\n\n\nread_url\nResolve a storage URL for reading.\n\n\nsupports_streaming\nWhether this store supports streaming reads.\n\n\nwrite_shards\nWrite dataset shards to storage.\n\n\n\n\n\nAbstractDataStore.read_url(url)\nResolve a storage URL for reading.\nSome storage backends may need to transform URLs (e.g., signing S3 URLs or resolving blob references). This method returns a URL that can be used directly with WebDataset.\nArgs: url: Storage URL to resolve.\nReturns: WebDataset-compatible URL for reading.\n\n\n\nAbstractDataStore.supports_streaming()\nWhether this store supports streaming reads.\nReturns: True if the store supports efficient streaming (like S3), False if data must be fully downloaded first.\n\n\n\nAbstractDataStore.write_shards(ds, *, prefix, **kwargs)\nWrite dataset shards to storage.\nArgs: ds: The Dataset to write. prefix: Path prefix for the shards (e.g., ‘datasets/mnist/v1’). **kwargs: Backend-specific options (e.g., maxcount for shard size).\nReturns: List of URLs for the written shards, suitable for use with WebDataset or atdata.Dataset()."
  },
  {
    "objectID": "api/AbstractDataStore.html#methods",
    "href": "api/AbstractDataStore.html#methods",
    "title": "AbstractDataStore",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nread_url\nResolve a storage URL for reading.\n\n\nsupports_streaming\nWhether this store supports streaming reads.\n\n\nwrite_shards\nWrite dataset shards to storage.\n\n\n\n\n\nAbstractDataStore.read_url(url)\nResolve a storage URL for reading.\nSome storage backends may need to transform URLs (e.g., signing S3 URLs or resolving blob references). This method returns a URL that can be used directly with WebDataset.\nArgs: url: Storage URL to resolve.\nReturns: WebDataset-compatible URL for reading.\n\n\n\nAbstractDataStore.supports_streaming()\nWhether this store supports streaming reads.\nReturns: True if the store supports efficient streaming (like S3), False if data must be fully downloaded first.\n\n\n\nAbstractDataStore.write_shards(ds, *, prefix, **kwargs)\nWrite dataset shards to storage.\nArgs: ds: The Dataset to write. prefix: Path prefix for the shards (e.g., ‘datasets/mnist/v1’). **kwargs: Backend-specific options (e.g., maxcount for shard size).\nReturns: List of URLs for the written shards, suitable for use with WebDataset or atdata.Dataset()."
  },
  {
    "objectID": "api/local.S3DataStore.html",
    "href": "api/local.S3DataStore.html",
    "title": "local.S3DataStore",
    "section": "",
    "text": "local.S3DataStore(credentials, *, bucket)\nS3-compatible data store implementing AbstractDataStore protocol.\nHandles writing dataset shards to S3-compatible object storage and resolving URLs for reading.\nAttributes: credentials: S3 credentials dictionary. bucket: Target bucket name. _fs: S3FileSystem instance.\n\n\n\n\n\nName\nDescription\n\n\n\n\nread_url\nResolve an S3 URL for reading/streaming.\n\n\nsupports_streaming\nS3 supports streaming reads.\n\n\nwrite_shards\nWrite dataset shards to S3.\n\n\n\n\n\nlocal.S3DataStore.read_url(url)\nResolve an S3 URL for reading/streaming.\nFor S3-compatible stores with custom endpoints (like Cloudflare R2, MinIO, etc.), converts s3:// URLs to HTTPS URLs that WebDataset can stream directly.\nFor standard AWS S3 (no custom endpoint), URLs are returned unchanged since WebDataset’s built-in s3fs integration handles them.\nArgs: url: S3 URL to resolve (e.g., ‘s3://bucket/path/file.tar’).\nReturns: HTTPS URL if custom endpoint is configured, otherwise unchanged. Example: ‘s3://bucket/path’ -&gt; ‘https://endpoint.com/bucket/path’\n\n\n\nlocal.S3DataStore.supports_streaming()\nS3 supports streaming reads.\nReturns: True.\n\n\n\nlocal.S3DataStore.write_shards(ds, *, prefix, cache_local=False, **kwargs)\nWrite dataset shards to S3.\nArgs: ds: The Dataset to write. prefix: Path prefix within bucket (e.g., ‘datasets/mnist/v1’). cache_local: If True, write locally first then copy to S3. **kwargs: Additional args passed to wds.ShardWriter (e.g., maxcount).\nReturns: List of S3 URLs for the written shards.\nRaises: RuntimeError: If no shards were written."
  },
  {
    "objectID": "api/local.S3DataStore.html#methods",
    "href": "api/local.S3DataStore.html#methods",
    "title": "local.S3DataStore",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nread_url\nResolve an S3 URL for reading/streaming.\n\n\nsupports_streaming\nS3 supports streaming reads.\n\n\nwrite_shards\nWrite dataset shards to S3.\n\n\n\n\n\nlocal.S3DataStore.read_url(url)\nResolve an S3 URL for reading/streaming.\nFor S3-compatible stores with custom endpoints (like Cloudflare R2, MinIO, etc.), converts s3:// URLs to HTTPS URLs that WebDataset can stream directly.\nFor standard AWS S3 (no custom endpoint), URLs are returned unchanged since WebDataset’s built-in s3fs integration handles them.\nArgs: url: S3 URL to resolve (e.g., ‘s3://bucket/path/file.tar’).\nReturns: HTTPS URL if custom endpoint is configured, otherwise unchanged. Example: ‘s3://bucket/path’ -&gt; ‘https://endpoint.com/bucket/path’\n\n\n\nlocal.S3DataStore.supports_streaming()\nS3 supports streaming reads.\nReturns: True.\n\n\n\nlocal.S3DataStore.write_shards(ds, *, prefix, cache_local=False, **kwargs)\nWrite dataset shards to S3.\nArgs: ds: The Dataset to write. prefix: Path prefix within bucket (e.g., ‘datasets/mnist/v1’). cache_local: If True, write locally first then copy to S3. **kwargs: Additional args passed to wds.ShardWriter (e.g., maxcount).\nReturns: List of S3 URLs for the written shards.\nRaises: RuntimeError: If no shards were written."
  },
  {
    "objectID": "api/AtUri.html",
    "href": "api/AtUri.html",
    "title": "AtUri",
    "section": "",
    "text": "atmosphere.AtUri(authority, collection, rkey)\nParsed AT Protocol URI.\nAT URIs follow the format: at:////\nExample: &gt;&gt;&gt; uri = AtUri.parse(“at://did:plc:abc123/ac.foundation.dataset.sampleSchema/xyz”) &gt;&gt;&gt; uri.authority ‘did:plc:abc123’ &gt;&gt;&gt; uri.collection ‘ac.foundation.dataset.sampleSchema’ &gt;&gt;&gt; uri.rkey ‘xyz’\n\n\n\n\n\nName\nDescription\n\n\n\n\nauthority\nThe DID or handle of the repository owner.\n\n\ncollection\nThe NSID of the record collection.\n\n\nrkey\nThe record key within the collection.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nparse\nParse an AT URI string into components.\n\n\n\n\n\natmosphere.AtUri.parse(uri)\nParse an AT URI string into components.\nArgs: uri: AT URI string in format at://&lt;authority&gt;/&lt;collection&gt;/&lt;rkey&gt;\nReturns: Parsed AtUri instance.\nRaises: ValueError: If the URI format is invalid."
  },
  {
    "objectID": "api/AtUri.html#attributes",
    "href": "api/AtUri.html#attributes",
    "title": "AtUri",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nauthority\nThe DID or handle of the repository owner.\n\n\ncollection\nThe NSID of the record collection.\n\n\nrkey\nThe record key within the collection."
  },
  {
    "objectID": "api/AtUri.html#methods",
    "href": "api/AtUri.html#methods",
    "title": "AtUri",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nparse\nParse an AT URI string into components.\n\n\n\n\n\natmosphere.AtUri.parse(uri)\nParse an AT URI string into components.\nArgs: uri: AT URI string in format at://&lt;authority&gt;/&lt;collection&gt;/&lt;rkey&gt;\nReturns: Parsed AtUri instance.\nRaises: ValueError: If the URI format is invalid."
  },
  {
    "objectID": "api/Packable-protocol.html",
    "href": "api/Packable-protocol.html",
    "title": "Packable",
    "section": "",
    "text": "Packable()\nStructural protocol for packable sample types.\nThis protocol allows classes decorated with @packable to be recognized as valid types for lens transformations and schema operations, even though the decorator doesn’t change the class’s nominal type at static analysis time.\nBoth PackableSample subclasses and @packable-decorated classes satisfy this protocol structurally.\nThe protocol captures the full interface needed for: - Lens type transformations (as_wds, from_data) - Schema publishing (class introspection via dataclass fields) - Serialization/deserialization (packed, from_bytes)\nExample: &gt;&gt;&gt; @packable … class MySample: … name: str … value: int … &gt;&gt;&gt; def process(sample_type: TypePackable) -&gt; None: … # Type checker knows sample_type has from_bytes, packed, etc. … instance = sample_type.from_bytes(data) … print(instance.packed)\n\n\n\n\n\nName\nDescription\n\n\n\n\nas_wds\nWebDataset-compatible representation with key and msgpack.\n\n\npacked\nPack this sample’s data into msgpack bytes.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfrom_bytes\nCreate instance from raw msgpack bytes.\n\n\nfrom_data\nCreate instance from unpacked msgpack data dictionary.\n\n\n\n\n\nPackable.from_bytes(bs)\nCreate instance from raw msgpack bytes.\n\n\n\nPackable.from_data(data)\nCreate instance from unpacked msgpack data dictionary."
  },
  {
    "objectID": "api/Packable-protocol.html#attributes",
    "href": "api/Packable-protocol.html#attributes",
    "title": "Packable",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nas_wds\nWebDataset-compatible representation with key and msgpack.\n\n\npacked\nPack this sample’s data into msgpack bytes."
  },
  {
    "objectID": "api/Packable-protocol.html#methods",
    "href": "api/Packable-protocol.html#methods",
    "title": "Packable",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfrom_bytes\nCreate instance from raw msgpack bytes.\n\n\nfrom_data\nCreate instance from unpacked msgpack data dictionary.\n\n\n\n\n\nPackable.from_bytes(bs)\nCreate instance from raw msgpack bytes.\n\n\n\nPackable.from_data(data)\nCreate instance from unpacked msgpack data dictionary."
  },
  {
    "objectID": "api/packable.html",
    "href": "api/packable.html",
    "title": "packable",
    "section": "",
    "text": "packable\npackable(cls)\nDecorator to convert a regular class into a PackableSample.\nThis decorator transforms a class into a dataclass that inherits from PackableSample, enabling automatic msgpack serialization/deserialization with special handling for NDArray fields.\nThe resulting class satisfies the Packable protocol, making it compatible with all atdata APIs that accept packable types (e.g., publish_schema, lens transformations, etc.).\nArgs: cls: The class to convert. Should have type annotations for its fields.\nReturns: A new dataclass that inherits from PackableSample with the same name and annotations as the original class. The class satisfies the Packable protocol and can be used with Type[Packable] signatures.\nExample: &gt;&gt;&gt; @packable … class MyData: … name: str … values: NDArray … &gt;&gt;&gt; sample = MyData(name=“test”, values=np.array([1, 2, 3])) &gt;&gt;&gt; bytes_data = sample.packed &gt;&gt;&gt; restored = MyData.from_bytes(bytes_data) &gt;&gt;&gt; &gt;&gt;&gt; # Works with Packable-typed APIs &gt;&gt;&gt; index.publish_schema(MyData, version=“1.0.0”) # Type-safe"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "atdata",
    "section": "",
    "text": "A loose federation of distributed, typed datasets built on WebDataset.\nGet Started View on GitHub",
    "crumbs": [
      "Guide",
      "atdata"
    ]
  },
  {
    "objectID": "index.html#what-is-atdata",
    "href": "index.html#what-is-atdata",
    "title": "atdata",
    "section": "What is atdata?",
    "text": "What is atdata?\natdata provides a typed dataset abstraction for machine learning workflows with:\n\n\nTyped Samples\nDefine dataclass-based sample types with automatic msgpack serialization.\n\n\nNDArray Handling\nTransparent numpy array conversion with efficient byte serialization.\n\n\nLens Transformations\nView datasets through different schemas without duplicating data.\n\n\nBatch Aggregation\nAutomatic numpy stacking for NDArray fields during iteration.\n\n\nWebDataset Integration\nEfficient large-scale storage with streaming tar file support.\n\n\nATProto Federation\nPublish and discover datasets on the decentralized AT Protocol network.",
    "crumbs": [
      "Guide",
      "atdata"
    ]
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "atdata",
    "section": "Installation",
    "text": "Installation\n\npip install atdata\n\n# With ATProto support\npip install atdata[atmosphere]",
    "crumbs": [
      "Guide",
      "atdata"
    ]
  },
  {
    "objectID": "index.html#quick-example",
    "href": "index.html#quick-example",
    "title": "atdata",
    "section": "Quick Example",
    "text": "Quick Example\n\nDefine a Sample Type\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport atdata\n\n@atdata.packable\nclass ImageSample:\n    image: NDArray\n    label: str\n    confidence: float\n\n\n\nCreate and Write Samples\n\nimport webdataset as wds\n\nsamples = [\n    ImageSample(\n        image=np.random.rand(224, 224, 3).astype(np.float32),\n        label=\"cat\",\n        confidence=0.95,\n    )\n    for _ in range(100)\n]\n\nwith wds.writer.TarWriter(\"data-000000.tar\") as sink:\n    for i, sample in enumerate(samples):\n        sink.write({**sample.as_wds, \"__key__\": f\"sample_{i:06d}\"})\n\n\n\nLoad and Iterate\n\ndataset = atdata.Dataset[ImageSample](\"data-000000.tar\")\n\n# Iterate with batching\nfor batch in dataset.shuffled(batch_size=32):\n    images = batch.image      # numpy array (32, 224, 224, 3)\n    labels = batch.label      # list of 32 strings\n    confs = batch.confidence  # list of 32 floats",
    "crumbs": [
      "Guide",
      "atdata"
    ]
  },
  {
    "objectID": "index.html#huggingface-style-loading",
    "href": "index.html#huggingface-style-loading",
    "title": "atdata",
    "section": "HuggingFace-Style Loading",
    "text": "HuggingFace-Style Loading\n\n# Load from local path\nds = atdata.load_dataset(\"path/to/data-{000000..000009}.tar\", split=\"train\")\n\n# Load with split detection\nds_dict = atdata.load_dataset(\"path/to/data/\")\ntrain_ds = ds_dict[\"train\"]\ntest_ds = ds_dict[\"test\"]",
    "crumbs": [
      "Guide",
      "atdata"
    ]
  },
  {
    "objectID": "index.html#local-storage-with-redis-s3",
    "href": "index.html#local-storage-with-redis-s3",
    "title": "atdata",
    "section": "Local Storage with Redis + S3",
    "text": "Local Storage with Redis + S3\n\nfrom atdata.local import LocalIndex, S3DataStore\nimport webdataset as wds\n\n# Create samples and write to local tar\nwith wds.writer.TarWriter(\"data.tar\") as sink:\n    for i, sample in enumerate(samples):\n        sink.write({**sample.as_wds, \"__key__\": f\"{i:06d}\"})\n\n# Set up index with S3 data store\nstore = S3DataStore(\n    credentials={\"AWS_ENDPOINT\": \"http://localhost:9000\", ...},\n    bucket=\"my-bucket\",\n)\nindex = LocalIndex(data_store=store)  # Connects to Redis\n\n# Insert dataset (writes to S3, indexes in Redis)\ndataset = atdata.Dataset[ImageSample](\"data.tar\")\nentry = index.insert_dataset(dataset, name=\"my-dataset\")\nprint(f\"Stored at: {entry.data_urls}\")",
    "crumbs": [
      "Guide",
      "atdata"
    ]
  },
  {
    "objectID": "index.html#publish-to-atproto-federation",
    "href": "index.html#publish-to-atproto-federation",
    "title": "atdata",
    "section": "Publish to ATProto Federation",
    "text": "Publish to ATProto Federation\n\nfrom atdata.atmosphere import AtmosphereClient\nfrom atdata.promote import promote_to_atmosphere\n\n# Authenticate\nclient = AtmosphereClient()\nclient.login(\"handle.bsky.social\", \"app-password\")\n\n# Promote local dataset to federation\nentry = index.get_dataset(\"my-dataset\")\nat_uri = promote_to_atmosphere(entry, index, client)\nprint(f\"Published at: {at_uri}\")",
    "crumbs": [
      "Guide",
      "atdata"
    ]
  },
  {
    "objectID": "index.html#next-steps",
    "href": "index.html#next-steps",
    "title": "atdata",
    "section": "Next Steps",
    "text": "Next Steps\n\nQuick Start Tutorial - Get up and running in 5 minutes\nPackable Samples - Learn about typed sample definitions\nDatasets - Master dataset iteration and batching\nAtmosphere - Publish to the ATProto federation",
    "crumbs": [
      "Guide",
      "atdata"
    ]
  },
  {
    "objectID": "api/SampleBatch.html",
    "href": "api/SampleBatch.html",
    "title": "SampleBatch",
    "section": "",
    "text": "SampleBatch(samples)\nA batch of samples with automatic attribute aggregation.\nThis class wraps a sequence of samples and provides magic __getattr__ access to aggregate sample attributes. When you access an attribute that exists on the sample type, it automatically aggregates values across all samples in the batch.\nNDArray fields are stacked into a numpy array with a batch dimension. Other fields are aggregated into a list.\nType Parameters: DT: The sample type, must derive from PackableSample.\nAttributes: samples: The list of sample instances in this batch.\nExample: &gt;&gt;&gt; batch = SampleBatchMyData &gt;&gt;&gt; batch.embeddings # Returns stacked numpy array of shape (3, …) &gt;&gt;&gt; batch.names # Returns list of names\nNote: This class uses Python’s __orig_class__ mechanism to extract the type parameter at runtime. Instances must be created using the subscripted syntax SampleBatch[MyType](samples) rather than calling the constructor directly with an unsubscripted class.\n\n\n\n\n\nName\nDescription\n\n\n\n\nsample_type\nThe type of each sample in this batch."
  },
  {
    "objectID": "api/SampleBatch.html#attributes",
    "href": "api/SampleBatch.html#attributes",
    "title": "SampleBatch",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nsample_type\nThe type of each sample in this batch."
  },
  {
    "objectID": "api/LensPublisher.html",
    "href": "api/LensPublisher.html",
    "title": "LensPublisher",
    "section": "",
    "text": "atmosphere.LensPublisher(client)\nPublishes Lens transformation records to ATProto.\nThis class creates lens records that reference source and target schemas and point to the transformation code in a git repository.\nExample: &gt;&gt;&gt; @atdata.lens … def my_lens(source: SourceType) -&gt; TargetType: … return TargetType(field=source.other_field) &gt;&gt;&gt; &gt;&gt;&gt; client = AtmosphereClient() &gt;&gt;&gt; client.login(“handle”, “password”) &gt;&gt;&gt; &gt;&gt;&gt; publisher = LensPublisher(client) &gt;&gt;&gt; uri = publisher.publish( … name=“my_lens”, … source_schema_uri=“at://did:plc:abc/ac.foundation.dataset.sampleSchema/source”, … target_schema_uri=“at://did:plc:abc/ac.foundation.dataset.sampleSchema/target”, … code_repository=“https://github.com/user/repo”, … code_commit=“abc123def456”, … getter_path=“mymodule.lenses:my_lens”, … putter_path=“mymodule.lenses:my_lens_putter”, … )\nSecurity Note: Lens code is stored as references to git repositories rather than inline code. This prevents arbitrary code execution from ATProto records. Users must manually install and trust lens implementations.\n\n\n\n\n\nName\nDescription\n\n\n\n\npublish\nPublish a lens transformation record to ATProto.\n\n\npublish_from_lens\nPublish a lens record from an existing Lens object.\n\n\n\n\n\natmosphere.LensPublisher.publish(\n    name,\n    source_schema_uri,\n    target_schema_uri,\n    description=None,\n    code_repository=None,\n    code_commit=None,\n    getter_path=None,\n    putter_path=None,\n    rkey=None,\n)\nPublish a lens transformation record to ATProto.\nArgs: name: Human-readable lens name. source_schema_uri: AT URI of the source schema. target_schema_uri: AT URI of the target schema. description: What this transformation does. code_repository: Git repository URL containing the lens code. code_commit: Git commit hash for reproducibility. getter_path: Module path to the getter function (e.g., ‘mymodule.lenses:my_getter’). putter_path: Module path to the putter function (e.g., ‘mymodule.lenses:my_putter’). rkey: Optional explicit record key.\nReturns: The AT URI of the created lens record.\nRaises: ValueError: If code references are incomplete.\n\n\n\natmosphere.LensPublisher.publish_from_lens(\n    lens_obj,\n    *,\n    name,\n    source_schema_uri,\n    target_schema_uri,\n    code_repository,\n    code_commit,\n    description=None,\n    rkey=None,\n)\nPublish a lens record from an existing Lens object.\nThis method extracts the getter and putter function names from the Lens object and publishes a record referencing them.\nArgs: lens_obj: The Lens object to publish. name: Human-readable lens name. source_schema_uri: AT URI of the source schema. target_schema_uri: AT URI of the target schema. code_repository: Git repository URL. code_commit: Git commit hash. description: What this transformation does. rkey: Optional explicit record key.\nReturns: The AT URI of the created lens record."
  },
  {
    "objectID": "api/LensPublisher.html#methods",
    "href": "api/LensPublisher.html#methods",
    "title": "LensPublisher",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\npublish\nPublish a lens transformation record to ATProto.\n\n\npublish_from_lens\nPublish a lens record from an existing Lens object.\n\n\n\n\n\natmosphere.LensPublisher.publish(\n    name,\n    source_schema_uri,\n    target_schema_uri,\n    description=None,\n    code_repository=None,\n    code_commit=None,\n    getter_path=None,\n    putter_path=None,\n    rkey=None,\n)\nPublish a lens transformation record to ATProto.\nArgs: name: Human-readable lens name. source_schema_uri: AT URI of the source schema. target_schema_uri: AT URI of the target schema. description: What this transformation does. code_repository: Git repository URL containing the lens code. code_commit: Git commit hash for reproducibility. getter_path: Module path to the getter function (e.g., ‘mymodule.lenses:my_getter’). putter_path: Module path to the putter function (e.g., ‘mymodule.lenses:my_putter’). rkey: Optional explicit record key.\nReturns: The AT URI of the created lens record.\nRaises: ValueError: If code references are incomplete.\n\n\n\natmosphere.LensPublisher.publish_from_lens(\n    lens_obj,\n    *,\n    name,\n    source_schema_uri,\n    target_schema_uri,\n    code_repository,\n    code_commit,\n    description=None,\n    rkey=None,\n)\nPublish a lens record from an existing Lens object.\nThis method extracts the getter and putter function names from the Lens object and publishes a record referencing them.\nArgs: lens_obj: The Lens object to publish. name: Human-readable lens name. source_schema_uri: AT URI of the source schema. target_schema_uri: AT URI of the target schema. code_repository: Git repository URL. code_commit: Git commit hash. description: What this transformation does. rkey: Optional explicit record key.\nReturns: The AT URI of the created lens record."
  },
  {
    "objectID": "api/AtmosphereIndexEntry.html",
    "href": "api/AtmosphereIndexEntry.html",
    "title": "AtmosphereIndexEntry",
    "section": "",
    "text": "atmosphere.AtmosphereIndexEntry(uri, record)\nEntry wrapper for ATProto dataset records implementing IndexEntry protocol.\nAttributes: _uri: AT URI of the record. _record: Raw record dictionary.\n\n\n\n\n\nName\nDescription\n\n\n\n\ndata_urls\nWebDataset URLs from external storage.\n\n\nmetadata\nMetadata from the record, if any.\n\n\nname\nHuman-readable dataset name.\n\n\nschema_ref\nAT URI of the schema record.\n\n\nuri\nAT URI of this record."
  },
  {
    "objectID": "api/AtmosphereIndexEntry.html#attributes",
    "href": "api/AtmosphereIndexEntry.html#attributes",
    "title": "AtmosphereIndexEntry",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndata_urls\nWebDataset URLs from external storage.\n\n\nmetadata\nMetadata from the record, if any.\n\n\nname\nHuman-readable dataset name.\n\n\nschema_ref\nAT URI of the schema record.\n\n\nuri\nAT URI of this record."
  },
  {
    "objectID": "api/AbstractIndex.html",
    "href": "api/AbstractIndex.html",
    "title": "AbstractIndex",
    "section": "",
    "text": "AbstractIndex()\nProtocol for index operations - implemented by LocalIndex and AtmosphereIndex.\nThis protocol defines the common interface for managing dataset metadata: - Publishing and retrieving schemas - Inserting and listing datasets - (Future) Publishing and retrieving lenses\nA single index can hold datasets of many different sample types. The sample type is tracked via schema references, not as a generic parameter on the index.\nOptional Extensions: Some index implementations support additional features: - data_store: An AbstractDataStore for reading/writing dataset shards. If present, load_dataset will use it for S3 credential resolution.\nExample: &gt;&gt;&gt; def publish_and_list(index: AbstractIndex) -&gt; None: … # Publish schemas for different types … schema1 = index.publish_schema(ImageSample, version=“1.0.0”) … schema2 = index.publish_schema(TextSample, version=“1.0.0”) … … # Insert datasets of different types … index.insert_dataset(image_ds, name=“images”) … index.insert_dataset(text_ds, name=“texts”) … … # List all datasets (mixed types) … for entry in index.list_datasets(): … print(f”{entry.name} -&gt; {entry.schema_ref}“)\n\n\n\n\n\nName\nDescription\n\n\n\n\ndatasets\nLazily iterate over all dataset entries in this index.\n\n\nschemas\nLazily iterate over all schema records in this index.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ndecode_schema\nReconstruct a Python Packable type from a stored schema.\n\n\nget_dataset\nGet a dataset entry by name or reference.\n\n\nget_schema\nGet a schema record by reference.\n\n\ninsert_dataset\nInsert a dataset into the index.\n\n\nlist_datasets\nGet all dataset entries as a materialized list.\n\n\nlist_schemas\nGet all schema records as a materialized list.\n\n\npublish_schema\nPublish a schema for a sample type.\n\n\n\n\n\nAbstractIndex.decode_schema(ref)\nReconstruct a Python Packable type from a stored schema.\nThis method enables loading datasets without knowing the sample type ahead of time. The index retrieves the schema record and dynamically generates a Packable class matching the schema definition.\nArgs: ref: Schema reference string (local:// or at://).\nReturns: A dynamically generated Packable class with fields matching the schema definition. The class can be used with Dataset[T] to load and iterate over samples.\nRaises: KeyError: If schema not found. ValueError: If schema cannot be decoded (unsupported field types).\nExample: &gt;&gt;&gt; entry = index.get_dataset(“my-dataset”) &gt;&gt;&gt; SampleType = index.decode_schema(entry.schema_ref) &gt;&gt;&gt; ds = DatasetSampleType &gt;&gt;&gt; for sample in ds.ordered(): … print(sample) # sample is instance of SampleType\n\n\n\nAbstractIndex.get_dataset(ref)\nGet a dataset entry by name or reference.\nArgs: ref: Dataset name, path, or full reference string.\nReturns: IndexEntry for the dataset.\nRaises: KeyError: If dataset not found.\n\n\n\nAbstractIndex.get_schema(ref)\nGet a schema record by reference.\nArgs: ref: Schema reference string (local:// or at://).\nReturns: Schema record as a dictionary with fields like ‘name’, ‘version’, ‘fields’, etc.\nRaises: KeyError: If schema not found.\n\n\n\nAbstractIndex.insert_dataset(ds, *, name, schema_ref=None, **kwargs)\nInsert a dataset into the index.\nThe sample type is inferred from ds.sample_type. If schema_ref is not provided, the schema may be auto-published based on the sample type.\nArgs: ds: The Dataset to register in the index (any sample type). name: Human-readable name for the dataset. schema_ref: Optional explicit schema reference. If not provided, the schema may be auto-published or inferred from ds.sample_type. **kwargs: Additional backend-specific options.\nReturns: IndexEntry for the inserted dataset.\n\n\n\nAbstractIndex.list_datasets()\nGet all dataset entries as a materialized list.\nReturns: List of IndexEntry for each dataset.\n\n\n\nAbstractIndex.list_schemas()\nGet all schema records as a materialized list.\nReturns: List of schema records as dictionaries.\n\n\n\nAbstractIndex.publish_schema(sample_type, *, version='1.0.0', **kwargs)\nPublish a schema for a sample type.\nArgs: sample_type: A Packable type (PackableSample subclass or @packable-decorated). version: Semantic version string for the schema. **kwargs: Additional backend-specific options.\nReturns: Schema reference string: - Local: ‘local://schemas/{module.Class}@version’ - Atmosphere: ‘at://did:plc:…/ac.foundation.dataset.sampleSchema/…’"
  },
  {
    "objectID": "api/AbstractIndex.html#attributes",
    "href": "api/AbstractIndex.html#attributes",
    "title": "AbstractIndex",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndatasets\nLazily iterate over all dataset entries in this index.\n\n\nschemas\nLazily iterate over all schema records in this index."
  },
  {
    "objectID": "api/AbstractIndex.html#methods",
    "href": "api/AbstractIndex.html#methods",
    "title": "AbstractIndex",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndecode_schema\nReconstruct a Python Packable type from a stored schema.\n\n\nget_dataset\nGet a dataset entry by name or reference.\n\n\nget_schema\nGet a schema record by reference.\n\n\ninsert_dataset\nInsert a dataset into the index.\n\n\nlist_datasets\nGet all dataset entries as a materialized list.\n\n\nlist_schemas\nGet all schema records as a materialized list.\n\n\npublish_schema\nPublish a schema for a sample type.\n\n\n\n\n\nAbstractIndex.decode_schema(ref)\nReconstruct a Python Packable type from a stored schema.\nThis method enables loading datasets without knowing the sample type ahead of time. The index retrieves the schema record and dynamically generates a Packable class matching the schema definition.\nArgs: ref: Schema reference string (local:// or at://).\nReturns: A dynamically generated Packable class with fields matching the schema definition. The class can be used with Dataset[T] to load and iterate over samples.\nRaises: KeyError: If schema not found. ValueError: If schema cannot be decoded (unsupported field types).\nExample: &gt;&gt;&gt; entry = index.get_dataset(“my-dataset”) &gt;&gt;&gt; SampleType = index.decode_schema(entry.schema_ref) &gt;&gt;&gt; ds = DatasetSampleType &gt;&gt;&gt; for sample in ds.ordered(): … print(sample) # sample is instance of SampleType\n\n\n\nAbstractIndex.get_dataset(ref)\nGet a dataset entry by name or reference.\nArgs: ref: Dataset name, path, or full reference string.\nReturns: IndexEntry for the dataset.\nRaises: KeyError: If dataset not found.\n\n\n\nAbstractIndex.get_schema(ref)\nGet a schema record by reference.\nArgs: ref: Schema reference string (local:// or at://).\nReturns: Schema record as a dictionary with fields like ‘name’, ‘version’, ‘fields’, etc.\nRaises: KeyError: If schema not found.\n\n\n\nAbstractIndex.insert_dataset(ds, *, name, schema_ref=None, **kwargs)\nInsert a dataset into the index.\nThe sample type is inferred from ds.sample_type. If schema_ref is not provided, the schema may be auto-published based on the sample type.\nArgs: ds: The Dataset to register in the index (any sample type). name: Human-readable name for the dataset. schema_ref: Optional explicit schema reference. If not provided, the schema may be auto-published or inferred from ds.sample_type. **kwargs: Additional backend-specific options.\nReturns: IndexEntry for the inserted dataset.\n\n\n\nAbstractIndex.list_datasets()\nGet all dataset entries as a materialized list.\nReturns: List of IndexEntry for each dataset.\n\n\n\nAbstractIndex.list_schemas()\nGet all schema records as a materialized list.\nReturns: List of schema records as dictionaries.\n\n\n\nAbstractIndex.publish_schema(sample_type, *, version='1.0.0', **kwargs)\nPublish a schema for a sample type.\nArgs: sample_type: A Packable type (PackableSample subclass or @packable-decorated). version: Semantic version string for the schema. **kwargs: Additional backend-specific options.\nReturns: Schema reference string: - Local: ‘local://schemas/{module.Class}@version’ - Atmosphere: ‘at://did:plc:…/ac.foundation.dataset.sampleSchema/…’"
  },
  {
    "objectID": "api/local.LocalDatasetEntry.html",
    "href": "api/local.LocalDatasetEntry.html",
    "title": "local.LocalDatasetEntry",
    "section": "",
    "text": "local.LocalDatasetEntry(\n    name,\n    schema_ref,\n    data_urls,\n    metadata=None,\n    _cid=None,\n    _legacy_uuid=None,\n)\nIndex entry for a dataset stored in the local repository.\nImplements the IndexEntry protocol for compatibility with AbstractIndex. Uses dual identity: a content-addressable CID (ATProto-compatible) and a human-readable name.\nThe CID is generated from the entry’s content (schema_ref + data_urls), ensuring the same data produces the same CID whether stored locally or in the atmosphere. This enables seamless promotion from local to ATProto.\nAttributes: name: Human-readable name for this dataset. schema_ref: Reference to the schema for this dataset. data_urls: WebDataset URLs for the data. metadata: Arbitrary metadata dictionary, or None if not set.\n\n\n\n\n\nName\nDescription\n\n\n\n\ncid\nContent identifier (ATProto-compatible CID).\n\n\ndata_urls\nWebDataset URLs for the data.\n\n\nmetadata\nArbitrary metadata dictionary, or None if not set.\n\n\nname\nHuman-readable name for this dataset.\n\n\nsample_kind\nLegacy property: returns schema_ref for backwards compatibility.\n\n\nschema_ref\nReference to the schema for this dataset.\n\n\nwds_url\nLegacy property: returns first data URL for backwards compatibility.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfrom_redis\nLoad an entry from Redis by CID.\n\n\nwrite_to\nPersist this index entry to Redis.\n\n\n\n\n\nlocal.LocalDatasetEntry.from_redis(redis, cid)\nLoad an entry from Redis by CID.\nArgs: redis: Redis connection to read from. cid: Content identifier of the entry to load.\nReturns: LocalDatasetEntry loaded from Redis.\nRaises: KeyError: If entry not found.\n\n\n\nlocal.LocalDatasetEntry.write_to(redis)\nPersist this index entry to Redis.\nStores the entry as a Redis hash with key ‘{REDIS_KEY_DATASET_ENTRY}:{cid}’.\nArgs: redis: Redis connection to write to."
  },
  {
    "objectID": "api/local.LocalDatasetEntry.html#attributes",
    "href": "api/local.LocalDatasetEntry.html#attributes",
    "title": "local.LocalDatasetEntry",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncid\nContent identifier (ATProto-compatible CID).\n\n\ndata_urls\nWebDataset URLs for the data.\n\n\nmetadata\nArbitrary metadata dictionary, or None if not set.\n\n\nname\nHuman-readable name for this dataset.\n\n\nsample_kind\nLegacy property: returns schema_ref for backwards compatibility.\n\n\nschema_ref\nReference to the schema for this dataset.\n\n\nwds_url\nLegacy property: returns first data URL for backwards compatibility."
  },
  {
    "objectID": "api/local.LocalDatasetEntry.html#methods",
    "href": "api/local.LocalDatasetEntry.html#methods",
    "title": "local.LocalDatasetEntry",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfrom_redis\nLoad an entry from Redis by CID.\n\n\nwrite_to\nPersist this index entry to Redis.\n\n\n\n\n\nlocal.LocalDatasetEntry.from_redis(redis, cid)\nLoad an entry from Redis by CID.\nArgs: redis: Redis connection to read from. cid: Content identifier of the entry to load.\nReturns: LocalDatasetEntry loaded from Redis.\nRaises: KeyError: If entry not found.\n\n\n\nlocal.LocalDatasetEntry.write_to(redis)\nPersist this index entry to Redis.\nStores the entry as a Redis hash with key ‘{REDIS_KEY_DATASET_ENTRY}:{cid}’.\nArgs: redis: Redis connection to write to."
  },
  {
    "objectID": "api/S3Source.html",
    "href": "api/S3Source.html",
    "title": "S3Source",
    "section": "",
    "text": "S3Source(\n    bucket,\n    keys,\n    endpoint=None,\n    access_key=None,\n    secret_key=None,\n    region=None,\n    _client=None,\n)\nData source for S3-compatible storage with explicit credentials.\nUses boto3 to stream directly from S3, supporting: - Standard AWS S3 - S3-compatible endpoints (Cloudflare R2, MinIO, etc.) - Private buckets with credentials - IAM role authentication (when keys not provided)\nUnlike URL-based approaches, this doesn’t require URL transformation or global gopen_schemes registration. Credentials are scoped to the source instance.\nAttributes: bucket: S3 bucket name. keys: List of object keys (paths within bucket). endpoint: Optional custom endpoint URL for S3-compatible services. access_key: Optional AWS access key ID. secret_key: Optional AWS secret access key. region: Optional AWS region (defaults to us-east-1).\nExample: &gt;&gt;&gt; source = S3Source( … bucket=“my-datasets”, … keys=[“train/shard-000.tar”, “train/shard-001.tar”], … endpoint=“https://abc123.r2.cloudflarestorage.com”, … access_key=“AKIAIOSFODNN7EXAMPLE”, … secret_key=“wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY”, … ) &gt;&gt;&gt; for shard_id, stream in source.shards: … process(stream)\n\n\n\n\n\nName\nDescription\n\n\n\n\nshard_list\nReturn list of S3 URIs for the shards (deprecated, use list_shards()).\n\n\nshards\nLazily yield (s3_uri, stream) pairs for each shard.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfrom_credentials\nCreate S3Source from a credentials dictionary.\n\n\nfrom_urls\nCreate S3Source from s3:// URLs.\n\n\nlist_shards\nReturn list of S3 URIs for the shards.\n\n\nopen_shard\nOpen a single shard by S3 URI.\n\n\n\n\n\nS3Source.from_credentials(credentials, bucket, keys)\nCreate S3Source from a credentials dictionary.\nAccepts the same credential format used by S3DataStore.\nArgs: credentials: Dict with AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and optionally AWS_ENDPOINT. bucket: S3 bucket name. keys: List of object keys.\nReturns: Configured S3Source.\nExample: &gt;&gt;&gt; creds = { … “AWS_ACCESS_KEY_ID”: “…”, … “AWS_SECRET_ACCESS_KEY”: “…”, … “AWS_ENDPOINT”: “https://r2.example.com”, … } &gt;&gt;&gt; source = S3Source.from_credentials(creds, “my-bucket”, [“data.tar”])\n\n\n\nS3Source.from_urls(\n    urls,\n    *,\n    endpoint=None,\n    access_key=None,\n    secret_key=None,\n    region=None,\n)\nCreate S3Source from s3:// URLs.\nParses s3://bucket/key URLs and extracts bucket and keys. All URLs must be in the same bucket.\nArgs: urls: List of s3:// URLs. endpoint: Optional custom endpoint. access_key: Optional access key. secret_key: Optional secret key. region: Optional region.\nReturns: S3Source configured for the given URLs.\nRaises: ValueError: If URLs are not valid s3:// URLs or span multiple buckets.\nExample: &gt;&gt;&gt; source = S3Source.from_urls( … [“s3://my-bucket/train-000.tar”, “s3://my-bucket/train-001.tar”], … endpoint=“https://r2.example.com”, … )\n\n\n\nS3Source.list_shards()\nReturn list of S3 URIs for the shards.\n\n\n\nS3Source.open_shard(shard_id)\nOpen a single shard by S3 URI.\nArgs: shard_id: S3 URI of the shard (s3://bucket/key).\nReturns: StreamingBody for reading the object.\nRaises: KeyError: If shard_id is not in list_shards()."
  },
  {
    "objectID": "api/S3Source.html#attributes",
    "href": "api/S3Source.html#attributes",
    "title": "S3Source",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nshard_list\nReturn list of S3 URIs for the shards (deprecated, use list_shards()).\n\n\nshards\nLazily yield (s3_uri, stream) pairs for each shard."
  },
  {
    "objectID": "api/S3Source.html#methods",
    "href": "api/S3Source.html#methods",
    "title": "S3Source",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfrom_credentials\nCreate S3Source from a credentials dictionary.\n\n\nfrom_urls\nCreate S3Source from s3:// URLs.\n\n\nlist_shards\nReturn list of S3 URIs for the shards.\n\n\nopen_shard\nOpen a single shard by S3 URI.\n\n\n\n\n\nS3Source.from_credentials(credentials, bucket, keys)\nCreate S3Source from a credentials dictionary.\nAccepts the same credential format used by S3DataStore.\nArgs: credentials: Dict with AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and optionally AWS_ENDPOINT. bucket: S3 bucket name. keys: List of object keys.\nReturns: Configured S3Source.\nExample: &gt;&gt;&gt; creds = { … “AWS_ACCESS_KEY_ID”: “…”, … “AWS_SECRET_ACCESS_KEY”: “…”, … “AWS_ENDPOINT”: “https://r2.example.com”, … } &gt;&gt;&gt; source = S3Source.from_credentials(creds, “my-bucket”, [“data.tar”])\n\n\n\nS3Source.from_urls(\n    urls,\n    *,\n    endpoint=None,\n    access_key=None,\n    secret_key=None,\n    region=None,\n)\nCreate S3Source from s3:// URLs.\nParses s3://bucket/key URLs and extracts bucket and keys. All URLs must be in the same bucket.\nArgs: urls: List of s3:// URLs. endpoint: Optional custom endpoint. access_key: Optional access key. secret_key: Optional secret key. region: Optional region.\nReturns: S3Source configured for the given URLs.\nRaises: ValueError: If URLs are not valid s3:// URLs or span multiple buckets.\nExample: &gt;&gt;&gt; source = S3Source.from_urls( … [“s3://my-bucket/train-000.tar”, “s3://my-bucket/train-001.tar”], … endpoint=“https://r2.example.com”, … )\n\n\n\nS3Source.list_shards()\nReturn list of S3 URIs for the shards.\n\n\n\nS3Source.open_shard(shard_id)\nOpen a single shard by S3 URI.\nArgs: shard_id: S3 URI of the shard (s3://bucket/key).\nReturns: StreamingBody for reading the object.\nRaises: KeyError: If shard_id is not in list_shards()."
  },
  {
    "objectID": "api/IndexEntry.html",
    "href": "api/IndexEntry.html",
    "title": "IndexEntry",
    "section": "",
    "text": "IndexEntry()\nCommon interface for index entries (local or atmosphere).\nBoth LocalDatasetEntry and atmosphere DatasetRecord-based entries should satisfy this protocol, enabling code that works with either.\nProperties: name: Human-readable dataset name schema_ref: Reference to schema (local:// path or AT URI) data_urls: WebDataset URLs for the data metadata: Arbitrary metadata dict, or None\n\n\n\n\n\nName\nDescription\n\n\n\n\ndata_urls\nWebDataset URLs for the data.\n\n\nmetadata\nArbitrary metadata dictionary, or None if not set.\n\n\nname\nHuman-readable dataset name.\n\n\nschema_ref\nReference to the schema for this dataset."
  },
  {
    "objectID": "api/IndexEntry.html#attributes",
    "href": "api/IndexEntry.html#attributes",
    "title": "IndexEntry",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndata_urls\nWebDataset URLs for the data.\n\n\nmetadata\nArbitrary metadata dictionary, or None if not set.\n\n\nname\nHuman-readable dataset name.\n\n\nschema_ref\nReference to the schema for this dataset."
  },
  {
    "objectID": "api/index.html",
    "href": "api/index.html",
    "title": "API Reference",
    "section": "",
    "text": "Core types, decorators, and dataset classes\n\n\n\npackable\nDecorator to convert a regular class into a PackableSample.\n\n\nPackableSample\nBase class for samples that can be serialized with msgpack.\n\n\nDictSample\nDynamic sample type providing dict-like access to raw msgpack data.\n\n\nDataset\nA typed dataset built on WebDataset with lens transformations.\n\n\nSampleBatch\nA batch of samples with automatic attribute aggregation.\n\n\nLens\nA bidirectional transformation between two sample types.\n\n\nlens\nLens-based type transformations for datasets.\n\n\nload_dataset\nLoad a dataset from local files, remote URLs, or an index.\n\n\nDatasetDict\nA dictionary of split names to Dataset instances.\n\n\n\n\n\n\nAbstract protocols for storage backends\n\n\n\nPackable\nStructural protocol for packable sample types.\n\n\nIndexEntry\nCommon interface for index entries (local or atmosphere).\n\n\nAbstractIndex\nProtocol for index operations - implemented by LocalIndex and AtmosphereIndex.\n\n\nAbstractDataStore\nProtocol for data storage operations.\n\n\nDataSource\nProtocol for data sources that provide streams to Dataset.\n\n\n\n\n\n\nData source implementations for streaming\n\n\n\nURLSource\nData source for WebDataset-compatible URLs.\n\n\nS3Source\nData source for S3-compatible storage with explicit credentials.\n\n\n\n\n\n\nLocal Redis/S3 storage backend\n\n\n\nlocal.Index\nRedis-backed index for tracking datasets in a repository.\n\n\nlocal.LocalDatasetEntry\nIndex entry for a dataset stored in the local repository.\n\n\nlocal.S3DataStore\nS3-compatible data store implementing AbstractDataStore protocol.\n\n\n\n\n\n\nATProto federation\n\n\n\nAtmosphereClient\nATProto client wrapper for atdata operations.\n\n\nAtmosphereIndex\nATProto index implementing AbstractIndex protocol.\n\n\nAtmosphereIndexEntry\nEntry wrapper for ATProto dataset records implementing IndexEntry protocol.\n\n\nSchemaPublisher\nPublishes PackableSample schemas to ATProto.\n\n\nSchemaLoader\nLoads PackableSample schemas from ATProto.\n\n\nDatasetPublisher\nPublishes dataset index records to ATProto.\n\n\nDatasetLoader\nLoads dataset records from ATProto.\n\n\nLensPublisher\nPublishes Lens transformation records to ATProto.\n\n\nLensLoader\nLoads lens records from ATProto.\n\n\nAtUri\nParsed AT Protocol URI.\n\n\n\n\n\n\nLocal to atmosphere migration\n\n\n\npromote_to_atmosphere\nPromote a local dataset to the atmosphere network."
  },
  {
    "objectID": "api/index.html#core",
    "href": "api/index.html#core",
    "title": "API Reference",
    "section": "",
    "text": "Core types, decorators, and dataset classes\n\n\n\npackable\nDecorator to convert a regular class into a PackableSample.\n\n\nPackableSample\nBase class for samples that can be serialized with msgpack.\n\n\nDictSample\nDynamic sample type providing dict-like access to raw msgpack data.\n\n\nDataset\nA typed dataset built on WebDataset with lens transformations.\n\n\nSampleBatch\nA batch of samples with automatic attribute aggregation.\n\n\nLens\nA bidirectional transformation between two sample types.\n\n\nlens\nLens-based type transformations for datasets.\n\n\nload_dataset\nLoad a dataset from local files, remote URLs, or an index.\n\n\nDatasetDict\nA dictionary of split names to Dataset instances."
  },
  {
    "objectID": "api/index.html#protocols",
    "href": "api/index.html#protocols",
    "title": "API Reference",
    "section": "",
    "text": "Abstract protocols for storage backends\n\n\n\nPackable\nStructural protocol for packable sample types.\n\n\nIndexEntry\nCommon interface for index entries (local or atmosphere).\n\n\nAbstractIndex\nProtocol for index operations - implemented by LocalIndex and AtmosphereIndex.\n\n\nAbstractDataStore\nProtocol for data storage operations.\n\n\nDataSource\nProtocol for data sources that provide streams to Dataset."
  },
  {
    "objectID": "api/index.html#data-sources",
    "href": "api/index.html#data-sources",
    "title": "API Reference",
    "section": "",
    "text": "Data source implementations for streaming\n\n\n\nURLSource\nData source for WebDataset-compatible URLs.\n\n\nS3Source\nData source for S3-compatible storage with explicit credentials."
  },
  {
    "objectID": "api/index.html#local-storage",
    "href": "api/index.html#local-storage",
    "title": "API Reference",
    "section": "",
    "text": "Local Redis/S3 storage backend\n\n\n\nlocal.Index\nRedis-backed index for tracking datasets in a repository.\n\n\nlocal.LocalDatasetEntry\nIndex entry for a dataset stored in the local repository.\n\n\nlocal.S3DataStore\nS3-compatible data store implementing AbstractDataStore protocol."
  },
  {
    "objectID": "api/index.html#atmosphere",
    "href": "api/index.html#atmosphere",
    "title": "API Reference",
    "section": "",
    "text": "ATProto federation\n\n\n\nAtmosphereClient\nATProto client wrapper for atdata operations.\n\n\nAtmosphereIndex\nATProto index implementing AbstractIndex protocol.\n\n\nAtmosphereIndexEntry\nEntry wrapper for ATProto dataset records implementing IndexEntry protocol.\n\n\nSchemaPublisher\nPublishes PackableSample schemas to ATProto.\n\n\nSchemaLoader\nLoads PackableSample schemas from ATProto.\n\n\nDatasetPublisher\nPublishes dataset index records to ATProto.\n\n\nDatasetLoader\nLoads dataset records from ATProto.\n\n\nLensPublisher\nPublishes Lens transformation records to ATProto.\n\n\nLensLoader\nLoads lens records from ATProto.\n\n\nAtUri\nParsed AT Protocol URI."
  },
  {
    "objectID": "api/index.html#promotion",
    "href": "api/index.html#promotion",
    "title": "API Reference",
    "section": "",
    "text": "Local to atmosphere migration\n\n\n\npromote_to_atmosphere\nPromote a local dataset to the atmosphere network."
  },
  {
    "objectID": "api/URLSource.html",
    "href": "api/URLSource.html",
    "title": "URLSource",
    "section": "",
    "text": "URLSource(url)\nData source for WebDataset-compatible URLs.\nWraps WebDataset’s gopen to open URLs using built-in handlers for http, https, pipe, gs, hf, sftp, etc. Supports brace expansion for shard patterns like “data-{000..099}.tar”.\nThis is the default source type when a string URL is passed to Dataset.\nAttributes: url: URL or brace pattern for the shards.\nExample: &gt;&gt;&gt; source = URLSource(“https://example.com/train-{000..009}.tar”) &gt;&gt;&gt; for shard_id, stream in source.shards: … print(f”Streaming {shard_id}“)\n\n\n\n\n\nName\nDescription\n\n\n\n\nshard_list\nExpand brace pattern and return list of shard URLs (deprecated, use list_shards()).\n\n\nshards\nLazily yield (url, stream) pairs for each shard.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nlist_shards\nExpand brace pattern and return list of shard URLs.\n\n\nopen_shard\nOpen a single shard by URL.\n\n\n\n\n\nURLSource.list_shards()\nExpand brace pattern and return list of shard URLs.\n\n\n\nURLSource.open_shard(shard_id)\nOpen a single shard by URL.\nArgs: shard_id: URL of the shard to open.\nReturns: File-like stream from gopen.\nRaises: KeyError: If shard_id is not in list_shards()."
  },
  {
    "objectID": "api/URLSource.html#attributes",
    "href": "api/URLSource.html#attributes",
    "title": "URLSource",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nshard_list\nExpand brace pattern and return list of shard URLs (deprecated, use list_shards()).\n\n\nshards\nLazily yield (url, stream) pairs for each shard."
  },
  {
    "objectID": "api/URLSource.html#methods",
    "href": "api/URLSource.html#methods",
    "title": "URLSource",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nlist_shards\nExpand brace pattern and return list of shard URLs.\n\n\nopen_shard\nOpen a single shard by URL.\n\n\n\n\n\nURLSource.list_shards()\nExpand brace pattern and return list of shard URLs.\n\n\n\nURLSource.open_shard(shard_id)\nOpen a single shard by URL.\nArgs: shard_id: URL of the shard to open.\nReturns: File-like stream from gopen.\nRaises: KeyError: If shard_id is not in list_shards()."
  },
  {
    "objectID": "api/DatasetPublisher.html",
    "href": "api/DatasetPublisher.html",
    "title": "DatasetPublisher",
    "section": "",
    "text": "atmosphere.DatasetPublisher(client)\nPublishes dataset index records to ATProto.\nThis class creates dataset records that reference a schema and point to external storage (WebDataset URLs) or ATProto blobs.\nExample: &gt;&gt;&gt; dataset = atdata.DatasetMySample &gt;&gt;&gt; &gt;&gt;&gt; client = AtmosphereClient() &gt;&gt;&gt; client.login(“handle”, “password”) &gt;&gt;&gt; &gt;&gt;&gt; publisher = DatasetPublisher(client) &gt;&gt;&gt; uri = publisher.publish( … dataset, … name=“My Training Data”, … description=“Training data for my model”, … tags=[“computer-vision”, “training”], … )\n\n\n\n\n\nName\nDescription\n\n\n\n\npublish\nPublish a dataset index record to ATProto.\n\n\npublish_with_blobs\nPublish a dataset with data stored as ATProto blobs.\n\n\npublish_with_urls\nPublish a dataset record with explicit URLs.\n\n\n\n\n\natmosphere.DatasetPublisher.publish(\n    dataset,\n    *,\n    name,\n    schema_uri=None,\n    description=None,\n    tags=None,\n    license=None,\n    auto_publish_schema=True,\n    schema_version='1.0.0',\n    rkey=None,\n)\nPublish a dataset index record to ATProto.\nArgs: dataset: The Dataset to publish. name: Human-readable dataset name. schema_uri: AT URI of the schema record. If not provided and auto_publish_schema is True, the schema will be published. description: Human-readable description. tags: Searchable tags for discovery. license: SPDX license identifier (e.g., ‘MIT’, ‘Apache-2.0’). auto_publish_schema: If True and schema_uri not provided, automatically publish the schema first. schema_version: Version for auto-published schema. rkey: Optional explicit record key.\nReturns: The AT URI of the created dataset record.\nRaises: ValueError: If schema_uri is not provided and auto_publish_schema is False.\n\n\n\natmosphere.DatasetPublisher.publish_with_blobs(\n    blobs,\n    schema_uri,\n    *,\n    name,\n    description=None,\n    tags=None,\n    license=None,\n    metadata=None,\n    mime_type='application/x-tar',\n    rkey=None,\n)\nPublish a dataset with data stored as ATProto blobs.\nThis method uploads the provided data as blobs to the PDS and creates a dataset record referencing them. Suitable for smaller datasets that fit within blob size limits (typically 50MB per blob, configurable).\nArgs: blobs: List of binary data (e.g., tar shards) to upload as blobs. schema_uri: AT URI of the schema record. name: Human-readable dataset name. description: Human-readable description. tags: Searchable tags for discovery. license: SPDX license identifier. metadata: Arbitrary metadata dictionary. mime_type: MIME type for the blobs (default: application/x-tar). rkey: Optional explicit record key.\nReturns: The AT URI of the created dataset record.\nNote: Blobs are only retained by the PDS when referenced in a committed record. This method handles that automatically.\n\n\n\natmosphere.DatasetPublisher.publish_with_urls(\n    urls,\n    schema_uri,\n    *,\n    name,\n    description=None,\n    tags=None,\n    license=None,\n    metadata=None,\n    rkey=None,\n)\nPublish a dataset record with explicit URLs.\nThis method allows publishing a dataset record without having a Dataset object, useful for registering existing WebDataset files.\nArgs: urls: List of WebDataset URLs with brace notation. schema_uri: AT URI of the schema record. name: Human-readable dataset name. description: Human-readable description. tags: Searchable tags for discovery. license: SPDX license identifier. metadata: Arbitrary metadata dictionary. rkey: Optional explicit record key.\nReturns: The AT URI of the created dataset record."
  },
  {
    "objectID": "api/DatasetPublisher.html#methods",
    "href": "api/DatasetPublisher.html#methods",
    "title": "DatasetPublisher",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\npublish\nPublish a dataset index record to ATProto.\n\n\npublish_with_blobs\nPublish a dataset with data stored as ATProto blobs.\n\n\npublish_with_urls\nPublish a dataset record with explicit URLs.\n\n\n\n\n\natmosphere.DatasetPublisher.publish(\n    dataset,\n    *,\n    name,\n    schema_uri=None,\n    description=None,\n    tags=None,\n    license=None,\n    auto_publish_schema=True,\n    schema_version='1.0.0',\n    rkey=None,\n)\nPublish a dataset index record to ATProto.\nArgs: dataset: The Dataset to publish. name: Human-readable dataset name. schema_uri: AT URI of the schema record. If not provided and auto_publish_schema is True, the schema will be published. description: Human-readable description. tags: Searchable tags for discovery. license: SPDX license identifier (e.g., ‘MIT’, ‘Apache-2.0’). auto_publish_schema: If True and schema_uri not provided, automatically publish the schema first. schema_version: Version for auto-published schema. rkey: Optional explicit record key.\nReturns: The AT URI of the created dataset record.\nRaises: ValueError: If schema_uri is not provided and auto_publish_schema is False.\n\n\n\natmosphere.DatasetPublisher.publish_with_blobs(\n    blobs,\n    schema_uri,\n    *,\n    name,\n    description=None,\n    tags=None,\n    license=None,\n    metadata=None,\n    mime_type='application/x-tar',\n    rkey=None,\n)\nPublish a dataset with data stored as ATProto blobs.\nThis method uploads the provided data as blobs to the PDS and creates a dataset record referencing them. Suitable for smaller datasets that fit within blob size limits (typically 50MB per blob, configurable).\nArgs: blobs: List of binary data (e.g., tar shards) to upload as blobs. schema_uri: AT URI of the schema record. name: Human-readable dataset name. description: Human-readable description. tags: Searchable tags for discovery. license: SPDX license identifier. metadata: Arbitrary metadata dictionary. mime_type: MIME type for the blobs (default: application/x-tar). rkey: Optional explicit record key.\nReturns: The AT URI of the created dataset record.\nNote: Blobs are only retained by the PDS when referenced in a committed record. This method handles that automatically.\n\n\n\natmosphere.DatasetPublisher.publish_with_urls(\n    urls,\n    schema_uri,\n    *,\n    name,\n    description=None,\n    tags=None,\n    license=None,\n    metadata=None,\n    rkey=None,\n)\nPublish a dataset record with explicit URLs.\nThis method allows publishing a dataset record without having a Dataset object, useful for registering existing WebDataset files.\nArgs: urls: List of WebDataset URLs with brace notation. schema_uri: AT URI of the schema record. name: Human-readable dataset name. description: Human-readable description. tags: Searchable tags for discovery. license: SPDX license identifier. metadata: Arbitrary metadata dictionary. rkey: Optional explicit record key.\nReturns: The AT URI of the created dataset record."
  },
  {
    "objectID": "api/SchemaPublisher.html",
    "href": "api/SchemaPublisher.html",
    "title": "SchemaPublisher",
    "section": "",
    "text": "atmosphere.SchemaPublisher(client)\nPublishes PackableSample schemas to ATProto.\nThis class introspects a PackableSample class to extract its field definitions and publishes them as an ATProto schema record.\nExample: &gt;&gt;&gt; @atdata.packable … class MySample: … image: NDArray … label: str … &gt;&gt;&gt; client = AtmosphereClient() &gt;&gt;&gt; client.login(“handle”, “password”) &gt;&gt;&gt; &gt;&gt;&gt; publisher = SchemaPublisher(client) &gt;&gt;&gt; uri = publisher.publish(MySample, version=“1.0.0”) &gt;&gt;&gt; print(uri) at://did:plc:…/ac.foundation.dataset.sampleSchema/…\n\n\n\n\n\nName\nDescription\n\n\n\n\npublish\nPublish a PackableSample schema to ATProto.\n\n\n\n\n\natmosphere.SchemaPublisher.publish(\n    sample_type,\n    *,\n    name=None,\n    version='1.0.0',\n    description=None,\n    metadata=None,\n    rkey=None,\n)\nPublish a PackableSample schema to ATProto.\nArgs: sample_type: The PackableSample class to publish. name: Human-readable name. Defaults to the class name. version: Semantic version string (e.g., ‘1.0.0’). description: Human-readable description. metadata: Arbitrary metadata dictionary. rkey: Optional explicit record key. If not provided, a TID is generated.\nReturns: The AT URI of the created schema record.\nRaises: ValueError: If sample_type is not a dataclass or client is not authenticated. TypeError: If a field type is not supported."
  },
  {
    "objectID": "api/SchemaPublisher.html#methods",
    "href": "api/SchemaPublisher.html#methods",
    "title": "SchemaPublisher",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\npublish\nPublish a PackableSample schema to ATProto.\n\n\n\n\n\natmosphere.SchemaPublisher.publish(\n    sample_type,\n    *,\n    name=None,\n    version='1.0.0',\n    description=None,\n    metadata=None,\n    rkey=None,\n)\nPublish a PackableSample schema to ATProto.\nArgs: sample_type: The PackableSample class to publish. name: Human-readable name. Defaults to the class name. version: Semantic version string (e.g., ‘1.0.0’). description: Human-readable description. metadata: Arbitrary metadata dictionary. rkey: Optional explicit record key. If not provided, a TID is generated.\nReturns: The AT URI of the created schema record.\nRaises: ValueError: If sample_type is not a dataclass or client is not authenticated. TypeError: If a field type is not supported."
  },
  {
    "objectID": "api/promote_to_atmosphere.html",
    "href": "api/promote_to_atmosphere.html",
    "title": "promote_to_atmosphere",
    "section": "",
    "text": "promote_to_atmosphere\npromote.promote_to_atmosphere(\n    local_entry,\n    local_index,\n    atmosphere_client,\n    *,\n    data_store=None,\n    name=None,\n    description=None,\n    tags=None,\n    license=None,\n)\nPromote a local dataset to the atmosphere network.\nThis function takes a locally-indexed dataset and publishes it to ATProto, making it discoverable on the federated atmosphere network.\nArgs: local_entry: The LocalDatasetEntry to promote. local_index: Local index containing the schema for this entry. atmosphere_client: Authenticated AtmosphereClient. data_store: Optional data store for copying data to new location. If None, the existing data_urls are used as-is. name: Override name for the atmosphere record. Defaults to local name. description: Optional description for the dataset. tags: Optional tags for discovery. license: Optional license identifier.\nReturns: AT URI of the created atmosphere dataset record.\nRaises: KeyError: If schema not found in local index. ValueError: If local entry has no data URLs.\nExample: &gt;&gt;&gt; entry = local_index.get_dataset(“mnist-train”) &gt;&gt;&gt; uri = promote_to_atmosphere(entry, local_index, client) &gt;&gt;&gt; print(uri) at://did:plc:abc123/ac.foundation.dataset.datasetIndex/…"
  },
  {
    "objectID": "api/load_dataset.html",
    "href": "api/load_dataset.html",
    "title": "load_dataset",
    "section": "",
    "text": "load_dataset\nload_dataset(\n    path,\n    sample_type=None,\n    *,\n    split=None,\n    data_files=None,\n    streaming=False,\n    index=None,\n)\nLoad a dataset from local files, remote URLs, or an index.\nThis function provides a HuggingFace Datasets-style interface for loading atdata typed datasets. It handles path resolution, split detection, and returns either a single Dataset or a DatasetDict depending on the split parameter.\nWhen no sample_type is provided, returns a Dataset[DictSample] that provides dynamic dict-like access to fields. Use .as_type(MyType) to convert to a typed schema.\nArgs: path: Path to dataset. Can be: - Index lookup: “@handle/dataset-name” or “@local/dataset-name” - WebDataset brace notation: “path/to/{train,test}-{000..099}.tar” - Local directory: “./data/” (scans for .tar files) - Glob pattern: “path/to/.tar” - Remote URL: ”s3://bucket/path/data-.tar” - Single file: “path/to/data.tar”\nsample_type: The PackableSample subclass defining the schema. If None,\n    returns ``Dataset[DictSample]`` with dynamic field access. Can also\n    be resolved from an index when using @handle/dataset syntax.\n\nsplit: Which split to load. If None, returns a DatasetDict with all\n    detected splits. If specified (e.g., \"train\", \"test\"), returns\n    a single Dataset for that split.\n\ndata_files: Optional explicit mapping of data files. Can be:\n    - str: Single file pattern\n    - list[str]: List of file patterns (assigned to \"train\")\n    - dict[str, str | list[str]]: Explicit split -&gt; files mapping\n\nstreaming: If True, explicitly marks the dataset for streaming mode.\n    Note: atdata Datasets are already lazy/streaming via WebDataset\n    pipelines, so this parameter primarily signals intent.\n\nindex: Optional AbstractIndex for dataset lookup. Required when using\n    @handle/dataset syntax. When provided with an indexed path, the\n    schema can be auto-resolved from the index.\nReturns: If split is None: DatasetDict with all detected splits. If split is specified: Dataset for that split. Type is ST if sample_type provided, otherwise DictSample.\nRaises: ValueError: If the specified split is not found. FileNotFoundError: If no data files are found at the path. KeyError: If dataset not found in index.\nExample: &gt;&gt;&gt; # Load without type - get DictSample for exploration &gt;&gt;&gt; ds = load_dataset(“./data/train.tar”, split=“train”) &gt;&gt;&gt; for sample in ds.ordered(): … print(sample.keys()) # Explore fields … print(sample[“text”]) # Dict-style access … print(sample.label) # Attribute access &gt;&gt;&gt; &gt;&gt;&gt; # Convert to typed schema &gt;&gt;&gt; typed_ds = ds.as_type(TextData) &gt;&gt;&gt; &gt;&gt;&gt; # Or load with explicit type directly &gt;&gt;&gt; train_ds = load_dataset(“./data/train-*.tar”, TextData, split=“train”) &gt;&gt;&gt; &gt;&gt;&gt; # Load from index with auto-type resolution &gt;&gt;&gt; index = LocalIndex() &gt;&gt;&gt; ds = load_dataset(“@local/my-dataset”, index=index, split=“train”)"
  },
  {
    "objectID": "api/PackableSample.html",
    "href": "api/PackableSample.html",
    "title": "PackableSample",
    "section": "",
    "text": "PackableSample()\nBase class for samples that can be serialized with msgpack.\nThis abstract base class provides automatic serialization/deserialization for dataclass-based samples. Fields annotated as NDArray or NDArray | None are automatically converted between numpy arrays and bytes during packing/unpacking.\nSubclasses should be defined either by: 1. Direct inheritance with the @dataclass decorator 2. Using the @packable decorator (recommended)\nExample: &gt;&gt;&gt; @packable … class MyData: … name: str … embeddings: NDArray … &gt;&gt;&gt; sample = MyData(name=“test”, embeddings=np.array([1.0, 2.0])) &gt;&gt;&gt; packed = sample.packed # Serialize to bytes &gt;&gt;&gt; restored = MyData.from_bytes(packed) # Deserialize\n\n\n\n\n\nName\nDescription\n\n\n\n\nas_wds\nPack this sample’s data for writing to WebDataset.\n\n\npacked\nPack this sample’s data into msgpack bytes.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfrom_bytes\nCreate a sample instance from raw msgpack bytes.\n\n\nfrom_data\nCreate a sample instance from unpacked msgpack data.\n\n\n\n\n\nPackableSample.from_bytes(bs)\nCreate a sample instance from raw msgpack bytes.\nArgs: bs: Raw bytes from a msgpack-serialized sample.\nReturns: A new instance of this sample class deserialized from the bytes.\n\n\n\nPackableSample.from_data(data)\nCreate a sample instance from unpacked msgpack data.\nArgs: data: Dictionary with keys matching the sample’s field names.\nReturns: New instance with NDArray fields auto-converted from bytes."
  },
  {
    "objectID": "api/PackableSample.html#attributes",
    "href": "api/PackableSample.html#attributes",
    "title": "PackableSample",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nas_wds\nPack this sample’s data for writing to WebDataset.\n\n\npacked\nPack this sample’s data into msgpack bytes."
  },
  {
    "objectID": "api/PackableSample.html#methods",
    "href": "api/PackableSample.html#methods",
    "title": "PackableSample",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfrom_bytes\nCreate a sample instance from raw msgpack bytes.\n\n\nfrom_data\nCreate a sample instance from unpacked msgpack data.\n\n\n\n\n\nPackableSample.from_bytes(bs)\nCreate a sample instance from raw msgpack bytes.\nArgs: bs: Raw bytes from a msgpack-serialized sample.\nReturns: A new instance of this sample class deserialized from the bytes.\n\n\n\nPackableSample.from_data(data)\nCreate a sample instance from unpacked msgpack data.\nArgs: data: Dictionary with keys matching the sample’s field names.\nReturns: New instance with NDArray fields auto-converted from bytes."
  },
  {
    "objectID": "api/SchemaLoader.html",
    "href": "api/SchemaLoader.html",
    "title": "SchemaLoader",
    "section": "",
    "text": "atmosphere.SchemaLoader(client)\nLoads PackableSample schemas from ATProto.\nThis class fetches schema records from ATProto and can list available schemas from a repository.\nExample: &gt;&gt;&gt; client = AtmosphereClient() &gt;&gt;&gt; client.login(“handle”, “password”) &gt;&gt;&gt; &gt;&gt;&gt; loader = SchemaLoader(client) &gt;&gt;&gt; schema = loader.get(“at://did:plc:…/ac.foundation.dataset.sampleSchema/…”) &gt;&gt;&gt; print(schema[“name”]) ‘MySample’\n\n\n\n\n\nName\nDescription\n\n\n\n\nget\nFetch a schema record by AT URI.\n\n\nlist_all\nList schema records from a repository.\n\n\n\n\n\natmosphere.SchemaLoader.get(uri)\nFetch a schema record by AT URI.\nArgs: uri: The AT URI of the schema record.\nReturns: The schema record as a dictionary.\nRaises: ValueError: If the record is not a schema record. atproto.exceptions.AtProtocolError: If record not found.\n\n\n\natmosphere.SchemaLoader.list_all(repo=None, limit=100)\nList schema records from a repository.\nArgs: repo: The DID of the repository. Defaults to authenticated user. limit: Maximum number of records to return.\nReturns: List of schema records."
  },
  {
    "objectID": "api/SchemaLoader.html#methods",
    "href": "api/SchemaLoader.html#methods",
    "title": "SchemaLoader",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget\nFetch a schema record by AT URI.\n\n\nlist_all\nList schema records from a repository.\n\n\n\n\n\natmosphere.SchemaLoader.get(uri)\nFetch a schema record by AT URI.\nArgs: uri: The AT URI of the schema record.\nReturns: The schema record as a dictionary.\nRaises: ValueError: If the record is not a schema record. atproto.exceptions.AtProtocolError: If record not found.\n\n\n\natmosphere.SchemaLoader.list_all(repo=None, limit=100)\nList schema records from a repository.\nArgs: repo: The DID of the repository. Defaults to authenticated user. limit: Maximum number of records to return.\nReturns: List of schema records."
  },
  {
    "objectID": "tutorials/atmosphere.html",
    "href": "tutorials/atmosphere.html",
    "title": "Atmosphere Publishing",
    "section": "",
    "text": "This tutorial demonstrates how to use the atmosphere module to publish datasets to the AT Protocol network, enabling federated discovery and sharing.",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#prerequisites",
    "href": "tutorials/atmosphere.html#prerequisites",
    "title": "Atmosphere Publishing",
    "section": "Prerequisites",
    "text": "Prerequisites\n\npip install atdata[atmosphere]\nA Bluesky account with an app-specific password\n\n\n\n\n\n\n\nWarning\n\n\n\nAlways use an app-specific password, not your main Bluesky password.",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#setup",
    "href": "tutorials/atmosphere.html#setup",
    "title": "Atmosphere Publishing",
    "section": "Setup",
    "text": "Setup\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport atdata\nfrom atdata.atmosphere import (\n    AtmosphereClient,\n    SchemaPublisher,\n    SchemaLoader,\n    DatasetPublisher,\n    DatasetLoader,\n    AtUri,\n)\nimport webdataset as wds",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#define-sample-types",
    "href": "tutorials/atmosphere.html#define-sample-types",
    "title": "Atmosphere Publishing",
    "section": "Define Sample Types",
    "text": "Define Sample Types\n\n@atdata.packable\nclass ImageSample:\n    \"\"\"A sample containing image data with metadata.\"\"\"\n    image: NDArray\n    label: str\n    confidence: float\n\n@atdata.packable\nclass TextEmbeddingSample:\n    \"\"\"A sample containing text with embedding vectors.\"\"\"\n    text: str\n    embedding: NDArray\n    source: str",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#type-introspection",
    "href": "tutorials/atmosphere.html#type-introspection",
    "title": "Atmosphere Publishing",
    "section": "Type Introspection",
    "text": "Type Introspection\nSee what information is available from a PackableSample type:\n\nfrom dataclasses import fields, is_dataclass\n\nprint(f\"Sample type: {ImageSample.__name__}\")\nprint(f\"Is dataclass: {is_dataclass(ImageSample)}\")\n\nprint(\"\\nFields:\")\nfor field in fields(ImageSample):\n    print(f\"  - {field.name}: {field.type}\")\n\n# Create and serialize a sample\nsample = ImageSample(\n    image=np.random.rand(224, 224, 3).astype(np.float32),\n    label=\"cat\",\n    confidence=0.95,\n)\n\npacked = sample.packed\nprint(f\"\\nSerialized size: {len(packed):,} bytes\")\n\n# Round-trip\nrestored = ImageSample.from_bytes(packed)\nprint(f\"Round-trip successful: {np.allclose(sample.image, restored.image)}\")",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#at-uri-parsing",
    "href": "tutorials/atmosphere.html#at-uri-parsing",
    "title": "Atmosphere Publishing",
    "section": "AT URI Parsing",
    "text": "AT URI Parsing\nATProto records are identified by AT URIs:\n\nuris = [\n    \"at://did:plc:abc123/ac.foundation.dataset.sampleSchema/xyz789\",\n    \"at://alice.bsky.social/ac.foundation.dataset.record/my-dataset\",\n]\n\nfor uri_str in uris:\n    print(f\"\\nParsing: {uri_str}\")\n    uri = AtUri.parse(uri_str)\n    print(f\"  Authority:  {uri.authority}\")\n    print(f\"  Collection: {uri.collection}\")\n    print(f\"  Rkey:       {uri.rkey}\")",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#authentication",
    "href": "tutorials/atmosphere.html#authentication",
    "title": "Atmosphere Publishing",
    "section": "Authentication",
    "text": "Authentication\nConnect to ATProto:\n\nclient = AtmosphereClient()\nclient.login(\"your.handle.social\", \"your-app-password\")\n\nprint(f\"Authenticated as: {client.handle}\")\nprint(f\"DID: {client.did}\")",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#publish-a-schema",
    "href": "tutorials/atmosphere.html#publish-a-schema",
    "title": "Atmosphere Publishing",
    "section": "Publish a Schema",
    "text": "Publish a Schema\n\nschema_publisher = SchemaPublisher(client)\nschema_uri = schema_publisher.publish(\n    ImageSample,\n    name=\"ImageSample\",\n    version=\"1.0.0\",\n    description=\"Demo: Image sample with label and confidence\",\n)\nprint(f\"Schema URI: {schema_uri}\")",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#list-your-schemas",
    "href": "tutorials/atmosphere.html#list-your-schemas",
    "title": "Atmosphere Publishing",
    "section": "List Your Schemas",
    "text": "List Your Schemas\n\nschema_loader = SchemaLoader(client)\nschemas = schema_loader.list_all(limit=10)\nprint(f\"Found {len(schemas)} schema(s)\")\n\nfor schema in schemas:\n    print(f\"  - {schema.get('name', 'Unknown')}: v{schema.get('version', '?')}\")",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#publish-a-dataset",
    "href": "tutorials/atmosphere.html#publish-a-dataset",
    "title": "Atmosphere Publishing",
    "section": "Publish a Dataset",
    "text": "Publish a Dataset\n\nWith External URLs\n\ndataset_publisher = DatasetPublisher(client)\ndataset_uri = dataset_publisher.publish_with_urls(\n    urls=[\"s3://example-bucket/demo-data-{000000..000009}.tar\"],\n    schema_uri=str(schema_uri),\n    name=\"Demo Image Dataset\",\n    description=\"Example dataset demonstrating atmosphere publishing\",\n    tags=[\"demo\", \"images\", \"atdata\"],\n    license=\"MIT\",\n)\nprint(f\"Dataset URI: {dataset_uri}\")\n\n\n\nWith Blob Storage\nFor smaller datasets, store data directly in ATProto blobs:\n\nimport io\n\n@atdata.packable\nclass DemoSample:\n    id: int\n    text: str\n\n# Create samples\nsamples = [\n    DemoSample(id=0, text=\"Hello from blob storage!\"),\n    DemoSample(id=1, text=\"ATProto is decentralized.\"),\n    DemoSample(id=2, text=\"atdata makes ML data easy.\"),\n]\n\n# Create tar in memory\ntar_buffer = io.BytesIO()\nwith wds.writer.TarWriter(tar_buffer) as sink:\n    for sample in samples:\n        sink.write(sample.as_wds)\n\ntar_data = tar_buffer.getvalue()\nprint(f\"Created tar with {len(samples)} samples ({len(tar_data):,} bytes)\")\n\n# Publish schema\nblob_schema_uri = schema_publisher.publish(DemoSample, version=\"1.0.0\")\n\n# Publish with blob storage\nblob_dataset_uri = dataset_publisher.publish_with_blobs(\n    blobs=[tar_data],\n    schema_uri=str(blob_schema_uri),\n    name=\"Blob Storage Demo Dataset\",\n    description=\"Small dataset stored directly in ATProto blobs\",\n    tags=[\"demo\", \"blob-storage\"],\n)\nprint(f\"Dataset URI: {blob_dataset_uri}\")",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#list-and-load-datasets",
    "href": "tutorials/atmosphere.html#list-and-load-datasets",
    "title": "Atmosphere Publishing",
    "section": "List and Load Datasets",
    "text": "List and Load Datasets\n\ndataset_loader = DatasetLoader(client)\ndatasets = dataset_loader.list_all(limit=10)\nprint(f\"Found {len(datasets)} dataset(s)\")\n\nfor ds in datasets:\n    print(f\"  - {ds.get('name', 'Unknown')}\")\n    print(f\"    Schema: {ds.get('schemaRef', 'N/A')}\")\n    tags = ds.get('tags', [])\n    if tags:\n        print(f\"    Tags: {', '.join(tags)}\")",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#load-a-dataset",
    "href": "tutorials/atmosphere.html#load-a-dataset",
    "title": "Atmosphere Publishing",
    "section": "Load a Dataset",
    "text": "Load a Dataset\n\n# Check storage type\nstorage_type = dataset_loader.get_storage_type(str(blob_dataset_uri))\nprint(f\"Storage type: {storage_type}\")\n\nif storage_type == \"blobs\":\n    blob_urls = dataset_loader.get_blob_urls(str(blob_dataset_uri))\n    print(f\"Blob URLs: {len(blob_urls)} blob(s)\")\n\n# Load and iterate (works for both storage types)\nds = dataset_loader.to_dataset(str(blob_dataset_uri), DemoSample)\nfor batch in ds.ordered():\n    print(f\"Sample id={batch.id}, text={batch.text}\")",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#complete-publishing-workflow",
    "href": "tutorials/atmosphere.html#complete-publishing-workflow",
    "title": "Atmosphere Publishing",
    "section": "Complete Publishing Workflow",
    "text": "Complete Publishing Workflow\n\n# 1. Define and create samples\n@atdata.packable\nclass FeatureSample:\n    features: NDArray\n    label: int\n    source: str\n\nsamples = [\n    FeatureSample(\n        features=np.random.randn(128).astype(np.float32),\n        label=i % 10,\n        source=\"synthetic\",\n    )\n    for i in range(1000)\n]\n\n# 2. Write to tar\nwith wds.writer.TarWriter(\"features.tar\") as sink:\n    for i, s in enumerate(samples):\n        sink.write({**s.as_wds, \"__key__\": f\"{i:06d}\"})\n\n# 3. Authenticate\nfrom atdata.atmosphere import AtmosphereIndex\n\nclient = AtmosphereClient()\nclient.login(\"myhandle.bsky.social\", \"app-password\")\nindex = AtmosphereIndex(client)\n\n# 4. Publish schema\nschema_uri = index.publish_schema(\n    FeatureSample,\n    version=\"1.0.0\",\n    description=\"Feature vectors with labels\",\n)\n\n# 5. Publish dataset\ndataset = atdata.Dataset[FeatureSample](\"features.tar\")\nentry = index.insert_dataset(\n    dataset,\n    name=\"synthetic-features-v1\",\n    schema_ref=schema_uri,\n    tags=[\"features\", \"synthetic\"],\n)\n\nprint(f\"Published: {entry.uri}\")",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/atmosphere.html#next-steps",
    "href": "tutorials/atmosphere.html#next-steps",
    "title": "Atmosphere Publishing",
    "section": "Next Steps",
    "text": "Next Steps\n\nPromotion Workflow - Migrate from local storage to atmosphere\nAtmosphere Reference - Complete API reference\nProtocols - Abstract interfaces",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Atmosphere Publishing"
    ]
  },
  {
    "objectID": "tutorials/quickstart.html",
    "href": "tutorials/quickstart.html",
    "title": "Quick Start",
    "section": "",
    "text": "This guide walks you through the basics of atdata: defining sample types, writing datasets, and iterating over them.",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Quick Start"
    ]
  },
  {
    "objectID": "tutorials/quickstart.html#installation",
    "href": "tutorials/quickstart.html#installation",
    "title": "Quick Start",
    "section": "Installation",
    "text": "Installation\npip install atdata\n\n# With ATProto support\npip install atdata[atmosphere]",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Quick Start"
    ]
  },
  {
    "objectID": "tutorials/quickstart.html#define-a-sample-type",
    "href": "tutorials/quickstart.html#define-a-sample-type",
    "title": "Quick Start",
    "section": "Define a Sample Type",
    "text": "Define a Sample Type\nUse the @packable decorator to create a typed sample:\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport atdata\n\n@atdata.packable\nclass ImageSample:\n    \"\"\"A sample containing an image with label and confidence.\"\"\"\n    image: NDArray\n    label: str\n    confidence: float\n\nThe @packable decorator:\n\nConverts your class into a dataclass\nAdds automatic msgpack serialization\nHandles NDArray conversion to/from bytes",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Quick Start"
    ]
  },
  {
    "objectID": "tutorials/quickstart.html#create-sample-instances",
    "href": "tutorials/quickstart.html#create-sample-instances",
    "title": "Quick Start",
    "section": "Create Sample Instances",
    "text": "Create Sample Instances\n\n# Create a single sample\nsample = ImageSample(\n    image=np.random.rand(224, 224, 3).astype(np.float32),\n    label=\"cat\",\n    confidence=0.95,\n)\n\n# Check serialization\npacked_bytes = sample.packed\nprint(f\"Serialized size: {len(packed_bytes):,} bytes\")\n\n# Verify round-trip\nrestored = ImageSample.from_bytes(packed_bytes)\nassert np.allclose(sample.image, restored.image)\nprint(\"Round-trip successful!\")",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Quick Start"
    ]
  },
  {
    "objectID": "tutorials/quickstart.html#write-a-dataset",
    "href": "tutorials/quickstart.html#write-a-dataset",
    "title": "Quick Start",
    "section": "Write a Dataset",
    "text": "Write a Dataset\nUse WebDataset’s TarWriter to create dataset files:\n\nimport webdataset as wds\n\n# Create 100 samples\nsamples = [\n    ImageSample(\n        image=np.random.rand(224, 224, 3).astype(np.float32),\n        label=f\"class_{i % 10}\",\n        confidence=np.random.rand(),\n    )\n    for i in range(100)\n]\n\n# Write to tar file\nwith wds.writer.TarWriter(\"my-dataset-000000.tar\") as sink:\n    for i, sample in enumerate(samples):\n        sink.write({**sample.as_wds, \"__key__\": f\"sample_{i:06d}\"})\n\nprint(\"Wrote 100 samples to my-dataset-000000.tar\")",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Quick Start"
    ]
  },
  {
    "objectID": "tutorials/quickstart.html#load-and-iterate",
    "href": "tutorials/quickstart.html#load-and-iterate",
    "title": "Quick Start",
    "section": "Load and Iterate",
    "text": "Load and Iterate\nCreate a typed Dataset and iterate with batching:\n\n# Load dataset with type\ndataset = atdata.Dataset[ImageSample](\"my-dataset-000000.tar\")\n\n# Iterate in order with batching\nfor batch in dataset.ordered(batch_size=16):\n    # NDArray fields are stacked\n    images = batch.image        # shape: (16, 224, 224, 3)\n\n    # Other fields become lists\n    labels = batch.label        # list of 16 strings\n    confidences = batch.confidence  # list of 16 floats\n\n    print(f\"Batch shape: {images.shape}\")\n    print(f\"Labels: {labels[:3]}...\")\n    break",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Quick Start"
    ]
  },
  {
    "objectID": "tutorials/quickstart.html#shuffled-iteration",
    "href": "tutorials/quickstart.html#shuffled-iteration",
    "title": "Quick Start",
    "section": "Shuffled Iteration",
    "text": "Shuffled Iteration\nFor training, use shuffled iteration:\n\nfor batch in dataset.shuffled(batch_size=32):\n    # Samples are shuffled at shard and sample level\n    images = batch.image\n    labels = batch.label\n\n    # Train your model\n    # model.train(images, labels)\n    break",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Quick Start"
    ]
  },
  {
    "objectID": "tutorials/quickstart.html#use-lenses-for-type-transformations",
    "href": "tutorials/quickstart.html#use-lenses-for-type-transformations",
    "title": "Quick Start",
    "section": "Use Lenses for Type Transformations",
    "text": "Use Lenses for Type Transformations\nView datasets through different schemas:\n\n# Define a simplified view type\n@atdata.packable\nclass SimplifiedSample:\n    label: str\n    confidence: float\n\n# Create a lens transformation\n@atdata.lens\ndef simplify(src: ImageSample) -&gt; SimplifiedSample:\n    return SimplifiedSample(label=src.label, confidence=src.confidence)\n\n# View dataset through lens\nsimple_ds = dataset.as_type(SimplifiedSample)\n\nfor batch in simple_ds.ordered(batch_size=8):\n    print(f\"Labels: {batch.label}\")\n    print(f\"Confidences: {batch.confidence}\")\n    break",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Quick Start"
    ]
  },
  {
    "objectID": "tutorials/quickstart.html#next-steps",
    "href": "tutorials/quickstart.html#next-steps",
    "title": "Quick Start",
    "section": "Next Steps",
    "text": "Next Steps\n\nLocal Workflow - Store datasets with Redis + S3\nAtmosphere Publishing - Publish to ATProto federation\nPackable Samples - Deep dive into sample types\nDatasets - Advanced dataset operations",
    "crumbs": [
      "Guide",
      "Getting Started",
      "Quick Start"
    ]
  },
  {
    "objectID": "reference/uri-spec.html",
    "href": "reference/uri-spec.html",
    "title": "URI Specification",
    "section": "",
    "text": "The atdata:// URI scheme provides a unified way to address atdata resources across local development and the ATProto federation.",
    "crumbs": [
      "Guide",
      "Reference",
      "URI Specification"
    ]
  },
  {
    "objectID": "reference/uri-spec.html#overview",
    "href": "reference/uri-spec.html#overview",
    "title": "URI Specification",
    "section": "Overview",
    "text": "Overview\nThe atdata URI scheme:\n\nFollows RFC 3986 syntax\nProvides consistent addressing for local and atmosphere resources\nEnables seamless promotion from development to production",
    "crumbs": [
      "Guide",
      "Reference",
      "URI Specification"
    ]
  },
  {
    "objectID": "reference/uri-spec.html#uri-format",
    "href": "reference/uri-spec.html#uri-format",
    "title": "URI Specification",
    "section": "URI Format",
    "text": "URI Format\natdata://{authority}/{resource_type}/{name}@{version}\n\nAuthority\nThe authority identifies where the resource is stored:\n\n\n\nAuthority\nDescription\nExample\n\n\n\n\nlocal\nLocal Redis/S3 storage\natdata://local/...\n\n\n{handle}\nATProto handle\natdata://alice.bsky.social/...\n\n\n{did}\nATProto DID\natdata://did:plc:abc123/...\n\n\n\n\n\nResource Types\n\n\n\nResource Type\nDescription\n\n\n\n\nsampleSchema\nPackableSample type definitions\n\n\ndataset\nDataset entries (future)\n\n\nlens\nLens transformations (future)\n\n\n\n\n\nVersion Specifiers\nVersions follow semantic versioning and are specified with @:\n\n\n\nSpecifier\nDescription\nExample\n\n\n\n\n@{major}.{minor}.{patch}\nExact version\n@1.0.0, @2.1.3\n\n\n(none)\nLatest version\nResolves to highest semver",
    "crumbs": [
      "Guide",
      "Reference",
      "URI Specification"
    ]
  },
  {
    "objectID": "reference/uri-spec.html#examples",
    "href": "reference/uri-spec.html#examples",
    "title": "URI Specification",
    "section": "Examples",
    "text": "Examples\n\nLocal Development\n\nfrom atdata.local import Index\n\nindex = Index()\n\n# Publish a schema (returns atdata:// URI)\nref = index.publish_schema(MySample, version=\"1.0.0\")\n# =&gt; \"atdata://local/sampleSchema/MySample@1.0.0\"\n\n# Auto-increment version\nref = index.publish_schema(MySample)\n# =&gt; \"atdata://local/sampleSchema/MySample@1.0.1\"\n\n# Retrieve by URI\nschema = index.get_schema(\"atdata://local/sampleSchema/MySample@1.0.0\")\n\n\n\nAtmosphere (ATProto Federation)\n\nfrom atdata.atmosphere import Client\n\nclient = Client()\n\n# Publish returns at:// URI that maps to atdata://\nref = client.publish_schema(MySample)\n# =&gt; \"at://did:plc:abc123/ac.foundation.dataset.sampleSchema/xyz\"\n\n# Can also be addressed as:\n# =&gt; \"atdata://did:plc:abc123/sampleSchema/MySample@1.0.0\"\n# =&gt; \"atdata://alice.bsky.social/sampleSchema/MySample@1.0.0\"",
    "crumbs": [
      "Guide",
      "Reference",
      "URI Specification"
    ]
  },
  {
    "objectID": "reference/uri-spec.html#relationship-to-at-protocol-uris",
    "href": "reference/uri-spec.html#relationship-to-at-protocol-uris",
    "title": "URI Specification",
    "section": "Relationship to AT Protocol URIs",
    "text": "Relationship to AT Protocol URIs\nThe atdata:// scheme is inspired by and maps to ATProto’s at:// scheme:\n\n\n\n\n\n\n\natdata://\nat://\n\n\n\n\natdata://{did}/sampleSchema/{name}@{version}\nat://{did}/ac.foundation.dataset.sampleSchema/{rkey}\n\n\natdata://local/...\n(local only, no at:// equivalent)\n\n\n\nWhen publishing to the atmosphere, atdata URIs are automatically resolved to their corresponding at:// URIs for federation compatibility.",
    "crumbs": [
      "Guide",
      "Reference",
      "URI Specification"
    ]
  },
  {
    "objectID": "reference/uri-spec.html#legacy-format",
    "href": "reference/uri-spec.html#legacy-format",
    "title": "URI Specification",
    "section": "Legacy Format",
    "text": "Legacy Format\nFor backwards compatibility, the local index also accepts the legacy format:\nlocal://schemas/{module.Class}@{version}\nThis format is deprecated and will be removed in a future version. Use atdata://local/sampleSchema/{name}@{version} instead.",
    "crumbs": [
      "Guide",
      "Reference",
      "URI Specification"
    ]
  },
  {
    "objectID": "reference/local-storage.html",
    "href": "reference/local-storage.html",
    "title": "Local Storage",
    "section": "",
    "text": "The local storage module provides a Redis + S3 backend for storing and managing datasets before publishing to the ATProto federation.",
    "crumbs": [
      "Guide",
      "Reference",
      "Local Storage"
    ]
  },
  {
    "objectID": "reference/local-storage.html#overview",
    "href": "reference/local-storage.html#overview",
    "title": "Local Storage",
    "section": "Overview",
    "text": "Overview\nLocal storage uses:\n\nRedis for indexing and tracking dataset metadata\nS3-compatible storage for dataset tar files\n\nThis enables development and small-scale deployment before promoting to the full ATProto infrastructure.",
    "crumbs": [
      "Guide",
      "Reference",
      "Local Storage"
    ]
  },
  {
    "objectID": "reference/local-storage.html#localindex",
    "href": "reference/local-storage.html#localindex",
    "title": "Local Storage",
    "section": "LocalIndex",
    "text": "LocalIndex\nThe index tracks datasets in Redis:\n\nfrom atdata.local import LocalIndex\n\n# Default connection (localhost:6379)\nindex = LocalIndex()\n\n# Custom Redis connection\nimport redis\nr = redis.Redis(host='custom-host', port=6379)\nindex = LocalIndex(redis=r)\n\n# With connection kwargs\nindex = LocalIndex(host='custom-host', port=6379, db=1)\n\n\nAdding Entries\n\nimport atdata\nfrom numpy.typing import NDArray\n\n@atdata.packable\nclass ImageSample:\n    image: NDArray\n    label: str\n\ndataset = atdata.Dataset[ImageSample](\"data-{000000..000009}.tar\")\n\nentry = index.add_entry(\n    dataset,\n    name=\"my-dataset\",\n    schema_ref=\"atdata://local/sampleSchema/ImageSample@1.0.0\",  # optional\n    metadata={\"description\": \"Training images\"},              # optional\n)\n\nprint(entry.cid)        # Content identifier\nprint(entry.name)       # \"my-dataset\"\nprint(entry.data_urls)  # [\"data-{000000..000009}.tar\"]\n\n\n\nListing and Retrieving\n\n# Iterate all entries\nfor entry in index.entries:\n    print(f\"{entry.name}: {entry.cid}\")\n\n# Get as list\nall_entries = index.all_entries\n\n# Get by name\nentry = index.get_entry_by_name(\"my-dataset\")\n\n# Get by CID\nentry = index.get_entry(\"bafyrei...\")",
    "crumbs": [
      "Guide",
      "Reference",
      "Local Storage"
    ]
  },
  {
    "objectID": "reference/local-storage.html#repo-deprecated",
    "href": "reference/local-storage.html#repo-deprecated",
    "title": "Local Storage",
    "section": "Repo (Deprecated)",
    "text": "Repo (Deprecated)\n\n\n\n\n\n\nWarning\n\n\n\nRepo is deprecated. Use LocalIndex with S3DataStore instead for new code.\n\n\nThe Repo class combines S3 storage with Redis indexing:\n\nfrom atdata.local import Repo\n\n# From credentials file\nrepo = Repo(\n    s3_credentials=\"path/to/.env\",\n    hive_path=\"my-bucket/datasets\",\n)\n\n# From credentials dict\nrepo = Repo(\n    s3_credentials={\n        \"AWS_ENDPOINT\": \"http://localhost:9000\",\n        \"AWS_ACCESS_KEY_ID\": \"minioadmin\",\n        \"AWS_SECRET_ACCESS_KEY\": \"minioadmin\",\n    },\n    hive_path=\"my-bucket/datasets\",\n)\n\nPreferred approach - Use LocalIndex with S3DataStore:\n\nfrom atdata.local import LocalIndex, S3DataStore\n\nstore = S3DataStore(\n    credentials={\n        \"AWS_ENDPOINT\": \"http://localhost:9000\",\n        \"AWS_ACCESS_KEY_ID\": \"minioadmin\",\n        \"AWS_SECRET_ACCESS_KEY\": \"minioadmin\",\n    },\n    bucket=\"my-bucket\",\n)\nindex = LocalIndex(data_store=store)\n\n# Insert dataset\nentry = index.insert_dataset(dataset, name=\"my-dataset\", prefix=\"datasets/v1\")\n\n\nCredentials File Format\nThe .env file should contain:\nAWS_ENDPOINT=http://localhost:9000\nAWS_ACCESS_KEY_ID=your-access-key\nAWS_SECRET_ACCESS_KEY=your-secret-key\n\n\n\n\n\n\nNote\n\n\n\nFor AWS S3, omit AWS_ENDPOINT to use the default endpoint.\n\n\n\n\nInserting Datasets\n\nimport webdataset as wds\nimport numpy as np\n\n# Create dataset from samples\nsamples = [ImageSample(\n    image=np.random.rand(224, 224, 3).astype(np.float32),\n    label=f\"sample_{i}\"\n) for i in range(1000)]\n\nwith wds.writer.TarWriter(\"temp.tar\") as sink:\n    for i, s in enumerate(samples):\n        sink.write({**s.as_wds, \"__key__\": f\"{i:06d}\"})\n\ndataset = atdata.Dataset[ImageSample](\"temp.tar\")\n\n# Insert into repo (writes to S3 + indexes in Redis)\nentry, stored_dataset = repo.insert(\n    dataset,\n    name=\"training-images-v1\",\n    cache_local=False,  # Stream directly to S3\n)\n\nprint(entry.cid)                # Content identifier\nprint(stored_dataset.url)       # S3 URL for the stored data\nprint(stored_dataset.shard_list)  # Individual shard URLs\n\n\n\nInsert Options\n\nentry, ds = repo.insert(\n    dataset,\n    name=\"my-dataset\",\n    cache_local=True,   # Write locally first, then copy (faster for some workloads)\n    maxcount=10000,     # Samples per shard\n    maxsize=100_000_000,  # Max shard size in bytes\n)",
    "crumbs": [
      "Guide",
      "Reference",
      "Local Storage"
    ]
  },
  {
    "objectID": "reference/local-storage.html#localdatasetentry",
    "href": "reference/local-storage.html#localdatasetentry",
    "title": "Local Storage",
    "section": "LocalDatasetEntry",
    "text": "LocalDatasetEntry\nIndex entries provide content-addressable identification:\n\nentry = index.get_entry_by_name(\"my-dataset\")\n\n# Core properties (IndexEntry protocol)\nentry.name        # Human-readable name\nentry.schema_ref  # Schema reference\nentry.data_urls   # WebDataset URLs\nentry.metadata    # Arbitrary metadata dict or None\n\n# Content addressing\nentry.cid         # ATProto-compatible CID (content identifier)\n\n# Legacy compatibility\nentry.wds_url     # First data URL\nentry.sample_kind # Same as schema_ref\n\n\n\n\n\n\n\nTip\n\n\n\nThe CID is generated from the entry’s content (schema_ref + data_urls), ensuring identical data produces identical CIDs whether stored locally or in the atmosphere.",
    "crumbs": [
      "Guide",
      "Reference",
      "Local Storage"
    ]
  },
  {
    "objectID": "reference/local-storage.html#schema-storage",
    "href": "reference/local-storage.html#schema-storage",
    "title": "Local Storage",
    "section": "Schema Storage",
    "text": "Schema Storage\nSchemas can be stored and retrieved from the index:\n\n# Publish a schema\nschema_ref = index.publish_schema(\n    ImageSample,\n    version=\"1.0.0\",\n    description=\"Image with label annotation\",\n)\n# Returns: \"atdata://local/sampleSchema/ImageSample@1.0.0\"\n\n# Retrieve schema record\nschema = index.get_schema(schema_ref)\n# {\n#     \"name\": \"ImageSample\",\n#     \"version\": \"1.0.0\",\n#     \"fields\": [...],\n#     \"description\": \"...\",\n#     \"createdAt\": \"...\",\n# }\n\n# List all schemas\nfor schema in index.list_schemas():\n    print(f\"{schema['name']}@{schema['version']}\")\n\n# Reconstruct sample type from schema\nSampleType = index.decode_schema(schema_ref)\ndataset = atdata.Dataset[SampleType](entry.data_urls[0])",
    "crumbs": [
      "Guide",
      "Reference",
      "Local Storage"
    ]
  },
  {
    "objectID": "reference/local-storage.html#s3datastore",
    "href": "reference/local-storage.html#s3datastore",
    "title": "Local Storage",
    "section": "S3DataStore",
    "text": "S3DataStore\nFor direct S3 operations without Redis indexing:\n\nfrom atdata.local import S3DataStore\n\nstore = S3DataStore(\n    credentials=\"path/to/.env\",\n    bucket=\"my-bucket\",\n)\n\n# Write dataset shards\nurls = store.write_shards(\n    dataset,\n    prefix=\"datasets/v1\",\n    maxcount=10000,\n)\n# Returns: [\"s3://my-bucket/datasets/v1/data--uuid--000000.tar\", ...]\n\n# Check capabilities\nstore.supports_streaming()  # True",
    "crumbs": [
      "Guide",
      "Reference",
      "Local Storage"
    ]
  },
  {
    "objectID": "reference/local-storage.html#complete-workflow-example",
    "href": "reference/local-storage.html#complete-workflow-example",
    "title": "Local Storage",
    "section": "Complete Workflow Example",
    "text": "Complete Workflow Example\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport atdata\nfrom atdata.local import LocalIndex, S3DataStore\nimport webdataset as wds\n\n# 1. Define sample type\n@atdata.packable\nclass TrainingSample:\n    features: NDArray\n    label: int\n    source: str\n\n# 2. Create samples\nsamples = [\n    TrainingSample(\n        features=np.random.randn(128).astype(np.float32),\n        label=i % 10,\n        source=\"synthetic\",\n    )\n    for i in range(10000)\n]\n\n# 3. Write to local tar\nwith wds.writer.TarWriter(\"local-data.tar\") as sink:\n    for i, sample in enumerate(samples):\n        sink.write({**sample.as_wds, \"__key__\": f\"{i:06d}\"})\n\n# 4. Set up index with S3 data store and insert\nstore = S3DataStore(\n    credentials={\n        \"AWS_ENDPOINT\": \"http://localhost:9000\",\n        \"AWS_ACCESS_KEY_ID\": \"minioadmin\",\n        \"AWS_SECRET_ACCESS_KEY\": \"minioadmin\",\n    },\n    bucket=\"datasets-bucket\",\n)\nindex = LocalIndex(data_store=store)\n\n# Publish schema and insert dataset\nindex.publish_schema(TrainingSample, version=\"1.0.0\")\nlocal_ds = atdata.Dataset[TrainingSample](\"local-data.tar\")\nentry = index.insert_dataset(local_ds, name=\"training-v1\", prefix=\"training\")\n\n# 5. Retrieve later\nentry = index.get_entry_by_name(\"training-v1\")\ndataset = atdata.Dataset[TrainingSample](entry.data_urls[0])\n\nfor batch in dataset.ordered(batch_size=32):\n    print(batch.features.shape)  # (32, 128)",
    "crumbs": [
      "Guide",
      "Reference",
      "Local Storage"
    ]
  },
  {
    "objectID": "reference/local-storage.html#related",
    "href": "reference/local-storage.html#related",
    "title": "Local Storage",
    "section": "Related",
    "text": "Related\n\nDatasets - Dataset iteration and batching\nProtocols - AbstractIndex and IndexEntry interfaces\nPromotion - Promoting local datasets to ATProto\nAtmosphere - ATProto federation",
    "crumbs": [
      "Guide",
      "Reference",
      "Local Storage"
    ]
  },
  {
    "objectID": "reference/atmosphere.html",
    "href": "reference/atmosphere.html",
    "title": "Atmosphere (ATProto Integration)",
    "section": "",
    "text": "The atmosphere module enables publishing and discovering datasets on the ATProto network, creating a federated ecosystem for typed datasets.",
    "crumbs": [
      "Guide",
      "Reference",
      "Atmosphere (ATProto Integration)"
    ]
  },
  {
    "objectID": "reference/atmosphere.html#installation",
    "href": "reference/atmosphere.html#installation",
    "title": "Atmosphere (ATProto Integration)",
    "section": "Installation",
    "text": "Installation\npip install atdata[atmosphere]\n# or\npip install atproto",
    "crumbs": [
      "Guide",
      "Reference",
      "Atmosphere (ATProto Integration)"
    ]
  },
  {
    "objectID": "reference/atmosphere.html#overview",
    "href": "reference/atmosphere.html#overview",
    "title": "Atmosphere (ATProto Integration)",
    "section": "Overview",
    "text": "Overview\nATProto integration publishes datasets, schemas, and lenses as records in the ac.foundation.dataset.* namespace. This enables:\n\nDiscovery through the ATProto network\nFederation across different hosts\nVerifiability through content-addressable records",
    "crumbs": [
      "Guide",
      "Reference",
      "Atmosphere (ATProto Integration)"
    ]
  },
  {
    "objectID": "reference/atmosphere.html#atmosphereclient",
    "href": "reference/atmosphere.html#atmosphereclient",
    "title": "Atmosphere (ATProto Integration)",
    "section": "AtmosphereClient",
    "text": "AtmosphereClient\nThe client handles authentication and record operations:\n\nfrom atdata.atmosphere import AtmosphereClient\n\nclient = AtmosphereClient()\n\n# Login with app-specific password (not your main password!)\nclient.login(\"alice.bsky.social\", \"app-password\")\n\nprint(client.did)     # 'did:plc:...'\nprint(client.handle)  # 'alice.bsky.social'\n\n\n\n\n\n\n\nWarning\n\n\n\nAlways use an app-specific password, not your main Bluesky password. Create app passwords at bsky.app/settings/app-passwords.\n\n\n\nSession Management\nSave and restore sessions to avoid re-authentication:\n\n# Export session for later\nsession_string = client.export_session()\n\n# Later: restore session\nnew_client = AtmosphereClient()\nnew_client.login_with_session(session_string)\n\n\n\nCustom PDS\nConnect to a custom PDS instead of bsky.social:\n\nclient = AtmosphereClient(base_url=\"https://pds.example.com\")",
    "crumbs": [
      "Guide",
      "Reference",
      "Atmosphere (ATProto Integration)"
    ]
  },
  {
    "objectID": "reference/atmosphere.html#atmosphereindex",
    "href": "reference/atmosphere.html#atmosphereindex",
    "title": "Atmosphere (ATProto Integration)",
    "section": "AtmosphereIndex",
    "text": "AtmosphereIndex\nThe unified interface for ATProto operations, implementing the AbstractIndex protocol:\n\nfrom atdata.atmosphere import AtmosphereClient, AtmosphereIndex\n\nclient = AtmosphereClient()\nclient.login(\"handle.bsky.social\", \"app-password\")\n\nindex = AtmosphereIndex(client)\n\n\nPublishing Schemas\n\nimport atdata\nfrom numpy.typing import NDArray\n\n@atdata.packable\nclass ImageSample:\n    image: NDArray\n    label: str\n    confidence: float\n\n# Publish schema\nschema_uri = index.publish_schema(\n    ImageSample,\n    version=\"1.0.0\",\n    description=\"Image classification sample\",\n)\n# Returns: \"at://did:plc:.../ac.foundation.dataset.sampleSchema/...\"\n\n\n\nPublishing Datasets\n\ndataset = atdata.Dataset[ImageSample](\"data-{000000..000009}.tar\")\n\nentry = index.insert_dataset(\n    dataset,\n    name=\"imagenet-subset\",\n    schema_ref=schema_uri,           # Optional - auto-publishes if omitted\n    description=\"ImageNet subset\",\n    tags=[\"images\", \"classification\"],\n    license=\"MIT\",\n)\n\nprint(entry.uri)        # AT URI of the record\nprint(entry.data_urls)  # WebDataset URLs\n\n\n\nListing and Retrieving\n\n# List your datasets\nfor entry in index.list_datasets():\n    print(f\"{entry.name}: {entry.schema_ref}\")\n\n# List from another user\nfor entry in index.list_datasets(repo=\"did:plc:other-user\"):\n    print(entry.name)\n\n# Get specific dataset\nentry = index.get_dataset(\"at://did:plc:.../ac.foundation.dataset.record/...\")\n\n# List schemas\nfor schema in index.list_schemas():\n    print(f\"{schema['name']} v{schema['version']}\")\n\n# Decode schema to Python type\nSampleType = index.decode_schema(schema_uri)",
    "crumbs": [
      "Guide",
      "Reference",
      "Atmosphere (ATProto Integration)"
    ]
  },
  {
    "objectID": "reference/atmosphere.html#lower-level-publishers",
    "href": "reference/atmosphere.html#lower-level-publishers",
    "title": "Atmosphere (ATProto Integration)",
    "section": "Lower-Level Publishers",
    "text": "Lower-Level Publishers\nFor more control, use the individual publisher classes:\n\nSchemaPublisher\n\nfrom atdata.atmosphere import SchemaPublisher\n\npublisher = SchemaPublisher(client)\n\nuri = publisher.publish(\n    ImageSample,\n    name=\"ImageSample\",\n    version=\"1.0.0\",\n    description=\"Image with label\",\n    metadata={\"source\": \"training\"},\n)\n\n\n\nDatasetPublisher\n\nfrom atdata.atmosphere import DatasetPublisher\n\npublisher = DatasetPublisher(client)\n\nuri = publisher.publish(\n    dataset,\n    name=\"training-images\",\n    schema_uri=schema_uri,           # Required if auto_publish_schema=False\n    auto_publish_schema=True,        # Publish schema automatically\n    description=\"Training images\",\n    tags=[\"training\", \"images\"],\n    license=\"MIT\",\n)\n\n\nBlob Storage\nFor smaller datasets (up to ~50MB per shard), you can store data directly in ATProto blobs instead of external URLs:\n\nimport io\nimport webdataset as wds\n\n# Create tar data in memory\ntar_buffer = io.BytesIO()\nwith wds.writer.TarWriter(tar_buffer) as sink:\n    for i, sample in enumerate(samples):\n        sink.write({**sample.as_wds, \"__key__\": f\"{i:06d}\"})\n\n# Publish with blob storage\nuri = publisher.publish_with_blobs(\n    blobs=[tar_buffer.getvalue()],\n    schema_uri=schema_uri,\n    name=\"small-dataset\",\n    description=\"Dataset stored in ATProto blobs\",\n    tags=[\"small\", \"demo\"],\n)\n\nTo load datasets with blob storage:\n\nfrom atdata.atmosphere import DatasetLoader\n\nloader = DatasetLoader(client)\n\n# Check storage type\nstorage_type = loader.get_storage_type(uri)  # \"external\" or \"blobs\"\n\nif storage_type == \"blobs\":\n    # Get blob URLs for direct access\n    blob_urls = loader.get_blob_urls(uri)\n\n# to_dataset() handles both storage types automatically\ndataset = loader.to_dataset(uri, MySample)\nfor batch in dataset.ordered(batch_size=32):\n    process(batch)\n\n\n\n\nLensPublisher\n\nfrom atdata.atmosphere import LensPublisher\n\npublisher = LensPublisher(client)\n\n# With code references\nuri = publisher.publish(\n    name=\"simplify\",\n    source_schema=full_schema_uri,\n    target_schema=simple_schema_uri,\n    description=\"Extract label only\",\n    getter_code={\n        \"repository\": \"https://github.com/org/repo\",\n        \"commit\": \"abc123def...\",\n        \"path\": \"transforms/simplify.py:simplify_getter\",\n    },\n    putter_code={\n        \"repository\": \"https://github.com/org/repo\",\n        \"commit\": \"abc123def...\",\n        \"path\": \"transforms/simplify.py:simplify_putter\",\n    },\n)\n\n# Or publish from a Lens object\nfrom atdata.lens import lens\n\n@lens\ndef simplify(src: FullSample) -&gt; SimpleSample:\n    return SimpleSample(label=src.label)\n\nuri = publisher.publish_from_lens(\n    simplify,\n    source_schema=full_schema_uri,\n    target_schema=simple_schema_uri,\n)",
    "crumbs": [
      "Guide",
      "Reference",
      "Atmosphere (ATProto Integration)"
    ]
  },
  {
    "objectID": "reference/atmosphere.html#lower-level-loaders",
    "href": "reference/atmosphere.html#lower-level-loaders",
    "title": "Atmosphere (ATProto Integration)",
    "section": "Lower-Level Loaders",
    "text": "Lower-Level Loaders\nFor direct access to records, use the loader classes:\n\nSchemaLoader\n\nfrom atdata.atmosphere import SchemaLoader\n\nloader = SchemaLoader(client)\n\n# Get a specific schema\nschema = loader.get(\"at://did:plc:abc/ac.foundation.dataset.sampleSchema/xyz\")\nprint(schema[\"name\"], schema[\"version\"])\n\n# List all schemas from a repository\nfor schema in loader.list_all(repo=\"did:plc:other-user\"):\n    print(schema[\"name\"])\n\n\n\nDatasetLoader\n\nfrom atdata.atmosphere import DatasetLoader\n\nloader = DatasetLoader(client)\n\n# Get a specific dataset record\nrecord = loader.get(\"at://did:plc:abc/ac.foundation.dataset.record/xyz\")\n\n# Check storage type\nstorage_type = loader.get_storage_type(uri)  # \"external\" or \"blobs\"\n\n# Get URLs based on storage type\nif storage_type == \"external\":\n    urls = loader.get_urls(uri)\nelse:\n    urls = loader.get_blob_urls(uri)\n\n# Get metadata\nmetadata = loader.get_metadata(uri)\n\n# Create a Dataset object directly\ndataset = loader.to_dataset(uri, MySampleType)\nfor batch in dataset.ordered(batch_size=32):\n    process(batch)\n\n\n\nLensLoader\n\nfrom atdata.atmosphere import LensLoader\n\nloader = LensLoader(client)\n\n# Get a specific lens record\nlens = loader.get(\"at://did:plc:abc/ac.foundation.dataset.lens/xyz\")\nprint(lens[\"name\"])\nprint(lens[\"sourceSchema\"], \"-&gt;\", lens[\"targetSchema\"])\n\n# List all lenses from a repository\nfor lens in loader.list_all():\n    print(lens[\"name\"])\n\n# Find lenses by schema\nlenses = loader.find_by_schemas(\n    source_schema_uri=\"at://did:plc:abc/ac.foundation.dataset.sampleSchema/source\",\n    target_schema_uri=\"at://did:plc:abc/ac.foundation.dataset.sampleSchema/target\",\n)",
    "crumbs": [
      "Guide",
      "Reference",
      "Atmosphere (ATProto Integration)"
    ]
  },
  {
    "objectID": "reference/atmosphere.html#at-uris",
    "href": "reference/atmosphere.html#at-uris",
    "title": "Atmosphere (ATProto Integration)",
    "section": "AT URIs",
    "text": "AT URIs\nATProto records are identified by AT URIs:\n\nfrom atdata.atmosphere import AtUri\n\n# Parse an AT URI\nuri = AtUri.parse(\"at://did:plc:abc123/ac.foundation.dataset.sampleSchema/xyz\")\n\nprint(uri.authority)   # 'did:plc:abc123'\nprint(uri.collection)  # 'ac.foundation.dataset.sampleSchema'\nprint(uri.rkey)        # 'xyz'\n\n# Format back to string\nprint(str(uri))  # 'at://did:plc:abc123/ac.foundation.dataset.sampleSchema/xyz'",
    "crumbs": [
      "Guide",
      "Reference",
      "Atmosphere (ATProto Integration)"
    ]
  },
  {
    "objectID": "reference/atmosphere.html#supported-field-types",
    "href": "reference/atmosphere.html#supported-field-types",
    "title": "Atmosphere (ATProto Integration)",
    "section": "Supported Field Types",
    "text": "Supported Field Types\nSchemas support these field types:\n\n\n\nPython Type\nATProto Type\n\n\n\n\nstr\nprimitive/str\n\n\nint\nprimitive/int\n\n\nfloat\nprimitive/float\n\n\nbool\nprimitive/bool\n\n\nbytes\nprimitive/bytes\n\n\nNDArray\nndarray (default dtype: float32)\n\n\nNDArray[np.float64]\nndarray (dtype: float64)\n\n\nlist[str]\narray with items\n\n\nT \\| None\nOptional field",
    "crumbs": [
      "Guide",
      "Reference",
      "Atmosphere (ATProto Integration)"
    ]
  },
  {
    "objectID": "reference/atmosphere.html#complete-example",
    "href": "reference/atmosphere.html#complete-example",
    "title": "Atmosphere (ATProto Integration)",
    "section": "Complete Example",
    "text": "Complete Example\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport atdata\nfrom atdata.atmosphere import AtmosphereClient, AtmosphereIndex\nimport webdataset as wds\n\n# 1. Define and create samples\n@atdata.packable\nclass FeatureSample:\n    features: NDArray\n    label: int\n    source: str\n\nsamples = [\n    FeatureSample(\n        features=np.random.randn(128).astype(np.float32),\n        label=i % 10,\n        source=\"synthetic\",\n    )\n    for i in range(1000)\n]\n\n# 2. Write to tar\nwith wds.writer.TarWriter(\"features.tar\") as sink:\n    for i, s in enumerate(samples):\n        sink.write({**s.as_wds, \"__key__\": f\"{i:06d}\"})\n\n# 3. Authenticate\nclient = AtmosphereClient()\nclient.login(\"myhandle.bsky.social\", \"app-password\")\n\nindex = AtmosphereIndex(client)\n\n# 4. Publish schema\nschema_uri = index.publish_schema(\n    FeatureSample,\n    version=\"1.0.0\",\n    description=\"Feature vectors with labels\",\n)\n\n# 5. Publish dataset\ndataset = atdata.Dataset[FeatureSample](\"features.tar\")\nentry = index.insert_dataset(\n    dataset,\n    name=\"synthetic-features-v1\",\n    schema_ref=schema_uri,\n    tags=[\"features\", \"synthetic\"],\n)\n\nprint(f\"Published: {entry.uri}\")\n\n# 6. Later: discover and load\nfor dataset_entry in index.list_datasets():\n    print(f\"Found: {dataset_entry.name}\")\n\n    # Reconstruct type from schema\n    SampleType = index.decode_schema(dataset_entry.schema_ref)\n\n    # Load dataset\n    ds = atdata.Dataset[SampleType](dataset_entry.data_urls[0])\n    for batch in ds.ordered(batch_size=32):\n        print(batch.features.shape)\n        break",
    "crumbs": [
      "Guide",
      "Reference",
      "Atmosphere (ATProto Integration)"
    ]
  },
  {
    "objectID": "reference/atmosphere.html#related",
    "href": "reference/atmosphere.html#related",
    "title": "Atmosphere (ATProto Integration)",
    "section": "Related",
    "text": "Related\n\nLocal Storage - Redis + S3 backend\nPromotion - Promoting local datasets to ATProto\nProtocols - AbstractIndex interface\nPackable Samples - Defining sample types",
    "crumbs": [
      "Guide",
      "Reference",
      "Atmosphere (ATProto Integration)"
    ]
  },
  {
    "objectID": "reference/deployment.html",
    "href": "reference/deployment.html",
    "title": "Deployment Guide",
    "section": "",
    "text": "This guide covers deploying atdata in production environments, including Redis setup for LocalIndex, S3 storage configuration, and ATProto publishing considerations.",
    "crumbs": [
      "Guide",
      "Reference",
      "Deployment Guide"
    ]
  },
  {
    "objectID": "reference/deployment.html#local-storage-deployment",
    "href": "reference/deployment.html#local-storage-deployment",
    "title": "Deployment Guide",
    "section": "Local Storage Deployment",
    "text": "Local Storage Deployment\nThe local storage backend uses Redis for metadata indexing and S3-compatible storage for dataset files.\n\nRedis Setup\n\nRequirements\n\nRedis 6.0+ (for Redis-OM compatibility)\nSufficient memory for index metadata (typically &lt; 100MB for most deployments)\n\n\n\nDocker Deployment\n# Basic Redis\ndocker run -d \\\n  --name atdata-redis \\\n  -p 6379:6379 \\\n  -v redis-data:/data \\\n  redis:7-alpine \\\n  redis-server --appendonly yes\n\n# With password\ndocker run -d \\\n  --name atdata-redis \\\n  -p 6379:6379 \\\n  -v redis-data:/data \\\n  redis:7-alpine \\\n  redis-server --appendonly yes --requirepass yourpassword\n\n\nConfiguration\nfrom redis import Redis\nfrom atdata.local import LocalIndex\n\n# Basic connection\nredis = Redis(host=\"localhost\", port=6379)\nindex = LocalIndex(redis=redis)\n\n# With authentication\nredis = Redis(\n    host=\"redis.example.com\",\n    port=6379,\n    password=\"yourpassword\",\n    ssl=True,  # For production\n)\nindex = LocalIndex(redis=redis)\n\n\nRedis Clustering\nFor high-availability deployments:\nfrom redis.cluster import RedisCluster\n\n# Redis Cluster connection\nredis = RedisCluster(\n    host=\"redis-cluster.example.com\",\n    port=6379,\n    password=\"yourpassword\",\n)\nindex = LocalIndex(redis=redis)\n\n\n\n\n\n\nNote\n\n\n\nRedis-OM (used internally) supports Redis Cluster mode. Ensure all nodes have the same configuration.\n\n\n\n\n\nS3 Storage Setup\n\nAWS S3\nfrom atdata.local import S3DataStore\n\n# Using environment credentials (recommended for AWS)\n# Set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY\nstore = S3DataStore(\n    bucket=\"my-atdata-bucket\",\n    prefix=\"datasets/\",\n)\n\n# Explicit credentials\nstore = S3DataStore(\n    bucket=\"my-atdata-bucket\",\n    prefix=\"datasets/\",\n    credentials={\n        \"AWS_ACCESS_KEY_ID\": \"...\",\n        \"AWS_SECRET_ACCESS_KEY\": \"...\",\n        \"AWS_DEFAULT_REGION\": \"us-west-2\",\n    },\n)\n\n\nS3-Compatible Storage (MinIO, Cloudflare R2, etc.)\nstore = S3DataStore(\n    bucket=\"my-bucket\",\n    prefix=\"datasets/\",\n    endpoint_url=\"https://s3.example.com\",\n    credentials={\n        \"AWS_ACCESS_KEY_ID\": \"...\",\n        \"AWS_SECRET_ACCESS_KEY\": \"...\",\n    },\n)\n\n\nMinIO Deployment\n# Docker deployment\ndocker run -d \\\n  --name minio \\\n  -p 9000:9000 \\\n  -p 9001:9001 \\\n  -v minio-data:/data \\\n  -e MINIO_ROOT_USER=minioadmin \\\n  -e MINIO_ROOT_PASSWORD=minioadmin \\\n  minio/minio server /data --console-address \":9001\"\nstore = S3DataStore(\n    bucket=\"atdata\",\n    endpoint_url=\"http://localhost:9000\",\n    credentials={\n        \"AWS_ACCESS_KEY_ID\": \"minioadmin\",\n        \"AWS_SECRET_ACCESS_KEY\": \"minioadmin\",\n    },\n)\n\n\n\nProduction Checklist\n\nRedis persistence enabled (appendonly yes)\nRedis password authentication configured\nRedis TLS enabled for remote connections\nS3 bucket access policies configured (least privilege)\nS3 bucket versioning enabled (for data recovery)\nMonitoring for Redis memory usage\nBackup strategy for Redis data",
    "crumbs": [
      "Guide",
      "Reference",
      "Deployment Guide"
    ]
  },
  {
    "objectID": "reference/deployment.html#atproto-deployment",
    "href": "reference/deployment.html#atproto-deployment",
    "title": "Deployment Guide",
    "section": "ATProto Deployment",
    "text": "ATProto Deployment\n\nAccount Setup\n\nCreate a Bluesky account or use your existing account\nGenerate an app-specific password at bsky.app/settings/app-passwords\nNever use your main account password in code\n\n\n\n\n\n\n\nWarning\n\n\n\nSecurity: Always use app passwords, never your main password. App passwords can be revoked without affecting your account.\n\n\n\n\nAuthentication Patterns\n\nEnvironment Variables (Recommended)\nimport os\nfrom atdata.atmosphere import AtmosphereClient\n\nclient = AtmosphereClient()\nclient.login(\n    os.environ[\"ATPROTO_HANDLE\"],\n    os.environ[\"ATPROTO_APP_PASSWORD\"],\n)\n\n\nSession Persistence\nFor long-running services, persist and reuse sessions:\nimport os\nfrom pathlib import Path\n\nSESSION_FILE = Path(\"~/.atdata/session\").expanduser()\n\nclient = AtmosphereClient()\n\nif SESSION_FILE.exists():\n    # Restore existing session\n    session_string = SESSION_FILE.read_text()\n    try:\n        client.login_with_session(session_string)\n    except Exception:\n        # Session expired, re-authenticate\n        client.login(handle, app_password)\n        SESSION_FILE.parent.mkdir(parents=True, exist_ok=True)\n        SESSION_FILE.write_text(client.export_session())\nelse:\n    # Initial login\n    client.login(handle, app_password)\n    SESSION_FILE.parent.mkdir(parents=True, exist_ok=True)\n    SESSION_FILE.write_text(client.export_session())\n\n\n\nCustom PDS Deployment\nFor self-hosted ATProto infrastructure:\nclient = AtmosphereClient(base_url=\"https://pds.example.com\")\nclient.login(\"handle.example.com\", \"app-password\")\nSee ATProto PDS documentation for self-hosting setup.\n\n\nRate Limiting Considerations\nATProto has rate limits. For bulk operations:\n\nSpace out record creation (1-2 per second for bulk uploads)\nUse batch operations where available\nImplement exponential backoff for retries\nConsider blob storage limits (~50MB per blob)\n\nimport time\n\nfor i, dataset in enumerate(datasets_to_publish):\n    index.insert_dataset(dataset, name=f\"dataset-{i}\", ...)\n    time.sleep(1)  # Rate limiting",
    "crumbs": [
      "Guide",
      "Reference",
      "Deployment Guide"
    ]
  },
  {
    "objectID": "reference/deployment.html#docker-compose-example",
    "href": "reference/deployment.html#docker-compose-example",
    "title": "Deployment Guide",
    "section": "Docker Compose Example",
    "text": "Docker Compose Example\nComplete local deployment with Redis and MinIO:\n# docker-compose.yml\nversion: '3.8'\n\nservices:\n  redis:\n    image: redis:7-alpine\n    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD}\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - redis-data:/data\n\n  minio:\n    image: minio/minio\n    command: server /data --console-address \":9001\"\n    ports:\n      - \"9000:9000\"\n      - \"9001:9001\"\n    environment:\n      MINIO_ROOT_USER: ${MINIO_USER}\n      MINIO_ROOT_PASSWORD: ${MINIO_PASSWORD}\n    volumes:\n      - minio-data:/data\n\nvolumes:\n  redis-data:\n  minio-data:\n# .env\nREDIS_PASSWORD=your-redis-password\nMINIO_USER=minioadmin\nMINIO_PASSWORD=your-minio-password",
    "crumbs": [
      "Guide",
      "Reference",
      "Deployment Guide"
    ]
  },
  {
    "objectID": "reference/deployment.html#monitoring",
    "href": "reference/deployment.html#monitoring",
    "title": "Deployment Guide",
    "section": "Monitoring",
    "text": "Monitoring\n\nRedis Metrics\nKey metrics to monitor:\n\nused_memory: Memory usage\nconnected_clients: Active connections\nkeyspace_hits/misses: Cache efficiency\naof_last_write_status: Persistence health\n\nredis-cli INFO | grep -E \"used_memory|connected_clients|keyspace\"\n\n\nS3 Metrics\n\nRequest counts and latency\nError rates (4xx, 5xx)\nStorage usage by prefix\nData transfer costs",
    "crumbs": [
      "Guide",
      "Reference",
      "Deployment Guide"
    ]
  },
  {
    "objectID": "reference/deployment.html#security-best-practices",
    "href": "reference/deployment.html#security-best-practices",
    "title": "Deployment Guide",
    "section": "Security Best Practices",
    "text": "Security Best Practices\n\nNetwork Isolation: Run Redis and S3 in private networks\nTLS Everywhere: Encrypt connections to Redis and S3\nCredential Rotation: Rotate API keys and passwords regularly\nAccess Logging: Enable S3 access logging for audit trails\nLeast Privilege: Use minimal IAM permissions for S3 access\n\n\nS3 IAM Policy Example\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:ListBucket\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::my-atdata-bucket\",\n        \"arn:aws:s3:::my-atdata-bucket/*\"\n      ]\n    }\n  ]\n}",
    "crumbs": [
      "Guide",
      "Reference",
      "Deployment Guide"
    ]
  },
  {
    "objectID": "reference/troubleshooting.html",
    "href": "reference/troubleshooting.html",
    "title": "Troubleshooting & FAQ",
    "section": "",
    "text": "This page covers common issues, error messages, and frequently asked questions when working with atdata.",
    "crumbs": [
      "Guide",
      "Reference",
      "Troubleshooting & FAQ"
    ]
  },
  {
    "objectID": "reference/troubleshooting.html#common-errors",
    "href": "reference/troubleshooting.html#common-errors",
    "title": "Troubleshooting & FAQ",
    "section": "Common Errors",
    "text": "Common Errors\n\nTypeError: ‘type’ object is not subscriptable\nError:\nTypeError: 'type' object is not subscriptable\nCause: Using Dataset or SampleBatch without subscripting the type parameter on Python &lt; 3.9, or using an unsubscripted generic.\nSolution: Always use the subscripted form:\n# Correct\nds = Dataset[MySample](\"data.tar\")\nbatch = SampleBatch[MySample](samples)\n\n# Incorrect\nds = Dataset(\"data.tar\")  # Missing type parameter\n\n\nAttributeError: ‘NoneType’ object has no attribute…\nError:\nAttributeError: 'NoneType' object has no attribute '__args__'\nCause: Creating a Dataset or SampleBatch without using the subscripted syntax Class[Type](...).\nSolution: These classes use Python’s __orig_class__ mechanism to extract type parameters at runtime. You must use:\nds = Dataset[MySample](url)  # Correct\nNot:\nds = Dataset(url)  # Wrong - no type information\n\n\nRuntimeError: msgpack field not found in sample\nError:\nRuntimeError: Malformed sample: 'msgpack' field not found\nCause: The tar file contains samples that weren’t written with atdata’s serialization format.\nSolution: Ensure samples are written using sample.as_wds:\nwith wds.writer.TarWriter(\"data.tar\") as sink:\n    for sample in samples:\n        sink.write(sample.as_wds)  # Correct\n\n\nValueError: Field type not supported\nError:\nTypeError: Unsupported type for schema field: &lt;class 'SomeType'&gt;\nCause: Using an unsupported Python type in a PackableSample field.\nSupported types:\n\n\n\nPython Type\nNotes\n\n\n\n\nstr\nUnicode strings\n\n\nint\nIntegers\n\n\nfloat\nFloating point\n\n\nbool\nBoolean\n\n\nbytes\nBinary data\n\n\nNDArray\nNumpy arrays (any dtype)\n\n\nlist[T]\nLists of primitives\n\n\nT \\| None\nOptional fields\n\n\n\nNot supported: Nested dataclasses, dicts, custom classes.\n\n\nKeyError when iterating dataset\nError:\nKeyError: 'msgpack'\nCause: The WebDataset tar file structure doesn’t match expected format.\nSolution: Verify your tar file was created correctly:\n# Check tar contents\ntar -tvf data.tar | head -20\nEach sample should have a .msgpack extension in the tar file.",
    "crumbs": [
      "Guide",
      "Reference",
      "Troubleshooting & FAQ"
    ]
  },
  {
    "objectID": "reference/troubleshooting.html#faq",
    "href": "reference/troubleshooting.html#faq",
    "title": "Troubleshooting & FAQ",
    "section": "FAQ",
    "text": "FAQ\n\nHow do I check the sample type of a dataset?\nds = Dataset[MySample](\"data.tar\")\nprint(ds.sample_type)  # &lt;class 'MySample'&gt;\n\n\nHow do I convert a dataset to a different type?\nUse the as_type() method with a registered lens:\n@atdata.lens\ndef my_lens(src: SourceType) -&gt; TargetType:\n    return TargetType(field=src.other_field)\n\nds_view = ds.as_type(TargetType)\n\n\nHow do I handle optional NDArray fields?\nUse NDArray | None annotation:\n@atdata.packable\nclass MySample:\n    required_array: NDArray\n    optional_array: NDArray | None = None\n\n\nWhy is my dataset iteration slow?\nCommon causes:\n\nNetwork latency: Use local caching for remote datasets\nSmall batch sizes: Increase batch_size in ordered() or shuffled()\nShuffle buffer: For shuffled(), the initial parameter controls buffer size\n\n# Larger batches = better throughput\nfor batch in ds.shuffled(batch_size=64, initial=1000):\n    ...\n\n\nHow do I export to parquet?\nds = Dataset[MySample](\"data.tar\")\nds.to_parquet(\"output.parquet\")\n\n# With sample limit (for large datasets)\nds.to_parquet(\"output.parquet\", maxcount=10000)\n\n\n\n\n\n\nWarning\n\n\n\nto_parquet() loads the dataset into memory. For very large datasets, use maxcount to limit samples or process in chunks.\n\n\n\n\nHow do I handle multiple shards?\nUse WebDataset brace notation:\n# Single shard\nds = Dataset[MySample](\"data-000000.tar\")\n\n# Multiple shards (range)\nds = Dataset[MySample](\"data-{000000..000009}.tar\")\n\n# Multiple shards (list)\nds = Dataset[MySample](\"data-{000000,000005,000009}.tar\")\n\n\nCan I use S3 or other cloud storage?\nYes, use S3Source for S3-compatible storage:\nfrom atdata import S3Source, Dataset\n\nsource = S3Source.from_urls(\n    [\"s3://bucket/data-000000.tar\", \"s3://bucket/data-000001.tar\"],\n    endpoint_url=\"https://s3.example.com\",  # Optional for non-AWS S3\n)\n\nds = Dataset[MySample](source)\n\n\nHow do I publish to ATProto/Atmosphere?\nfrom atdata.atmosphere import AtmosphereClient, AtmosphereIndex\n\nclient = AtmosphereClient()\nclient.login(\"handle.bsky.social\", \"app-password\")  # Use app password!\n\nindex = AtmosphereIndex(client)\n\n# Publish schema\nschema_uri = index.publish_schema(MySample, version=\"1.0.0\")\n\n# Publish dataset\nentry = index.insert_dataset(ds, name=\"my-dataset\", schema_ref=schema_uri)\n\n\nWhat’s the difference between LocalIndex and AtmosphereIndex?\n\n\n\nFeature\nLocalIndex\nAtmosphereIndex\n\n\n\n\nStorage\nRedis + S3\nATProto PDS\n\n\nDiscovery\nLocal only\nFederated network\n\n\nAuth\nNone required\nATProto account\n\n\nUse case\nDevelopment, private data\nPublic distribution\n\n\n\nBoth implement the AbstractIndex protocol, so code can work with either.",
    "crumbs": [
      "Guide",
      "Reference",
      "Troubleshooting & FAQ"
    ]
  },
  {
    "objectID": "reference/troubleshooting.html#getting-help",
    "href": "reference/troubleshooting.html#getting-help",
    "title": "Troubleshooting & FAQ",
    "section": "Getting Help",
    "text": "Getting Help\n\nGitHub Issues: github.com/your-org/atdata/issues\nDocumentation: Check the reference pages for detailed API documentation\nExamples: See the examples/ directory for working code samples",
    "crumbs": [
      "Guide",
      "Reference",
      "Troubleshooting & FAQ"
    ]
  }
]
---
title: "Manifest-Powered Queries"
description: "Build per-shard manifests and query samples by metadata without scanning raw data"
execute:
  enabled: true
---

Large datasets become unwieldy when you need to find specific samples.
atdata's **manifest system** records per-shard metadata at write time, then
lets you query across shards using pandas predicates---without ever opening
the tar files themselves.

This example builds a labeled image dataset with manifests, writes them to
JSON + Parquet sidecar files, and queries for specific samples.

## 1 --- Define a sample type with manifest-aware fields

Primitive fields (`str`, `int`, `float`, `bool`, `list`) are automatically
included in manifests with inferred aggregate types.  You can also use
`Annotated[..., ManifestField(...)]` for explicit control.

```{python}
import numpy as np
from numpy.typing import NDArray
from typing import Annotated
import atdata
from atdata import ManifestField


@atdata.packable
class LabeledImage:
    """Image sample with queryable metadata."""

    pixels: NDArray
    label: Annotated[str, ManifestField("categorical")]
    confidence: Annotated[float, ManifestField("numeric")]
    tags: Annotated[list[str], ManifestField("set")]
```

The `ManifestField` annotations tell atdata which aggregate statistics to
collect:

- **categorical** --- tracks the set of distinct values
- **numeric** --- tracks min, max, mean, count
- **set** --- tracks the union of all values across list fields

## 2 --- See which fields are manifest-included

```{python}
from atdata.manifest import resolve_manifest_fields

fields = resolve_manifest_fields(LabeledImage)
for name, mf in fields.items():
    print(f"  {name:15s} -> {mf.aggregate}")
```

## 3 --- Generate data and write shards with manifests

We'll write samples manually using `TarWriter` while building manifests
with `ManifestBuilder`, then serialize them with `ManifestWriter`.

```{python}
import tempfile
from pathlib import Path
import webdataset as wds
from atdata import ManifestBuilder, ManifestWriter

tmpdir = Path(tempfile.mkdtemp(prefix="atdata_manifest_"))
rng = np.random.default_rng(7)

categories = ["cat", "dog", "bird", "fish", "horse"]
all_tags = ["outdoor", "indoor", "closeup", "wide", "blurry", "sharp"]
shard_size = 100
num_shards = 3

for shard_idx in range(num_shards):
    shard_name = f"images-{shard_idx:06d}"
    tar_path = tmpdir / f"{shard_name}.tar"

    builder = ManifestBuilder(
        sample_type=LabeledImage,
        shard_id=shard_name,
    )

    with wds.writer.TarWriter(str(tar_path)) as sink:
        for j in range(shard_size):
            sample = LabeledImage(
                pixels=rng.integers(0, 255, (32, 32, 3), dtype=np.uint8),
                label=categories[rng.integers(0, len(categories))],
                confidence=round(float(rng.uniform(0.5, 1.0)), 3),
                tags=list(rng.choice(all_tags, size=rng.integers(1, 4), replace=False)),
            )
            key = f"sample_{shard_idx * shard_size + j:06d}"
            wds_dict = {**sample.as_wds, "__key__": key}
            sink.write(wds_dict)

            builder.add_sample(
                key=key,
                offset=0,  # simplified; real pipelines track tar offsets
                size=len(sample.packed),
                sample=sample,
            )

    manifest = builder.build()
    writer = ManifestWriter(str(tmpdir / shard_name))
    json_path, parquet_path = writer.write(manifest)

print(f"Wrote {num_shards} shards with manifests to {tmpdir}")
```

## 4 --- Inspect a manifest

Each shard gets a JSON header (aggregates + metadata) and a Parquet file
(per-sample columns).

```{python}
import json

with open(tmpdir / "images-000000.manifest.json") as f:
    header = json.load(f)

print("Shard:", header["shard_id"])
print("Samples:", header["num_samples"])
print()
print("Aggregates:")
for field_name, agg in header["aggregates"].items():
    print(f"  {field_name}: {agg}")
```

## 5 --- Query across shards

`QueryExecutor` loads all manifests from a directory and runs a pandas
predicate over the per-sample Parquet data.

```{python}
from atdata import QueryExecutor

executor = QueryExecutor.from_directory(tmpdir)

# Find high-confidence dog samples
results = executor.query(
    where=lambda df: (df["confidence"] > 0.9) & (df["label"] == "dog")
)

print(f"Found {len(results)} high-confidence dog samples")
for loc in results[:5]:
    print(f"  shard={loc.shard}  key={loc.key}")
```

## 6 --- Compound queries

The predicate receives a full DataFrame, so you can compose arbitrarily
complex filters.

```{python}
# Samples tagged "outdoor" with confidence below 0.7
results = executor.query(
    where=lambda df: (df["confidence"] < 0.7)
    & (df["tags"].apply(lambda t: "outdoor" in t if isinstance(t, list) else False))
)

print(f"Found {len(results)} low-confidence outdoor samples")
```

```{python}
# Count samples per label across all shards
import pandas as pd

all_dfs = [m.samples for m in executor._manifests if not m.samples.empty]
combined = pd.concat(all_dfs, ignore_index=True)
print(combined["label"].value_counts().to_string())
```

## 7 --- Clean up

```{python}
import shutil

shutil.rmtree(tmpdir, ignore_errors=True)
```

## Key takeaways

| Concept | API |
|---------|-----|
| Mark fields for manifests | `Annotated[T, ManifestField("categorical")]` |
| Resolve manifest fields | `resolve_manifest_fields(SampleType)` |
| Build manifest during writes | `ManifestBuilder.add_sample(...)` / `.build()` |
| Serialize to sidecar files | `ManifestWriter(base_path).write(manifest)` |
| Query across shards | `QueryExecutor.from_directory(path).query(where=...)` |
| Locate matching samples | Returns `list[SampleLocation]` with shard + key |

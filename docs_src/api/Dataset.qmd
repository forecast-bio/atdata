# Dataset { #atdata.Dataset }

```python
Dataset(source=None, metadata_url=None, *, url=None)
```

A typed dataset built on WebDataset with lens transformations.

This class wraps WebDataset tar archives and provides type-safe iteration
over samples of a specific ``PackableSample`` type. Samples are stored as
msgpack-serialized data within WebDataset shards.

The dataset supports:
- Ordered and shuffled iteration
- Automatic batching with ``SampleBatch``
- Type transformations via the lens system (``as_type()``)
- Export to parquet format

## Parameters {.doc-section .doc-section-parameters}

| Name   | Type   | Description                                                            | Default    |
|--------|--------|------------------------------------------------------------------------|------------|
| ST     |        | The sample type for this dataset, must derive from ``PackableSample``. | _required_ |

## Attributes {.doc-section .doc-section-attributes}

| Name   | Type   | Description                                        |
|--------|--------|----------------------------------------------------|
| url    |        | WebDataset brace-notation URL for the tar file(s). |

## Examples {.doc-section .doc-section-examples}

```python
>>> ds = Dataset[MyData]("path/to/data-{000000..000009}.tar")
>>> for sample in ds.ordered(batch_size=32):
...     # sample is SampleBatch[MyData] with batch_size samples
...     embeddings = sample.embeddings  # shape: (32, ...)
...
>>> # Transform to a different view
>>> ds_view = ds.as_type(MyDataView)
```

## Note {.doc-section .doc-section-note}

This class uses Python's ``__orig_class__`` mechanism to extract the
type parameter at runtime. Instances must be created using the
subscripted syntax ``Dataset[MyType](url)`` rather than calling the
constructor directly with an unsubscripted class.

## Methods

| Name | Description |
| --- | --- |
| [as_type](#atdata.Dataset.as_type) | View this dataset through a different sample type via a registered lens. |
| [describe](#atdata.Dataset.describe) | Summary statistics: sample_type, fields, num_shards, shards, url, metadata. |
| [filter](#atdata.Dataset.filter) | Return a new dataset that yields only samples matching *predicate*. |
| [get](#atdata.Dataset.get) | Retrieve a single sample by its ``__key__``. |
| [head](#atdata.Dataset.head) | Return the first *n* samples from the dataset. |
| [list_shards](#atdata.Dataset.list_shards) | Return all shard paths/URLs as a list. |
| [map](#atdata.Dataset.map) | Return a new dataset that applies *fn* to each sample during iteration. |
| [ordered](#atdata.Dataset.ordered) | Iterate over the dataset in order. |
| [process_shards](#atdata.Dataset.process_shards) | Process each shard independently, collecting per-shard results. |
| [query](#atdata.Dataset.query) | Query this dataset using per-shard manifest metadata. |
| [select](#atdata.Dataset.select) | Return samples at the given integer indices. |
| [shuffled](#atdata.Dataset.shuffled) | Iterate over the dataset in random order. |
| [to_dict](#atdata.Dataset.to_dict) | Materialize the dataset as a column-oriented dictionary. |
| [to_pandas](#atdata.Dataset.to_pandas) | Materialize the dataset (or first *limit* samples) as a DataFrame. |
| [to_parquet](#atdata.Dataset.to_parquet) | Export dataset to parquet file(s). |
| [wrap](#atdata.Dataset.wrap) | Deserialize a raw WDS sample dict into type ``ST``. |
| [wrap_batch](#atdata.Dataset.wrap_batch) | Deserialize a raw WDS batch dict into ``SampleBatch[ST]``. |

### as_type { #atdata.Dataset.as_type }

```python
Dataset.as_type(other)
```

View this dataset through a different sample type via a registered lens.

#### Raises {.doc-section .doc-section-raises}

| Name   | Type                       | Description                                             |
|--------|----------------------------|---------------------------------------------------------|
|        | [ValueError](`ValueError`) | If no lens exists between the current and target types. |

### describe { #atdata.Dataset.describe }

```python
Dataset.describe()
```

Summary statistics: sample_type, fields, num_shards, shards, url, metadata.

### filter { #atdata.Dataset.filter }

```python
Dataset.filter(predicate)
```

Return a new dataset that yields only samples matching *predicate*.

The filter is applied lazily during iteration — no data is copied.

#### Parameters {.doc-section .doc-section-parameters}

| Name      | Type                                                                           | Description                                                                                | Default    |
|-----------|--------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------|------------|
| predicate | [Callable](`typing.Callable`)\[\[[ST](`atdata.dataset.ST`)\], [bool](`bool`)\] | A function that takes a sample and returns ``True`` to keep it or ``False`` to discard it. | _required_ |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                                                             | Description                                         |
|--------|------------------------------------------------------------------|-----------------------------------------------------|
|        | [Dataset](`atdata.dataset.Dataset`)\[[ST](`atdata.dataset.ST`)\] | A new ``Dataset`` whose iterators apply the filter. |

#### Examples {.doc-section .doc-section-examples}

```python
>>> long_names = ds.filter(lambda s: len(s.name) > 10)
>>> for sample in long_names:
...     assert len(sample.name) > 10
```

### get { #atdata.Dataset.get }

```python
Dataset.get(key)
```

Retrieve a single sample by its ``__key__``.

Scans shards sequentially until a sample with a matching key is found.
This is O(n) for streaming datasets.

#### Parameters {.doc-section .doc-section-parameters}

| Name   | Type         | Description                                      | Default    |
|--------|--------------|--------------------------------------------------|------------|
| key    | [str](`str`) | The WebDataset ``__key__`` string to search for. | _required_ |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                      | Description          |
|--------|---------------------------|----------------------|
|        | [ST](`atdata.dataset.ST`) | The matching sample. |

#### Raises {.doc-section .doc-section-raises}

| Name   | Type                                                  | Description                             |
|--------|-------------------------------------------------------|-----------------------------------------|
|        | [SampleKeyError](`atdata._exceptions.SampleKeyError`) | If no sample with the given key exists. |

#### Examples {.doc-section .doc-section-examples}

```python
>>> sample = ds.get("00000001-0001-1000-8000-010000000000")
```

### head { #atdata.Dataset.head }

```python
Dataset.head(n=5)
```

Return the first *n* samples from the dataset.

#### Parameters {.doc-section .doc-section-parameters}

| Name   | Type         | Description                              | Default   |
|--------|--------------|------------------------------------------|-----------|
| n      | [int](`int`) | Number of samples to return. Default: 5. | `5`       |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                                        | Description                               |
|--------|---------------------------------------------|-------------------------------------------|
|        | [list](`list`)\[[ST](`atdata.dataset.ST`)\] | List of up to *n* samples in shard order. |

#### Examples {.doc-section .doc-section-examples}

```python
>>> samples = ds.head(3)
>>> len(samples)
3
```

### list_shards { #atdata.Dataset.list_shards }

```python
Dataset.list_shards()
```

Return all shard paths/URLs as a list.

### map { #atdata.Dataset.map }

```python
Dataset.map(fn)
```

Return a new dataset that applies *fn* to each sample during iteration.

The mapping is applied lazily during iteration — no data is copied.

#### Parameters {.doc-section .doc-section-parameters}

| Name   | Type                                                                                | Description                                                                    | Default    |
|--------|-------------------------------------------------------------------------------------|--------------------------------------------------------------------------------|------------|
| fn     | [Callable](`typing.Callable`)\[\[[ST](`atdata.dataset.ST`)\], [Any](`typing.Any`)\] | A function that takes a sample of type ``ST`` and returns a transformed value. | _required_ |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                                | Description                                          |
|--------|-------------------------------------|------------------------------------------------------|
|        | [Dataset](`atdata.dataset.Dataset`) | A new ``Dataset`` whose iterators apply the mapping. |

#### Examples {.doc-section .doc-section-examples}

```python
>>> names = ds.map(lambda s: s.name)
>>> for name in names:
...     print(name)
```

### ordered { #atdata.Dataset.ordered }

```python
Dataset.ordered(batch_size=None)
```

Iterate over the dataset in order.

#### Parameters {.doc-section .doc-section-parameters}

| Name       | Type                 | Description                                                                                                                       | Default   |
|------------|----------------------|-----------------------------------------------------------------------------------------------------------------------------------|-----------|
| batch_size | [int](`int`) \| None | The size of iterated batches. Default: None (unbatched). If ``None``, iterates over one sample at a time with no batch dimension. | `None`    |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                                                                                                                                                                    | Description                                                       |
|--------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------|
|        | [Iterable](`typing.Iterable`)\[[ST](`atdata.dataset.ST`)\] \| [Iterable](`typing.Iterable`)\[[SampleBatch](`atdata.dataset.SampleBatch`)\[[ST](`atdata.dataset.ST`)\]\] | A data pipeline that iterates over the dataset in its original    |
|        | [Iterable](`typing.Iterable`)\[[ST](`atdata.dataset.ST`)\] \| [Iterable](`typing.Iterable`)\[[SampleBatch](`atdata.dataset.SampleBatch`)\[[ST](`atdata.dataset.ST`)\]\] | sample order. When ``batch_size`` is ``None``, yields individual  |
|        | [Iterable](`typing.Iterable`)\[[ST](`atdata.dataset.ST`)\] \| [Iterable](`typing.Iterable`)\[[SampleBatch](`atdata.dataset.SampleBatch`)\[[ST](`atdata.dataset.ST`)\]\] | samples of type ``ST``. When ``batch_size`` is an integer, yields |
|        | [Iterable](`typing.Iterable`)\[[ST](`atdata.dataset.ST`)\] \| [Iterable](`typing.Iterable`)\[[SampleBatch](`atdata.dataset.SampleBatch`)\[[ST](`atdata.dataset.ST`)\]\] | ``SampleBatch[ST]`` instances containing that many samples.       |

#### Examples {.doc-section .doc-section-examples}

```python
>>> for sample in ds.ordered():
...     process(sample)  # sample is ST
>>> for batch in ds.ordered(batch_size=32):
...     process(batch)  # batch is SampleBatch[ST]
```

### process_shards { #atdata.Dataset.process_shards }

```python
Dataset.process_shards(fn, *, shards=None)
```

Process each shard independently, collecting per-shard results.

Unlike :meth:`map` (which is lazy and per-sample), this method eagerly
processes each shard in turn, calling *fn* with the full list of samples
from that shard. If some shards fail, raises
:class:`~atdata._exceptions.PartialFailureError` containing both the
successful results and the per-shard errors.

#### Parameters {.doc-section .doc-section-parameters}

| Name   | Type                                                                                                  | Description                                                                                                                                                                          | Default    |
|--------|-------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------|
| fn     | [Callable](`typing.Callable`)\[\[[list](`list`)\[[ST](`atdata.dataset.ST`)\]\], [Any](`typing.Any`)\] | Function receiving a list of samples from one shard and returning an arbitrary result.                                                                                               | _required_ |
| shards | [list](`list`)\[[str](`str`)\] \| None                                                                | Optional list of shard identifiers to process. If ``None``, processes all shards in the dataset. Useful for retrying only the failed shards from a previous ``PartialFailureError``. | `None`     |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                                                | Description                                                          |
|--------|-----------------------------------------------------|----------------------------------------------------------------------|
|        | [dict](`dict`)\[[str](`str`), [Any](`typing.Any`)\] | Dict mapping shard identifier to *fn*'s return value for each shard. |

#### Raises {.doc-section .doc-section-raises}

| Name   | Type                                                            | Description                                                                                                                                           |
|--------|-----------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------|
|        | [PartialFailureError](`atdata._exceptions.PartialFailureError`) | If at least one shard fails. The exception carries ``.succeeded_shards``, ``.failed_shards``, ``.errors``, and ``.results`` for inspection and retry. |

#### Examples {.doc-section .doc-section-examples}

```python
>>> results = ds.process_shards(lambda samples: len(samples))
>>> # On partial failure, retry just the failed shards:
>>> try:
...     results = ds.process_shards(expensive_fn)
... except PartialFailureError as e:
...     retry = ds.process_shards(expensive_fn, shards=e.failed_shards)
```

### query { #atdata.Dataset.query }

```python
Dataset.query(where)
```

Query this dataset using per-shard manifest metadata.

Requires manifests to have been generated during shard writing.
Discovers manifest files alongside the tar shards, loads them,
and executes a two-phase query (shard-level aggregate pruning,
then sample-level parquet filtering).

#### Parameters {.doc-section .doc-section-parameters}

| Name   | Type                                                                                                                          | Description                                                                                                                  | Default    |
|--------|-------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|------------|
| where  | [Callable](`typing.Callable`)\[\[[pd](`pandas`).[DataFrame](`pandas.DataFrame`)\], [pd](`pandas`).[Series](`pandas.Series`)\] | Predicate function that receives a pandas DataFrame of manifest fields and returns a boolean Series selecting matching rows. | _required_ |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                                                                        | Description                                      |
|--------|-----------------------------------------------------------------------------|--------------------------------------------------|
|        | [list](`list`)\[[SampleLocation](`atdata.manifest._query.SampleLocation`)\] | List of ``SampleLocation`` for matching samples. |

#### Raises {.doc-section .doc-section-raises}

| Name   | Type                                     | Description                                      |
|--------|------------------------------------------|--------------------------------------------------|
|        | [FileNotFoundError](`FileNotFoundError`) | If no manifest files are found alongside shards. |

#### Examples {.doc-section .doc-section-examples}

```python
>>> locs = ds.query(where=lambda df: df["confidence"] > 0.9)
>>> len(locs)
42
```

### select { #atdata.Dataset.select }

```python
Dataset.select(indices)
```

Return samples at the given integer indices.

Iterates through the dataset in order and collects samples whose
positional index matches. This is O(n) for streaming datasets.

#### Parameters {.doc-section .doc-section-parameters}

| Name    | Type                                          | Description                               | Default    |
|---------|-----------------------------------------------|-------------------------------------------|------------|
| indices | [Sequence](`typing.Sequence`)\[[int](`int`)\] | Sequence of zero-based indices to select. | _required_ |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                                        | Description                                                 |
|--------|---------------------------------------------|-------------------------------------------------------------|
|        | [list](`list`)\[[ST](`atdata.dataset.ST`)\] | List of samples at the requested positions, in index order. |

#### Examples {.doc-section .doc-section-examples}

```python
>>> samples = ds.select([0, 5, 10])
>>> len(samples)
3
```

### shuffled { #atdata.Dataset.shuffled }

```python
Dataset.shuffled(buffer_shards=100, buffer_samples=10000, batch_size=None)
```

Iterate over the dataset in random order.

#### Parameters {.doc-section .doc-section-parameters}

| Name           | Type                 | Description                                                                                                                       | Default   |
|----------------|----------------------|-----------------------------------------------------------------------------------------------------------------------------------|-----------|
| buffer_shards  | [int](`int`)         | Number of shards to buffer for shuffling at the shard level. Larger values increase randomness but use more memory. Default: 100. | `100`     |
| buffer_samples | [int](`int`)         | Number of samples to buffer for shuffling within shards. Larger values increase randomness but use more memory. Default: 10,000.  | `10000`   |
| batch_size     | [int](`int`) \| None | The size of iterated batches. Default: None (unbatched). If ``None``, iterates over one sample at a time with no batch dimension. | `None`    |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                                                                                                                                                                    | Description                                                           |
|--------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------|
|        | [Iterable](`typing.Iterable`)\[[ST](`atdata.dataset.ST`)\] \| [Iterable](`typing.Iterable`)\[[SampleBatch](`atdata.dataset.SampleBatch`)\[[ST](`atdata.dataset.ST`)\]\] | A data pipeline that iterates over the dataset in randomized order.   |
|        | [Iterable](`typing.Iterable`)\[[ST](`atdata.dataset.ST`)\] \| [Iterable](`typing.Iterable`)\[[SampleBatch](`atdata.dataset.SampleBatch`)\[[ST](`atdata.dataset.ST`)\]\] | When ``batch_size`` is ``None``, yields individual samples of type    |
|        | [Iterable](`typing.Iterable`)\[[ST](`atdata.dataset.ST`)\] \| [Iterable](`typing.Iterable`)\[[SampleBatch](`atdata.dataset.SampleBatch`)\[[ST](`atdata.dataset.ST`)\]\] | ``ST``. When ``batch_size`` is an integer, yields ``SampleBatch[ST]`` |
|        | [Iterable](`typing.Iterable`)\[[ST](`atdata.dataset.ST`)\] \| [Iterable](`typing.Iterable`)\[[SampleBatch](`atdata.dataset.SampleBatch`)\[[ST](`atdata.dataset.ST`)\]\] | instances containing that many samples.                               |

#### Examples {.doc-section .doc-section-examples}

```python
>>> for sample in ds.shuffled():
...     process(sample)  # sample is ST
>>> for batch in ds.shuffled(batch_size=32):
...     process(batch)  # batch is SampleBatch[ST]
```

### to_dict { #atdata.Dataset.to_dict }

```python
Dataset.to_dict(limit=None)
```

Materialize the dataset as a column-oriented dictionary.

#### Parameters {.doc-section .doc-section-parameters}

| Name   | Type                 | Description                                               | Default   |
|--------|----------------------|-----------------------------------------------------------|-----------|
| limit  | [int](`int`) \| None | Maximum number of samples to include. ``None`` means all. | `None`    |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                                                                  | Description                                                  |
|--------|-----------------------------------------------------------------------|--------------------------------------------------------------|
|        | [dict](`dict`)\[[str](`str`), [list](`list`)\[[Any](`typing.Any`)\]\] | Dictionary mapping field names to lists of values (one entry |
|        | [dict](`dict`)\[[str](`str`), [list](`list`)\[[Any](`typing.Any`)\]\] | per sample).                                                 |

#### Warning {.doc-section .doc-section-warning}

With ``limit=None`` this loads the entire dataset into memory.

#### Examples {.doc-section .doc-section-examples}

```python
>>> d = ds.to_dict(limit=10)
>>> d.keys()
dict_keys(['name', 'embedding'])
>>> len(d['name'])
10
```

### to_pandas { #atdata.Dataset.to_pandas }

```python
Dataset.to_pandas(limit=None)
```

Materialize the dataset (or first *limit* samples) as a DataFrame.

#### Parameters {.doc-section .doc-section-parameters}

| Name   | Type                 | Description                                                                                                       | Default   |
|--------|----------------------|-------------------------------------------------------------------------------------------------------------------|-----------|
| limit  | [int](`int`) \| None | Maximum number of samples to include. ``None`` means all samples (may use significant memory for large datasets). | `None`    |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                                           | Description                                                     |
|--------|------------------------------------------------|-----------------------------------------------------------------|
|        | [pd](`pandas`).[DataFrame](`pandas.DataFrame`) | A pandas DataFrame with one row per sample and columns matching |
|        | [pd](`pandas`).[DataFrame](`pandas.DataFrame`) | the sample fields.                                              |

#### Warning {.doc-section .doc-section-warning}

With ``limit=None`` this loads the entire dataset into memory.

#### Examples {.doc-section .doc-section-examples}

```python
>>> df = ds.to_pandas(limit=100)
>>> df.columns.tolist()
['name', 'embedding']
```

### to_parquet { #atdata.Dataset.to_parquet }

```python
Dataset.to_parquet(path, sample_map=None, maxcount=None, **kwargs)
```

Export dataset to parquet file(s).

#### Parameters {.doc-section .doc-section-parameters}

| Name       | Type                                                                                 | Description                                                                                          | Default    |
|------------|--------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|------------|
| path       | [Pathlike](`atdata.dataset.Pathlike`)                                                | Output path. With *maxcount*, files are named ``{stem}-{segment:06d}.parquet``.                      | _required_ |
| sample_map | [Optional](`typing.Optional`)\[[SampleExportMap](`atdata.dataset.SampleExportMap`)\] | Convert sample to dict. Defaults to ``dataclasses.asdict``.                                          | `None`     |
| maxcount   | [Optional](`typing.Optional`)\[[int](`int`)\]                                        | Split into files of at most this many samples. Without it, the entire dataset is loaded into memory. | `None`     |
| **kwargs   |                                                                                      | Passed to ``pandas.DataFrame.to_parquet()``.                                                         | `{}`       |

#### Examples {.doc-section .doc-section-examples}

```python
>>> ds.to_parquet("output.parquet", maxcount=50000)
```

### wrap { #atdata.Dataset.wrap }

```python
Dataset.wrap(sample)
```

Deserialize a raw WDS sample dict into type ``ST``.

### wrap_batch { #atdata.Dataset.wrap_batch }

```python
Dataset.wrap_batch(batch)
```

Deserialize a raw WDS batch dict into ``SampleBatch[ST]``.
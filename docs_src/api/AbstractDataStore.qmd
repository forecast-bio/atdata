# AbstractDataStore { #atdata.AbstractDataStore }

```python
AbstractDataStore()
```

Protocol for data storage backends (S3, local disk, PDS blobs).

Separates index (metadata) from data store (shard files), enabling
flexible deployment combinations.

## Examples {.doc-section .doc-section-examples}

```python
>>> store = S3DataStore(credentials, bucket="my-bucket")
>>> urls = store.write_shards(dataset, prefix="training/v1")
```

## Methods

| Name | Description |
| --- | --- |
| [read_url](#atdata.AbstractDataStore.read_url) | Resolve a storage URL for reading (e.g., sign S3 URLs). |
| [write_shards](#atdata.AbstractDataStore.write_shards) | Write dataset shards to storage. |

### read_url { #atdata.AbstractDataStore.read_url }

```python
AbstractDataStore.read_url(url)
```

Resolve a storage URL for reading (e.g., sign S3 URLs).

### write_shards { #atdata.AbstractDataStore.write_shards }

```python
AbstractDataStore.write_shards(ds, *, prefix, **kwargs)
```

Write dataset shards to storage.

#### Parameters {.doc-section .doc-section-parameters}

| Name     | Type                                | Description                                                 | Default    |
|----------|-------------------------------------|-------------------------------------------------------------|------------|
| ds       | [Dataset](`atdata.dataset.Dataset`) | The Dataset to write.                                       | _required_ |
| prefix   | [str](`str`)                        | Path prefix (e.g., ``'datasets/mnist/v1'``).                | _required_ |
| **kwargs |                                     | Backend-specific options (``maxcount``, ``maxsize``, etc.). | `{}`       |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                           | Description                                           |
|--------|--------------------------------|-------------------------------------------------------|
|        | [list](`list`)\[[str](`str`)\] | List of shard URLs suitable for ``atdata.Dataset()``. |
---
title: "atdata"
subtitle: "A loose federation of distributed, typed datasets built on WebDataset"
---

::: {.hero}
# atdata

A loose federation of distributed, typed datasets built on WebDataset.

[Get Started](tutorials/quickstart.qmd){.btn .btn-primary .btn-lg}
[View on GitHub](https://github.com/your-org/atdata){.btn .btn-outline-secondary .btn-lg}
:::

## What is atdata?

atdata provides a typed dataset abstraction for machine learning workflows with:

::: {.feature-cards}

::: {.feature-card}
### Typed Samples
Define dataclass-based sample types with automatic msgpack serialization.
:::

::: {.feature-card}
### NDArray Handling
Transparent numpy array conversion with efficient byte serialization.
:::

::: {.feature-card}
### Lens Transformations
View datasets through different schemas without duplicating data.
:::

::: {.feature-card}
### Batch Aggregation
Automatic numpy stacking for NDArray fields during iteration.
:::

::: {.feature-card}
### WebDataset Integration
Efficient large-scale storage with streaming tar file support.
:::

::: {.feature-card}
### ATProto Federation
Publish and discover datasets on the decentralized AT Protocol network.
:::

:::

## Installation

::: {.install-box}
```bash
pip install atdata

# With ATProto support
pip install atdata[atmosphere]
```
:::

## Quick Example

### Define a Sample Type

```{python}
#| eval: false
import numpy as np
from numpy.typing import NDArray
import atdata

@atdata.packable
class ImageSample:
    image: NDArray
    label: str
    confidence: float
```

### Create and Write Samples

```{python}
#| eval: false
import webdataset as wds

samples = [
    ImageSample(
        image=np.random.rand(224, 224, 3).astype(np.float32),
        label="cat",
        confidence=0.95,
    )
    for _ in range(100)
]

with wds.writer.TarWriter("data-000000.tar") as sink:
    for i, sample in enumerate(samples):
        sink.write({**sample.as_wds, "__key__": f"sample_{i:06d}"})
```

### Load and Iterate

```{python}
#| eval: false
dataset = atdata.Dataset[ImageSample]("data-000000.tar")

# Iterate with batching
for batch in dataset.shuffled(batch_size=32):
    images = batch.image      # numpy array (32, 224, 224, 3)
    labels = batch.label      # list of 32 strings
    confs = batch.confidence  # list of 32 floats
```

## HuggingFace-Style Loading

```{python}
#| eval: false
# Load from local path
ds = atdata.load_dataset("path/to/data-{000000..000009}.tar", split="train")

# Load with split detection
ds_dict = atdata.load_dataset("path/to/data/")
train_ds = ds_dict["train"]
test_ds = ds_dict["test"]
```

## Local Storage with Redis + S3

```{python}
#| eval: false
from atdata.local import LocalIndex, Repo

# Set up local index
index = LocalIndex()  # Connects to Redis

# Create repo with S3 storage
repo = Repo(
    s3_credentials={"AWS_ENDPOINT": "http://localhost:9000", ...},
    bucket="my-bucket",
    index=index,
)

# Insert dataset
entry = repo.insert(samples, name="my-dataset")
print(f"Stored at: {entry.data_urls}")
```

## Publish to ATProto Federation

```{python}
#| eval: false
from atdata.atmosphere import AtmosphereClient
from atdata.promote import promote_to_atmosphere

# Authenticate
client = AtmosphereClient()
client.login("handle.bsky.social", "app-password")

# Promote local dataset to federation
entry = index.get_dataset("my-dataset")
at_uri = promote_to_atmosphere(entry, index, client)
print(f"Published at: {at_uri}")
```

## Next Steps

- **[Quick Start Tutorial](tutorials/quickstart.qmd)** - Get up and running in 5 minutes
- **[Packable Samples](reference/packable-samples.qmd)** - Learn about typed sample definitions
- **[Datasets](reference/datasets.qmd)** - Master dataset iteration and batching
- **[Atmosphere](reference/atmosphere.qmd)** - Publish to the ATProto federation

---
title: "Promotion Workflow"
description: "Migrating datasets from local storage to ATProto"
---

The promotion workflow migrates datasets from local storage to the ATProto atmosphere network, enabling federation and discovery.

## Overview

Promotion handles:

- **Schema deduplication**: Avoids publishing duplicate schemas
- **Data URL preservation**: Keeps existing URLs or copies to new storage
- **Metadata transfer**: Preserves tags, descriptions, and custom metadata

## Using Index.promote_entry()

The recommended approach uses the unified `Index` with an attached atmosphere client:

```{python}
#| eval: false
import atdata
from atdata.atmosphere import Atmosphere

# Set up index with both local and atmosphere backends
client = Atmosphere.login("handle.bsky.social", "app-password")
index = atdata.Index(
    atmosphere=client,
    data_store=atdata.LocalDiskStore(),
)

# Promote a local entry to the atmosphere by name
at_uri = index.promote_entry("my-dataset")
print(f"Published: {at_uri}")
```

## With Metadata

```{python}
#| eval: false
at_uri = index.promote_entry(
    "my-dataset",
    name="my-dataset-v2",           # Override name
    description="Training images",  # Add description
    tags=["images", "training"],    # Add discovery tags
    license="MIT",                  # Specify license
)
```

## Schema Deduplication

The promotion workflow automatically checks for existing schemas:

```{python}
#| eval: false
# First promotion: publishes schema
uri1 = index.promote_entry("batch-1")

# Second promotion with same schema type + version: reuses existing schema
uri2 = index.promote_entry("batch-2")
```

Schema matching is based on:

- `{module}.{class_name}` (e.g., `mymodule.ImageSample`)
- Version string (e.g., `1.0.0`)

## Data Storage Options

::: {.panel-tabset}

## Keep Existing URLs (Default)

By default, promotion keeps the original data URLs:

```{python}
#| eval: false
# Data stays in original storage location
at_uri = index.promote_entry("my-dataset")
```

- Data stays in original S3 location
- Dataset record points to existing URLs
- Fastest option, no data copying
- Requires original storage to remain accessible

## Copy to New Storage

To copy data to a different storage location, use `promote_dataset()` with
a Dataset loaded from the entry's URLs:

```{python}
#| eval: false
# Load the dataset and promote directly
entry = index.get_entry_by_name("my-dataset")
ds = atdata.Dataset[MySample](entry.data_urls[0])

at_uri = index.promote_dataset(
    ds,
    name="my-dataset",
    description="Training images",
)
```

- Data is copied to new storage
- Good for moving from private to public storage
- Original storage can be retired

:::

## Complete Workflow

```{python}
#| eval: false
import numpy as np
from numpy.typing import NDArray
import atdata
from atdata.atmosphere import Atmosphere

# 1. Define sample type
@atdata.packable
class FeatureSample:
    features: NDArray
    label: int

# 2. Create samples
samples = [
    FeatureSample(
        features=np.random.randn(128).astype(np.float32),
        label=i % 10,
    )
    for i in range(1000)
]

# 3. Write to local index
index = atdata.Index(data_store=atdata.LocalDiskStore())
entry = index.write_samples(samples, name="feature-vectors-v1", maxcount=500)

# 4. Attach atmosphere client and promote
client = Atmosphere.login("myhandle.bsky.social", "app-password")
index = atdata.Index(atmosphere=client, data_store=atdata.LocalDiskStore())

at_uri = index.promote_entry(
    "feature-vectors-v1",
    description="Feature vectors for classification",
    tags=["features", "embeddings"],
    license="MIT",
)
print(f"Dataset published: {at_uri}")

# 5. Verify on atmosphere
atm_entry = index.get_dataset(at_uri)
print(f"Name: {atm_entry.name}")
print(f"Schema: {atm_entry.schema_ref}")
print(f"URLs: {atm_entry.data_urls}")
```

## Error Handling

```{python}
#| eval: false
try:
    at_uri = index.promote_entry("my-dataset")
except KeyError as e:
    # Entry or schema not found in index
    print(f"Not found: {e}")
except ValueError as e:
    # Entry has no data URLs or atmosphere not available
    print(f"Invalid state: {e}")
```

## Requirements

Before promotion:

1. Dataset must be in the index (via `index.write_samples()` or `index.insert_dataset()`)
2. Schema must be published (automatic when using `index.write_samples()`)
3. `Atmosphere` client must be authenticated and attached to the index

## Related

- [Local Storage](local-storage.qmd) - Setting up local datasets
- [Atmosphere](atmosphere.qmd) - ATProto integration
- [Protocols](protocols.qmd) - AbstractIndex and AbstractDataStore

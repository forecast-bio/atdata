---
title: "Local Storage"
description: "Index providers and data stores for dataset management"
---

The local storage layer provides pluggable backends for storing and managing datasets. By default, `Index()` uses SQLite with no external dependencies. For team deployments, swap in Redis or PostgreSQL as the index provider and S3-compatible storage for data.

## Overview

Local storage has two components:

| Component | Purpose | Options |
|-----------|---------|---------|
| **Index Provider** | Metadata queries, schema registry, dataset discovery | SQLite (default), Redis, PostgreSQL |
| **Data Store** | Persistent shard storage | `LocalDiskStore` (filesystem), `S3DataStore` (S3-compatible) |

This separation means metadata operations (listing datasets, resolving schemas) are fast and don't touch large data files.

## Index

The `Index` class is the unified entry point:

```{python}
#| eval: false
import atdata

# Zero-config: SQLite in memory (default)
index = atdata.Index()

# Persist SQLite to disk
index = atdata.Index(path="~/.atdata/index.db")

# With a data store for shard storage
index = atdata.Index(
    data_store=atdata.LocalDiskStore(root="~/.atdata/data/"),
)
```

### Alternative Providers

For team-scale deployments:

```{python}
#| eval: false
from redis import Redis

# Redis provider
index = atdata.Index(redis=Redis(host="localhost", port=6379))

# PostgreSQL provider
index = atdata.Index(provider="postgres", dsn="postgresql://user:pass@host/db")
```

### Writing Datasets

`index.write_samples()` handles sharding, storage, schema persistence, and indexing in one call:

```{python}
#| eval: false
import numpy as np
from numpy.typing import NDArray

@atdata.packable
class ImageSample:
    image: NDArray
    label: str

samples = [
    ImageSample(
        image=np.random.rand(224, 224, 3).astype(np.float32),
        label=f"sample_{i}",
    )
    for i in range(1000)
]

entry = index.write_samples(
    samples,
    name="my-dataset",
    maxcount=500,
    metadata={"description": "Training images"},
)

print(entry.cid)        # Content identifier
print(entry.name)       # "my-dataset"
print(entry.data_urls)  # Shard URLs
```

### Listing and Retrieving

```{python}
#| eval: false
# Lazy iteration (property)
for entry in index.datasets:
    print(f"{entry.name}: {entry.cid}")

# Eager list (method)
all_entries = index.list_datasets()

# Get by name
entry = index.get_entry_by_name("my-dataset")

# Get by CID
entry = index.get_entry("bafyrei...")
```

## LocalDatasetEntry

Index entries provide content-addressable identification:

```{python}
#| eval: false
entry = index.get_entry_by_name("my-dataset")

# Core properties (IndexEntry protocol)
entry.name        # Human-readable name
entry.schema_ref  # Schema reference
entry.data_urls   # WebDataset URLs
entry.metadata    # Arbitrary metadata dict or None

# Content addressing
entry.cid         # ATProto-compatible CID (content identifier)
```

::: {.callout-tip}
The CID is generated from the entry's content (schema_ref + data_urls), ensuring identical data produces identical CIDs whether stored locally or in the atmosphere.
:::

## Data Stores

Data stores implement the `AbstractDataStore` protocol for persistent shard storage.

### LocalDiskStore

The simplest option — stores shards on the local filesystem:

```{python}
#| eval: false
store = atdata.LocalDiskStore(root="~/.atdata/data/")

# Or accept the default root (~/.atdata/data/)
store = atdata.LocalDiskStore()
```

### S3DataStore

For team-scale storage with S3-compatible backends (AWS S3, MinIO, Cloudflare R2):

```{python}
#| eval: false
from atdata.stores import S3DataStore

store = S3DataStore(
    credentials={
        "AWS_ENDPOINT": "http://localhost:9000",
        "AWS_ACCESS_KEY_ID": "minioadmin",
        "AWS_SECRET_ACCESS_KEY": "minioadmin",
    },
    bucket="my-bucket",
)

index = atdata.Index(data_store=store)
```

## Schema Storage

Schemas are persisted automatically by `index.write_samples()`. You can also manage them explicitly:

```{python}
#| eval: false
# Publish a schema
schema_ref = index.publish_schema(
    ImageSample,
    version="1.0.0",
    description="Image with label annotation",
)

# Retrieve schema record
schema = index.get_schema(schema_ref)

# List all schemas
for schema in index.list_schemas():
    print(f"{schema['name']}@{schema['version']}")

# Reconstruct sample type from schema
SampleType = index.get_schema_type(schema_ref)
dataset = atdata.Dataset[SampleType](entry.data_urls[0])
```

## Labels

Labels provide human-readable, versioned aliases for dataset entries:

```{python}
#| eval: false
# Create a label
index.label("training-latest", entry.cid)

# With version
index.label("training", entry.cid, version="2.0.0")

# Resolve label to entry
entry = index.get_label("training-latest")

# List all labels
for name, cid, version in index.list_labels():
    print(f"  {name} v{version or 'latest'} -> {cid[:12]}...")
```

## Complete Workflow

```{python}
#| eval: false
import numpy as np
from numpy.typing import NDArray
import atdata

# 1. Define sample type
@atdata.packable
class TrainingSample:
    features: NDArray
    label: int
    source: str

# 2. Create samples
samples = [
    TrainingSample(
        features=np.random.randn(128).astype(np.float32),
        label=i % 10,
        source="synthetic",
    )
    for i in range(10000)
]

# 3. Set up index with data store
store = atdata.LocalDiskStore(root="~/.atdata/data/")
index = atdata.Index(data_store=store)

# 4. Write through index (schema persisted automatically)
entry = index.write_samples(samples, name="training-v1", maxcount=5000)

# 5. Load by name — schema auto-resolved
atdata.set_default_index(index)
ds = atdata.load_dataset("@local/training-v1", split="train")

for batch in ds.ordered(batch_size=32):
    print(batch.features.shape)  # (32, 128)
    break
```

## Related

- [Datasets](datasets.qmd) - Dataset iteration and batching
- [Protocols](protocols.qmd) - AbstractIndex and IndexEntry interfaces
- [Promotion](promotion.qmd) - Promoting local datasets to ATProto
- [Atmosphere](atmosphere.qmd) - ATProto federation

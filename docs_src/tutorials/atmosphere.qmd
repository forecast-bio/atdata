---
title: "Atmosphere Publishing"
description: "Publish and discover datasets on the ATProto network"
---

This tutorial demonstrates how to use the atmosphere module to publish datasets to the AT Protocol network, enabling federated discovery and sharing.

## Prerequisites

- `pip install atdata[atmosphere]`
- A Bluesky account with an [app-specific password](https://bsky.app/settings/app-passwords)

::: {.callout-warning}
Always use an app-specific password, not your main Bluesky password.
:::

## Setup

```{python}
#| eval: false
import numpy as np
from numpy.typing import NDArray
import atdata
from atdata.atmosphere import (
    AtmosphereClient,
    AtmosphereIndex,
    PDSBlobStore,
    SchemaPublisher,
    SchemaLoader,
    DatasetPublisher,
    DatasetLoader,
    AtUri,
)
from atdata import BlobSource
import webdataset as wds
```

## Define Sample Types

```{python}
#| eval: false
@atdata.packable
class ImageSample:
    """A sample containing image data with metadata."""
    image: NDArray
    label: str
    confidence: float

@atdata.packable
class TextEmbeddingSample:
    """A sample containing text with embedding vectors."""
    text: str
    embedding: NDArray
    source: str
```

## Type Introspection

See what information is available from a PackableSample type:

```{python}
#| eval: false
from dataclasses import fields, is_dataclass

print(f"Sample type: {ImageSample.__name__}")
print(f"Is dataclass: {is_dataclass(ImageSample)}")

print("\nFields:")
for field in fields(ImageSample):
    print(f"  - {field.name}: {field.type}")

# Create and serialize a sample
sample = ImageSample(
    image=np.random.rand(224, 224, 3).astype(np.float32),
    label="cat",
    confidence=0.95,
)

packed = sample.packed
print(f"\nSerialized size: {len(packed):,} bytes")

# Round-trip
restored = ImageSample.from_bytes(packed)
print(f"Round-trip successful: {np.allclose(sample.image, restored.image)}")
```

## AT URI Parsing

ATProto records are identified by AT URIs:

```{python}
#| eval: false
uris = [
    "at://did:plc:abc123/ac.foundation.dataset.sampleSchema/xyz789",
    "at://alice.bsky.social/ac.foundation.dataset.record/my-dataset",
]

for uri_str in uris:
    print(f"\nParsing: {uri_str}")
    uri = AtUri.parse(uri_str)
    print(f"  Authority:  {uri.authority}")
    print(f"  Collection: {uri.collection}")
    print(f"  Rkey:       {uri.rkey}")
```

## Authentication

Connect to ATProto:

```{python}
#| eval: false
client = AtmosphereClient()
client.login("your.handle.social", "your-app-password")

print(f"Authenticated as: {client.handle}")
print(f"DID: {client.did}")
```

## Publish a Schema

```{python}
#| eval: false
schema_publisher = SchemaPublisher(client)
schema_uri = schema_publisher.publish(
    ImageSample,
    name="ImageSample",
    version="1.0.0",
    description="Demo: Image sample with label and confidence",
)
print(f"Schema URI: {schema_uri}")
```

## List Your Schemas

```{python}
#| eval: false
schema_loader = SchemaLoader(client)
schemas = schema_loader.list_all(limit=10)
print(f"Found {len(schemas)} schema(s)")

for schema in schemas:
    print(f"  - {schema.get('name', 'Unknown')}: v{schema.get('version', '?')}")
```

## Publish a Dataset

### With External URLs

```{python}
#| eval: false
dataset_publisher = DatasetPublisher(client)
dataset_uri = dataset_publisher.publish_with_urls(
    urls=["s3://example-bucket/demo-data-{000000..000009}.tar"],
    schema_uri=str(schema_uri),
    name="Demo Image Dataset",
    description="Example dataset demonstrating atmosphere publishing",
    tags=["demo", "images", "atdata"],
    license="MIT",
)
print(f"Dataset URI: {dataset_uri}")
```

### With PDS Blob Storage (Recommended)

For fully decentralized storage, use `PDSBlobStore` to store dataset shards directly as ATProto blobs in your PDS:

```{python}
#| eval: false
# Create store and index with blob storage
store = PDSBlobStore(client)
index = AtmosphereIndex(client, data_store=store)

# Define sample type
@atdata.packable
class FeatureSample:
    features: NDArray
    label: int

# Create dataset in memory or from existing tar
samples = [FeatureSample(features=np.random.randn(64).astype(np.float32), label=i % 10) for i in range(100)]

# Write to temporary tar
with wds.writer.TarWriter("temp.tar") as sink:
    for i, s in enumerate(samples):
        sink.write({**s.as_wds, "__key__": f"{i:06d}"})

dataset = atdata.Dataset[FeatureSample]("temp.tar")

# Publish - shards are uploaded as blobs automatically
schema_uri = index.publish_schema(FeatureSample, version="1.0.0")
entry = index.insert_dataset(
    dataset,
    name="blob-stored-features",
    schema_ref=schema_uri,
    description="Features stored as PDS blobs",
)

print(f"Dataset URI: {entry.uri}")
print(f"Blob URLs: {entry.data_urls}")  # at://did/blob/cid format
```

::: {.callout-tip}
## Reading Blob-Stored Datasets

Use `BlobSource` to stream directly from PDS blobs:

```{python}
#| eval: false
# Create source from the blob URLs
source = store.create_source(entry.data_urls)

# Or manually from blob references
source = BlobSource.from_refs([
    {"did": client.did, "cid": "bafyrei..."},
])

# Load and iterate
ds = atdata.Dataset[FeatureSample](source)
for batch in ds.ordered(batch_size=32):
    print(batch.features.shape)
```
:::

### With External URLs

For larger datasets or when using existing object storage:

```{python}
#| eval: false
dataset_publisher = DatasetPublisher(client)
dataset_uri = dataset_publisher.publish_with_urls(
    urls=["s3://example-bucket/demo-data-{000000..000009}.tar"],
    schema_uri=str(schema_uri),
    name="Demo Image Dataset",
    description="Example dataset demonstrating atmosphere publishing",
    tags=["demo", "images", "atdata"],
    license="MIT",
)
print(f"Dataset URI: {dataset_uri}")
```

## List and Load Datasets

```{python}
#| eval: false
dataset_loader = DatasetLoader(client)
datasets = dataset_loader.list_all(limit=10)
print(f"Found {len(datasets)} dataset(s)")

for ds in datasets:
    print(f"  - {ds.get('name', 'Unknown')}")
    print(f"    Schema: {ds.get('schemaRef', 'N/A')}")
    tags = ds.get('tags', [])
    if tags:
        print(f"    Tags: {', '.join(tags)}")
```

## Load a Dataset

```{python}
#| eval: false
# Check storage type
storage_type = dataset_loader.get_storage_type(str(blob_dataset_uri))
print(f"Storage type: {storage_type}")

if storage_type == "blobs":
    blob_urls = dataset_loader.get_blob_urls(str(blob_dataset_uri))
    print(f"Blob URLs: {len(blob_urls)} blob(s)")

# Load and iterate (works for both storage types)
ds = dataset_loader.to_dataset(str(blob_dataset_uri), DemoSample)
for batch in ds.ordered():
    print(f"Sample id={batch.id}, text={batch.text}")
```

## Complete Publishing Workflow

This example shows the recommended workflow using `PDSBlobStore` for fully decentralized storage:

```{python}
#| eval: false
# 1. Define and create samples
@atdata.packable
class FeatureSample:
    features: NDArray
    label: int
    source: str

samples = [
    FeatureSample(
        features=np.random.randn(128).astype(np.float32),
        label=i % 10,
        source="synthetic",
    )
    for i in range(1000)
]

# 2. Write to tar
with wds.writer.TarWriter("features.tar") as sink:
    for i, s in enumerate(samples):
        sink.write({**s.as_wds, "__key__": f"{i:06d}"})

# 3. Authenticate and create index with blob storage
client = AtmosphereClient()
client.login("myhandle.bsky.social", "app-password")

store = PDSBlobStore(client)
index = AtmosphereIndex(client, data_store=store)

# 4. Publish schema
schema_uri = index.publish_schema(
    FeatureSample,
    version="1.0.0",
    description="Feature vectors with labels",
)

# 5. Publish dataset (shards uploaded as blobs automatically)
dataset = atdata.Dataset[FeatureSample]("features.tar")
entry = index.insert_dataset(
    dataset,
    name="synthetic-features-v1",
    schema_ref=schema_uri,
    tags=["features", "synthetic"],
)

print(f"Published: {entry.uri}")
print(f"Data stored at: {entry.data_urls}")  # at://did/blob/cid URLs

# 6. Later: load from blobs
source = store.create_source(entry.data_urls)
ds = atdata.Dataset[FeatureSample](source)
for batch in ds.ordered(batch_size=32):
    print(f"Loaded batch with {len(batch.label)} samples")
    break
```

## Next Steps

- **[Promotion Workflow](promotion.qmd)** - Migrate from local storage to atmosphere
- **[Atmosphere Reference](../reference/atmosphere.qmd)** - Complete API reference
- **[Protocols](../reference/protocols.qmd)** - Abstract interfaces

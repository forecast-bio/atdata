---
title: "Promotion Workflow"
description: "Migrate datasets from local storage to ATProto"
---

This tutorial demonstrates the workflow for migrating datasets from local Redis/S3 storage to the federated ATProto atmosphere network.

## Overview

The promotion workflow moves datasets from local storage to the atmosphere:

```
LOCAL                           ATMOSPHERE
-----                           ----------
Redis Index                     ATProto PDS
S3 Storage            -->       (same S3 or new location)
local://schemas/...             at://did:plc:.../schema/...
```

Key features:

- **Schema deduplication**: Won't republish identical schemas
- **Flexible data handling**: Keep existing URLs or copy to new storage
- **Metadata preservation**: Local metadata carries over to atmosphere

## Setup

```{python}
#| eval: false
import numpy as np
from numpy.typing import NDArray
import atdata
from atdata.local import LocalIndex, Repo, LocalDatasetEntry
from atdata.atmosphere import AtmosphereClient
from atdata.promote import promote_to_atmosphere
import webdataset as wds
```

## Prepare a Local Dataset

First, set up a dataset in local storage:

```{python}
#| eval: false
# 1. Define sample type
@atdata.packable
class ExperimentSample:
    """A sample from a scientific experiment."""
    measurement: NDArray
    timestamp: float
    sensor_id: str

# 2. Create samples
samples = [
    ExperimentSample(
        measurement=np.random.randn(64).astype(np.float32),
        timestamp=float(i),
        sensor_id=f"sensor_{i % 4}",
    )
    for i in range(1000)
]

# 3. Write to tar
with wds.writer.TarWriter("experiment.tar") as sink:
    for i, s in enumerate(samples):
        sink.write({**s.as_wds, "__key__": f"{i:06d}"})

# 4. Store in local repo
repo = Repo(
    s3_credentials={
        "AWS_ENDPOINT": "http://localhost:9000",
        "AWS_ACCESS_KEY_ID": "minioadmin",
        "AWS_SECRET_ACCESS_KEY": "minioadmin",
    },
    hive_path="datasets-bucket/experiments",
)

dataset = atdata.Dataset[ExperimentSample]("experiment.tar")
local_entry, _ = repo.insert(dataset, name="experiment-2024-001")

# 5. Publish schema to local index
local_index = LocalIndex()
local_index.publish_schema(ExperimentSample, version="1.0.0")

print(f"Local entry name: {local_entry.name}")
print(f"Local entry CID: {local_entry.cid}")
print(f"Data URLs: {local_entry.data_urls}")
```

## Basic Promotion

Promote the dataset to ATProto:

```{python}
#| eval: false
# Connect to atmosphere
client = AtmosphereClient()
client.login("myhandle.bsky.social", "app-password")

# Promote to atmosphere
at_uri = promote_to_atmosphere(local_entry, local_index, client)
print(f"Published: {at_uri}")
```

## Promotion with Metadata

Add description, tags, and license:

```{python}
#| eval: false
at_uri = promote_to_atmosphere(
    local_entry,
    local_index,
    client,
    name="experiment-2024-001-v2",   # Override name
    description="Sensor measurements from Lab 302",
    tags=["experiment", "physics", "2024"],
    license="CC-BY-4.0",
)
print(f"Published with metadata: {at_uri}")
```

## Schema Deduplication

The promotion workflow automatically checks for existing schemas:

```{python}
#| eval: false
from atdata.promote import _find_existing_schema

# Check if schema already exists
existing = _find_existing_schema(client, "ExperimentSample", "1.0.0")
if existing:
    print(f"Found existing schema: {existing}")
    print("Will reuse instead of republishing")
else:
    print("No existing schema found, will publish new one")
```

When you promote multiple datasets with the same sample type:

```{python}
#| eval: false
# First promotion: publishes schema
uri1 = promote_to_atmosphere(entry1, local_index, client)

# Second promotion with same schema type + version: reuses existing schema
uri2 = promote_to_atmosphere(entry2, local_index, client)
```

## Data Migration Options

::: {.panel-tabset}

## Keep Existing URLs

By default, promotion keeps the original data URLs:

```{python}
#| eval: false
# Data stays in original S3 location
at_uri = promote_to_atmosphere(local_entry, local_index, client)
```

Benefits:

- Fastest option, no data copying
- Dataset record points to existing URLs
- Requires original storage to remain accessible

## Copy to New Storage

To copy data to a different storage location:

```{python}
#| eval: false
from atdata.local import S3DataStore

# Create new data store
new_store = S3DataStore(
    credentials="new-s3-creds.env",
    bucket="public-datasets",
)

# Promote with data copy
at_uri = promote_to_atmosphere(
    local_entry,
    local_index,
    client,
    data_store=new_store,  # Copy data to new storage
)
```

Benefits:

- Data is copied to new bucket
- Good for moving from private to public storage
- Original storage can be retired

:::

## Verify on Atmosphere

After promotion, verify the dataset is accessible:

```{python}
#| eval: false
from atdata.atmosphere import AtmosphereIndex

atm_index = AtmosphereIndex(client)
entry = atm_index.get_dataset(at_uri)

print(f"Name: {entry.name}")
print(f"Schema: {entry.schema_ref}")
print(f"URLs: {entry.data_urls}")

# Load and iterate
SampleType = atm_index.decode_schema(entry.schema_ref)
ds = atdata.Dataset[SampleType](entry.data_urls[0])

for batch in ds.ordered(batch_size=32):
    print(f"Measurement shape: {batch.measurement.shape}")
    break
```

## Error Handling

```{python}
#| eval: false
try:
    at_uri = promote_to_atmosphere(local_entry, local_index, client)
except KeyError as e:
    # Schema not found in local index
    print(f"Missing schema: {e}")
    print("Publish schema first: local_index.publish_schema(SampleType)")
except ValueError as e:
    # Entry has no data URLs
    print(f"Invalid entry: {e}")
```

## Requirements Checklist

Before promotion:

- [ ] Dataset is in local index (via `Repo.insert()` or `Index.add_entry()`)
- [ ] Schema is published to local index (via `Index.publish_schema()`)
- [ ] AtmosphereClient is authenticated
- [ ] Data URLs are publicly accessible (or will be copied)

## Complete Workflow

```{python}
#| eval: false
# Complete local-to-atmosphere workflow
import numpy as np
from numpy.typing import NDArray
import atdata
from atdata.local import LocalIndex, Repo
from atdata.atmosphere import AtmosphereClient, AtmosphereIndex
from atdata.promote import promote_to_atmosphere
import webdataset as wds

# 1. Define sample type
@atdata.packable
class FeatureSample:
    features: NDArray
    label: int

# 2. Create and store locally
samples = [
    FeatureSample(
        features=np.random.randn(128).astype(np.float32),
        label=i % 10,
    )
    for i in range(1000)
]

with wds.writer.TarWriter("features.tar") as sink:
    for i, s in enumerate(samples):
        sink.write({**s.as_wds, "__key__": f"{i:06d}"})

repo = Repo(s3_credentials="creds.env", hive_path="bucket/features")
dataset = atdata.Dataset[FeatureSample]("features.tar")
local_entry, _ = repo.insert(dataset, name="feature-vectors-v1")

# 3. Publish schema locally
local_index = LocalIndex()
local_index.publish_schema(FeatureSample, version="1.0.0")

# 4. Promote to atmosphere
client = AtmosphereClient()
client.login("myhandle.bsky.social", "app-password")

at_uri = promote_to_atmosphere(
    local_entry,
    local_index,
    client,
    description="Feature vectors for classification",
    tags=["features", "embeddings"],
    license="MIT",
)

print(f"Dataset published: {at_uri}")

# 5. Others can now discover and load
# ds = atdata.load_dataset("@myhandle.bsky.social/feature-vectors-v1")
```

## Next Steps

- **[Atmosphere Reference](../reference/atmosphere.qmd)** - Complete atmosphere API
- **[Protocols](../reference/protocols.qmd)** - Abstract interfaces
- **[Local Storage](../reference/local-storage.qmd)** - Local storage reference

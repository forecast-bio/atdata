{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df3f0691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import atdata\n",
    "from atdata.local import LocalDatasetEntry, S3DataStore, Index\n",
    "import webdataset as wds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f7ea651",
   "metadata": {},
   "outputs": [],
   "source": [
    "@atdata.packable\n",
    "class TrainingSample:\n",
    "    \"\"\"A sample containing features and label for training.\"\"\"\n",
    "    features: NDArray\n",
    "    label: int\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@atdata.packable\n",
    "class TextSample( atdata.PackableSample ):\n",
    "    \"\"\"A sample containing text data.\"\"\"\n",
    "    text: str\n",
    "    category: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55549f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = TextSample(\n",
    "    text = 'Hello',\n",
    "    category = 'test',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462a780b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed0821b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket: analysis-hive\n",
      "Supports streaming: True\n",
      "LocalIndex connected\n"
     ]
    }
   ],
   "source": [
    "# Connect to S3\n",
    "store = S3DataStore( '.credentials/r2-analysis-hive.env',\n",
    "    bucket = \"analysis-hive\"\n",
    ")\n",
    "print(f\"Bucket: {store.bucket}\")\n",
    "print(f\"Supports streaming: {store.supports_streaming()}\")\n",
    "\n",
    "# Connect to Redis\n",
    "index = Index(\n",
    "    data_store = store,\n",
    "    auto_stubs = True,\n",
    ")\n",
    "print( \"LocalIndex connected\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fd2229f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'TrainingSample',\n",
       "  'version': '1.0.0',\n",
       "  'fields': [{'name': 'features',\n",
       "    'fieldType': {'$type': 'local#ndarray', 'dtype': 'float32'},\n",
       "    'optional': False},\n",
       "   {'name': 'label',\n",
       "    'fieldType': {'$type': 'local#primitive', 'primitive': 'int'},\n",
       "    'optional': False}],\n",
       "  '$ref': 'atdata://local/sampleSchema/TrainingSample@1.0.0',\n",
       "  'description': 'A sample containing features and label for training.',\n",
       "  'createdAt': '2026-01-22T22:01:47.560660+00:00'},\n",
       " {'name': 'TextSample',\n",
       "  'version': '1.0.1',\n",
       "  'fields': [{'name': 'text',\n",
       "    'fieldType': {'$type': 'local#primitive', 'primitive': 'str'},\n",
       "    'optional': False},\n",
       "   {'name': 'category',\n",
       "    'fieldType': {'$type': 'local#primitive', 'primitive': 'str'},\n",
       "    'optional': False}],\n",
       "  '$ref': 'atdata://local/sampleSchema/TextSample@1.0.1',\n",
       "  'description': 'A sample containing text data.',\n",
       "  'createdAt': '2026-01-22T22:09:51.907476+00:00'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list( index.list_schemas() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c23765ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = next( index.schemas )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4be08f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'atdata://local/sampleSchema/TrainingSample@1.0.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51829873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published schema: atdata://local/sampleSchema/TrainingSample@1.0.0\n",
      "  - TrainingSample v1.0.0\n",
      "  - TextSample v1.0.1\n",
      "Schema fields: ['features', 'label']\n",
      "Decoded type: TrainingSample\n"
     ]
    }
   ],
   "source": [
    "# Publish a schema\n",
    "schema_ref = index.publish_schema( TrainingSample, version=\"1.0.0\")\n",
    "print(f\"Published schema: {schema_ref}\")\n",
    "\n",
    "# List all schemas\n",
    "for schema in index.list_schemas():\n",
    "    print(f\"  - {schema.get('name', 'Unknown')} v{schema.get('version', '?')}\")\n",
    "\n",
    "# Get schema record\n",
    "schema_record = index.get_schema(schema_ref)\n",
    "print(f\"Schema fields: {[f['name'] for f in schema_record.get('fields', [])]}\")\n",
    "\n",
    "# Decode schema back to a PackableSample class\n",
    "decoded_type = index.decode_schema(schema_ref)\n",
    "print(f\"Decoded type: {decoded_type.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fadbddaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published schema: atdata://local/sampleSchema/TextSample@1.0.1\n",
      "  - TrainingSample v1.0.0\n",
      "  - TextSample v1.0.1\n",
      "Schema fields: ['text', 'category']\n",
      "Decoded type: TextSample\n"
     ]
    }
   ],
   "source": [
    "# Publish a schema\n",
    "schema_ref_2 = index.publish_schema(TextSample, version=\"1.0.1\")\n",
    "print(f\"Published schema: {schema_ref_2}\")\n",
    "\n",
    "# List all schemas\n",
    "for schema in index.list_schemas():\n",
    "    print(f\"  - {schema.get('name', 'Unknown')} v{schema.get('version', '?')}\")\n",
    "\n",
    "# Get schema record\n",
    "schema_record = index.get_schema(schema_ref_2)\n",
    "print(f\"Schema fields: {[f['name'] for f in schema_record.get('fields', [])]}\")\n",
    "\n",
    "# Decode schema back to a PackableSample class\n",
    "decoded_type = index.decode_schema(schema_ref_2)\n",
    "print(f\"Decoded type: {decoded_type.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18e14e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar, TypeAlias, Generic, Callable, Any\n",
    "\n",
    "S = TypeVar( 'S', bound = atdata.PackableSample )\n",
    "V = TypeVar( 'V', bound = atdata.PackableSample )\n",
    "\n",
    "FromAnyTo = Callable[[Any], V]\n",
    "\n",
    "def make_local_lens( f: FromAnyTo[V], remote: type[S], local: type[V] ) -> atdata.Lens[S, V]:\n",
    "    \"\"\"TODO\"\"\"\n",
    "    @atdata.lens\n",
    "    def _to_local( s: S ) -> V:\n",
    "        return f( s )\n",
    "    return _to_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fede400",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.load_schema( 'atdata://local/sampleSchema/TextSample@1.0.1' )\n",
    "TextSampleRemote = index.types.TextSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53979bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = TextSampleRemote(\n",
    "    text = 'hello',\n",
    "    category = 'test',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a3122bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_text_sample( s: Any ) -> TextSample:\n",
    "    return TextSample(\n",
    "        text = s.text,\n",
    "        category = s.category,\n",
    "    )\n",
    "\n",
    "l = make_local_lens( _to_text_sample, TextSampleRemote, TextSample )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a730c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = l( x )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b8d647",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55d944d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# writing data/TextSample_test-000000.tar 0 0.0 GB 0\n",
      "# writing data/TextSample_test-000001.tar 1000 0.0 GB 1000\n",
      "# writing data/TextSample_test-000002.tar 1000 0.0 GB 2000\n",
      "# writing data/TextSample_test-000003.tar 1000 0.0 GB 3000\n",
      "# writing data/TextSample_test-000004.tar 1000 0.0 GB 4000\n",
      "# writing data/TextSample_test-000005.tar 1000 0.0 GB 5000\n",
      "# writing data/TextSample_test-000006.tar 1000 0.0 GB 6000\n",
      "# writing data/TextSample_test-000007.tar 1000 0.0 GB 7000\n",
      "# writing data/TextSample_test-000008.tar 1000 0.0 GB 8000\n",
      "# writing data/TextSample_test-000009.tar 1000 0.0 GB 9000\n"
     ]
    }
   ],
   "source": [
    "import webdataset as wds\n",
    "from uuid import uuid4\n",
    "\n",
    "data_pattern = 'data/TextSample_test-%06d.tar'\n",
    "\n",
    "with wds.writer.ShardWriter( data_pattern, maxcount = 1_000 ) as sink:\n",
    "    for i in range( 10_000 ):\n",
    "        new_sample = TextSample(\n",
    "            text = str( uuid4() ),\n",
    "            category = 'test',\n",
    "        )\n",
    "        sink.write( new_sample.as_wds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5978b632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from atdata import load_dataset\n",
    "\n",
    "ds = (\n",
    "    load_dataset( 'data/TextSample_test-{000000..000009}.tar',\n",
    "        split = 'test'\n",
    "    )\n",
    "    .as_type( TextSample )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc81a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next( iter( ds.ordered() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3beac49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextSample(text='d06a8072-5833-4867-9bc6-03baa3cee75b', category='test')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ebfcc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# writing analysis-hive/prototyping/data--2b5dd738-c9f6-46d4-8c31-e218531601be--000000.tar 0 0.0 GB 0\n"
     ]
    }
   ],
   "source": [
    "entry = index.insert_dataset( ds, \n",
    "    name = 'proto-text-samples-3',\n",
    "    prefix = 'prototyping',\n",
    "    schema_ref = 'atdata://local/sampleSchema/TextSample@1.0.1',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74d68f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LocalDatasetEntry(name='proto-text-samples-3', schema_ref='atdata://local/sampleSchema/TextSample@1.0.1', data_urls=['s3://analysis-hive/prototyping/data--2b5dd738-c9f6-46d4-8c31-e218531601be--000000.tar'], metadata=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51090c3",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* We should make sure that the `s3` URI-scheme here is properly used\n",
    "    * Should we be using the `https` URI since actually this is doing data streaming with `wds`? Or does this indicate that we should think more deeply about the `Dataset` API design and generalizing how we're setting up the `wds` data streaming ...\n",
    "    * No matter what, we're definitely going to want to make sure that we incorporate the actual host details of the `LocalIndex`'s `S3DataStore` for this, since the S3 host is definitely not local.\n",
    "    * Should there be underscores here? These feel like public properties ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90872fe7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "02abbcc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<atdata.dataset.Dataset at 0x114eb9160>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a50853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2736f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "(\"((['curl', '--connect-timeout', '30', '--retry', '30', '--retry-delay', '2', '-f', '-s', '-L', 'https://f5bf77c06cb35b5136ff6d61ab4b7dbc.r2.cloudflarestorage.com/analysis-hive/prototyping/data--2b5dd738-c9f6-46d4-8c31-e218531601be--000000.tar'],), {'bufsize': 8192}): exit 22 (read) {}\", <webdataset.gopen.Pipe object at 0x11425e150>, 'https://f5bf77c06cb35b5136ff6d61ab4b7dbc.r2.cloudflarestorage.com/analysis-hive/prototyping/data--2b5dd738-c9f6-46d4-8c31-e218531601be--000000.tar')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      4\u001b[39m ds = load_dataset( \u001b[33m\"\u001b[39m\u001b[33m@local/proto-text-samples-3\u001b[39m\u001b[33m\"\u001b[39m, TextSample,\n\u001b[32m      5\u001b[39m     split = \u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      7\u001b[39m     index = index,\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# The index resolves the dataset name to URLs and schema\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshuffled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-forecast/atdata/.venv/lib/python3.12/site-packages/webdataset/pipeline.py:105\u001b[39m, in \u001b[36mDataPipeline.iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.repetitions):\n\u001b[32m    104\u001b[39m     count = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-forecast/atdata/.venv/lib/python3.12/site-packages/webdataset/filters.py:520\u001b[39m, in \u001b[36m_map\u001b[39m\u001b[34m(data, f, handler)\u001b[39m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_map\u001b[39m(data, f, handler=reraise_exception):\n\u001b[32m    506\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    507\u001b[39m \u001b[33;03m    Map samples through a function.\u001b[39;00m\n\u001b[32m    508\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    518\u001b[39m \u001b[33;03m        Exception: If the handler doesn't handle an exception.\u001b[39;00m\n\u001b[32m    519\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-forecast/atdata/.venv/lib/python3.12/site-packages/webdataset/filters.py:358\u001b[39m, in \u001b[36m_shuffle\u001b[39m\u001b[34m(data, bufsize, initial, rng, seed, handler)\u001b[39m\n\u001b[32m    356\u001b[39m initial = \u001b[38;5;28mmin\u001b[39m(initial, bufsize)\n\u001b[32m    357\u001b[39m buf = []\n\u001b[32m--> \u001b[39m\u001b[32m358\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-forecast/atdata/.venv/lib/python3.12/site-packages/webdataset/tariterators.py:230\u001b[39m, in \u001b[36mgroup_by_keys\u001b[39m\u001b[34m(data, keys, lcase, suffixes, handler)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Group tarfile contents by keys and yield samples.\u001b[39;00m\n\u001b[32m    215\u001b[39m \n\u001b[32m    216\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m \u001b[33;03m    Iterator over samples.\u001b[39;00m\n\u001b[32m    228\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    229\u001b[39m current_sample = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilesample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01massert\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilesample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-forecast/atdata/.venv/lib/python3.12/site-packages/webdataset/tariterators.py:201\u001b[39m, in \u001b[36mtar_file_expander\u001b[39m\u001b[34m(data, handler, select_files, rename_files, eof_value)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exn:\n\u001b[32m    200\u001b[39m     exn.args = exn.args + (source.get(\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m), source.get(\u001b[33m\"\u001b[39m\u001b[33murl\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexn\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    202\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    203\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-forecast/atdata/.venv/lib/python3.12/site-packages/webdataset/handlers.py:31\u001b[39m, in \u001b[36mreraise_exception\u001b[39m\u001b[34m(exn)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise_exception\u001b[39m(exn):\n\u001b[32m     23\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Re-raise the given exception.\u001b[39;00m\n\u001b[32m     24\u001b[39m \n\u001b[32m     25\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m \u001b[33;03m        The input exception.\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exn\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-forecast/atdata/.venv/lib/python3.12/site-packages/webdataset/tariterators.py:184\u001b[39m, in \u001b[36mtar_file_expander\u001b[39m\u001b[34m(data, handler, select_files, rename_files, eof_value)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m source\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtar_file_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhandler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhandler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mselect_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mselect_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrename_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrename_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01massert\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfname\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m__url__\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-forecast/atdata/.venv/lib/python3.12/site-packages/webdataset/tariterators.py:128\u001b[39m, in \u001b[36mtar_file_iterator\u001b[39m\u001b[34m(fileobj, skip_meta, handler, select_files, rename_files)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtar_file_iterator\u001b[39m(\n\u001b[32m    110\u001b[39m     fileobj: tarfile.TarFile,\n\u001b[32m    111\u001b[39m     skip_meta: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m__[^/]*__($|/)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    114\u001b[39m     rename_files: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    115\u001b[39m ) -> Iterator[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    116\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Iterate over tar file, yielding filename, content pairs for the given tar stream.\u001b[39;00m\n\u001b[32m    117\u001b[39m \n\u001b[32m    118\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    126\u001b[39m \u001b[33;03m        A stream of samples.\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     stream = \u001b[43mtarfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr|*\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m tarinfo \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m    130\u001b[39m         fname = tarinfo.name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/tarfile.py:1883\u001b[39m, in \u001b[36mTarFile.open\u001b[39m\u001b[34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[39m\n\u001b[32m   1880\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mmode must be \u001b[39m\u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1882\u001b[39m compresslevel = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcompresslevel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m9\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1883\u001b[39m stream = \u001b[43m_Stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomptype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mcompresslevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1886\u001b[39m     t = \u001b[38;5;28mcls\u001b[39m(name, filemode, stream, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/tarfile.py:355\u001b[39m, in \u001b[36m_Stream.__init__\u001b[39m\u001b[34m(self, name, mode, comptype, fileobj, bufsize, compresslevel)\u001b[39m\n\u001b[32m    350\u001b[39m     \u001b[38;5;28mself\u001b[39m._extfileobj = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m comptype == \u001b[33m'\u001b[39m\u001b[33m*\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    353\u001b[39m     \u001b[38;5;66;03m# Enable transparent compression detection for the\u001b[39;00m\n\u001b[32m    354\u001b[39m     \u001b[38;5;66;03m# stream interface\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     fileobj = \u001b[43m_StreamProxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m     comptype = fileobj.getcomptype()\n\u001b[32m    358\u001b[39m \u001b[38;5;28mself\u001b[39m.name     = name \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/tarfile.py:583\u001b[39m, in \u001b[36m_StreamProxy.__init__\u001b[39m\u001b[34m(self, fileobj)\u001b[39m\n\u001b[32m    581\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileobj):\n\u001b[32m    582\u001b[39m     \u001b[38;5;28mself\u001b[39m.fileobj = fileobj\n\u001b[32m--> \u001b[39m\u001b[32m583\u001b[39m     \u001b[38;5;28mself\u001b[39m.buf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBLOCKSIZE\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-forecast/atdata/.venv/lib/python3.12/site-packages/webdataset/gopen.py:105\u001b[39m, in \u001b[36mPipe.read\u001b[39m\u001b[34m(self, *args, **kw)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Wrap stream.read and checks status.\u001b[39;00m\n\u001b[32m     96\u001b[39m \n\u001b[32m     97\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    102\u001b[39m \u001b[33;03m    The result of stream.read\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    104\u001b[39m result = \u001b[38;5;28mself\u001b[39m.stream.read(*args, **kw)\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-forecast/atdata/.venv/lib/python3.12/site-packages/webdataset/gopen.py:77\u001b[39m, in \u001b[36mPipe.check_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     75\u001b[39m status = \u001b[38;5;28mself\u001b[39m.proc.poll()\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwait_for_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-forecast/atdata/.venv/lib/python3.12/site-packages/webdataset/gopen.py:92\u001b[39m, in \u001b[36mPipe.wait_for_child\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     87\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     88\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpipe exit [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos.getpid()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.proc.pid\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.args\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minfo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     89\u001b[39m         file=sys.stderr,\n\u001b[32m     90\u001b[39m     )\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.status \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ignore_status \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ignore_errors:\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.args\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: exit \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (read) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minfo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOSError\u001b[39m: (\"((['curl', '--connect-timeout', '30', '--retry', '30', '--retry-delay', '2', '-f', '-s', '-L', 'https://f5bf77c06cb35b5136ff6d61ab4b7dbc.r2.cloudflarestorage.com/analysis-hive/prototyping/data--2b5dd738-c9f6-46d4-8c31-e218531601be--000000.tar'],), {'bufsize': 8192}): exit 22 (read) {}\", <webdataset.gopen.Pipe object at 0x11425e150>, 'https://f5bf77c06cb35b5136ff6d61ab4b7dbc.r2.cloudflarestorage.com/analysis-hive/prototyping/data--2b5dd738-c9f6-46d4-8c31-e218531601be--000000.tar')"
     ]
    }
   ],
   "source": [
    "from atdata import load_dataset\n",
    "\n",
    "# Load from local index\n",
    "ds = load_dataset( \"@local/proto-text-samples-3\", TextSample,\n",
    "    split = 'train',\n",
    "    #\n",
    "    index = index,\n",
    ")\n",
    "\n",
    "# The index resolves the dataset name to URLs and schema\n",
    "for batch in ds.shuffled():\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b30bb49",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* This is also getting linting errors on `load_dataset` that there are no matching overloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c2afcd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://analysis-hive/prototyping/data--4a5ff662-803b-4700-81f4-45f288f6e565--000000.tar'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4238ba",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* We're getting linting errors because of the protocol use for `AbstractIndex`; better to subclass, or is there a way for this to get the protocol adherence?\n",
    "* The S3 URI error is showing up here now because of how dataset loading works! The data is uploaded correctly on my end, but it can't be accessed because of this URI not being the correct way to access the data for `wds` streaming over `https`; we should think of how best to encode this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbedcd2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

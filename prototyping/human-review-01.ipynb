{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df3f0691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import atdata\n",
    "from atdata.local import LocalDatasetEntry, S3DataStore, Index\n",
    "import webdataset as wds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f7ea651",
   "metadata": {},
   "outputs": [],
   "source": [
    "@atdata.packable\n",
    "class TrainingSample:\n",
    "    \"\"\"A sample containing features and label for training.\"\"\"\n",
    "    features: NDArray\n",
    "    label: int\n",
    "\n",
    "@atdata.packable\n",
    "class TextSample:\n",
    "    \"\"\"A sample containing text data.\"\"\"\n",
    "    text: str\n",
    "    category: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55549f64",
   "metadata": {},
   "source": [
    "x = TextSample(\n",
    "    text = 'Hello',\n",
    "    category = 'test',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462a780b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed0821b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket: analysis-hive\n",
      "Supports streaming: True\n",
      "LocalIndex connected\n"
     ]
    }
   ],
   "source": [
    "from redis import Redis\n",
    "\n",
    "# Connect to S3\n",
    "store = S3DataStore( '.credentials/r2-analysis-hive.env',\n",
    "    bucket = \"analysis-hive\"\n",
    ")\n",
    "\n",
    "print(f\"Bucket: {store.bucket}\")\n",
    "print(f\"Supports streaming: {store.supports_streaming()}\")\n",
    "\n",
    "# Connect to Redis\n",
    "index = Index(\n",
    "    data_store = store,\n",
    "    auto_stubs = True,\n",
    ")\n",
    "\n",
    "print(\"LocalIndex connected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5b7e2c",
   "metadata": {},
   "source": [
    "TextSample = index.decode_schema( 'atdata://local/sampleSchema/TextSample@1.0.1' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "301ded22",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = TextSample(\n",
    "    text = 'hello',\n",
    "    category = 'test',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51829873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published schema: atdata://local/sampleSchema/TrainingSample@1.0.0\n",
      "  - TrainingSample v1.0.0\n",
      "Schema fields: ['features', 'label']\n",
      "Decoded type: TrainingSample\n"
     ]
    }
   ],
   "source": [
    "# Publish a schema\n",
    "schema_ref = index.publish_schema(TrainingSample, version=\"1.0.0\")\n",
    "print(f\"Published schema: {schema_ref}\")\n",
    "\n",
    "# List all schemas\n",
    "for schema in index.list_schemas():\n",
    "    print(f\"  - {schema.get('name', 'Unknown')} v{schema.get('version', '?')}\")\n",
    "\n",
    "# Get schema record\n",
    "schema_record = index.get_schema(schema_ref)\n",
    "print(f\"Schema fields: {[f['name'] for f in schema_record.get('fields', [])]}\")\n",
    "\n",
    "# Decode schema back to a PackableSample class\n",
    "decoded_type = index.decode_schema(schema_ref)\n",
    "print(f\"Decoded type: {decoded_type.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fadbddaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published schema: atdata://local/sampleSchema/TextSample@1.0.1\n",
      "  - TrainingSample v1.0.0\n",
      "  - TextSample v1.0.1\n",
      "Schema fields: ['text', 'category']\n",
      "Decoded type: TextSample\n"
     ]
    }
   ],
   "source": [
    "# Publish a schema\n",
    "schema_ref_2 = index.publish_schema(TextSample, version=\"1.0.1\")\n",
    "print(f\"Published schema: {schema_ref_2}\")\n",
    "\n",
    "# List all schemas\n",
    "for schema in index.list_schemas():\n",
    "    print(f\"  - {schema.get('name', 'Unknown')} v{schema.get('version', '?')}\")\n",
    "\n",
    "# Get schema record\n",
    "schema_record = index.get_schema(schema_ref_2)\n",
    "print(f\"Schema fields: {[f['name'] for f in schema_record.get('fields', [])]}\")\n",
    "\n",
    "# Decode schema back to a PackableSample class\n",
    "decoded_type = index.decode_schema(schema_ref_2)\n",
    "print(f\"Decoded type: {decoded_type.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afdc07f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del TextSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a3122bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_atdata_generated_TextSample_1_0_1.TextSample"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.load_schema( 'atdata://local/sampleSchema/TextSample@1.0.1' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "492f1d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "TextSample = index.types.TextSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04cf7394",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = TextSample(\n",
    "    text = 'hello',\n",
    "    category = 'test',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5022c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@atdata.packable\n",
    "class LocalTextSample:\n",
    "    content: str\n",
    "    \"Test\"\n",
    "    category: str\n",
    "    \"stuff\"\n",
    "\n",
    "@atdata.lens\n",
    "def _convert_text_sample( s: TextSample ) -> LocalTextSample:\n",
    "    return LocalTextSample(\n",
    "        content = s.text,\n",
    "        category = s.category,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d785df",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* We get linting errors here on `@atdata.lens` because `LocalTextSample` doesn't show up as a subclass of `PackableSample`; is there a way to resolve this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e70b084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = _convert_text_sample( x )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b8d647",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55d944d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# writing data/TextSample_test-000000.tar 0 0.0 GB 0\n",
      "# writing data/TextSample_test-000001.tar 1000 0.0 GB 1000\n",
      "# writing data/TextSample_test-000002.tar 1000 0.0 GB 2000\n",
      "# writing data/TextSample_test-000003.tar 1000 0.0 GB 3000\n",
      "# writing data/TextSample_test-000004.tar 1000 0.0 GB 4000\n",
      "# writing data/TextSample_test-000005.tar 1000 0.0 GB 5000\n",
      "# writing data/TextSample_test-000006.tar 1000 0.0 GB 6000\n",
      "# writing data/TextSample_test-000007.tar 1000 0.0 GB 7000\n",
      "# writing data/TextSample_test-000008.tar 1000 0.0 GB 8000\n",
      "# writing data/TextSample_test-000009.tar 1000 0.0 GB 9000\n"
     ]
    }
   ],
   "source": [
    "import webdataset as wds\n",
    "from uuid import uuid4\n",
    "\n",
    "data_pattern = 'data/TextSample_test-%06d.tar'\n",
    "\n",
    "with wds.writer.ShardWriter( data_pattern, maxcount = 1_000 ) as sink:\n",
    "    for i in range( 10_000 ):\n",
    "        new_sample = TextSample(\n",
    "            text = str( uuid4() ),\n",
    "            category = 'test',\n",
    "        )\n",
    "        sink.write( new_sample.as_wds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab656145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from atdata import Dataset\n",
    "ds = Dataset[TextSample]( 'data/TextSample_test-{000000..000009}.tar' )\n",
    "x = next( iter( ds.ordered( batch_size = None ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb32688",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* We should make the default for `Dataset.ordered` and `Dataset.shuffled` be to have `batch_size` be `None`, rather than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ebfcc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# writing analysis-hive/prototyping/data--4a5ff662-803b-4700-81f4-45f288f6e565--000000.tar 0 0.0 GB 0\n"
     ]
    }
   ],
   "source": [
    "entry = index.insert_dataset( ds, \n",
    "    name = 'proto-text-samples-2',\n",
    "    prefix = 'prototyping',\n",
    "    schema_ref = 'atdata://local/sampleSchema/TextSample@1.0.1',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e74d68f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LocalDatasetEntry(_name='proto-text-samples-2', _schema_ref='atdata://local/sampleSchema/TextSample@1.0.1', _data_urls=['s3://analysis-hive/prototyping/data--4a5ff662-803b-4700-81f4-45f288f6e565--000000.tar'], _metadata=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51090c3",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* We should make sure that the `s3` URI-scheme here is properly used\n",
    "    * Should we be using the `https` URI since actually this is doing data streaming with `wds`? Or does this indicate that we should think more deeply about the `Dataset` API design and generalizing how we're setting up the `wds` data streaming ...\n",
    "    * No matter what, we're definitely going to want to make sure that we incorporate the actual host details of the `LocalIndex`'s `S3DataStore` for this, since the S3 host is definitely not local.\n",
    "    * Should there be underscores here? These feel like public properties ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90872fe7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a2736f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('s3://analysis-hive/prototyping/data--4a5ff662-803b-4700-81f4-45f288f6e565--000000.tar: no gopen handler defined', 's3://analysis-hive/prototyping/data--4a5ff662-803b-4700-81f4-45f288f6e565--000000.tar')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      4\u001b[39m ds = load_dataset( \u001b[33m\"\u001b[39m\u001b[33m@local/proto-text-samples-2\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     index = index,\n\u001b[32m      6\u001b[39m     split = \u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# The index resolves the dataset name to URLs and schema\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshuffled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-forecast/atdata/.venv/lib/python3.12/site-packages/webdataset/pipeline.py:105\u001b[39m, in \u001b[36mDataPipeline.iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.repetitions):\n\u001b[32m    104\u001b[39m     count = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-forecast/atdata/.venv/lib/python3.12/site-packages/webdataset/filters.py:520\u001b[39m, in \u001b[36m_map\u001b[39m\u001b[34m(data, f, handler)\u001b[39m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_map\u001b[39m(data, f, handler=reraise_exception):\n\u001b[32m    506\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    507\u001b[39m \u001b[33;03m    Map samples through a function.\u001b[39;00m\n\u001b[32m    508\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    518\u001b[39m \u001b[33;03m        Exception: If the handler doesn't handle an exception.\u001b[39;00m\n\u001b[32m    519\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-forecast/atdata/.venv/lib/python3.12/site-packages/webdataset/filters.py:783\u001b[39m, in \u001b[36m_batched\u001b[39m\u001b[34m(data, batchsize, collation_fn, partial)\u001b[39m\n\u001b[32m    770\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    771\u001b[39m \u001b[33;03mCreate batches of the given size.\u001b[39;00m\n\u001b[32m    772\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    780\u001b[39m \u001b[33;03m    Batches of samples.\u001b[39;00m\n\u001b[32m    781\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    782\u001b[39m batch = []\n\u001b[32m--> \u001b[39m\u001b[32m783\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatchsize\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcollation_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-forecast/atdata/.venv/lib/python3.12/site-packages/webdataset/filters.py:358\u001b[39m, in \u001b[36m_shuffle\u001b[39m\u001b[34m(data, bufsize, initial, rng, seed, handler)\u001b[39m\n\u001b[32m    356\u001b[39m initial = \u001b[38;5;28mmin\u001b[39m(initial, bufsize)\n\u001b[32m    357\u001b[39m buf = []\n\u001b[32m--> \u001b[39m\u001b[32m358\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-forecast/atdata/.venv/lib/python3.12/site-packages/webdataset/tariterators.py:230\u001b[39m, in \u001b[36mgroup_by_keys\u001b[39m\u001b[34m(data, keys, lcase, suffixes, handler)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Group tarfile contents by keys and yield samples.\u001b[39;00m\n\u001b[32m    215\u001b[39m \n\u001b[32m    216\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m \u001b[33;03m    Iterator over samples.\u001b[39;00m\n\u001b[32m    228\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    229\u001b[39m current_sample = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilesample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01massert\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilesample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-forecast/atdata/.venv/lib/python3.12/site-packages/webdataset/tariterators.py:178\u001b[39m, in \u001b[36mtar_file_expander\u001b[39m\u001b[34m(data, handler, select_files, rename_files, eof_value)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtar_file_expander\u001b[39m(\n\u001b[32m    160\u001b[39m     data: Iterable[Dict[\u001b[38;5;28mstr\u001b[39m, Any]],\n\u001b[32m    161\u001b[39m     handler: Callable[[\u001b[38;5;167;01mException\u001b[39;00m], \u001b[38;5;28mbool\u001b[39m] = reraise_exception,\n\u001b[32m   (...)\u001b[39m\u001b[32m    164\u001b[39m     eof_value: Optional[Any] = {},\n\u001b[32m    165\u001b[39m ) -> Iterator[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    166\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Expand tar files.\u001b[39;00m\n\u001b[32m    167\u001b[39m \n\u001b[32m    168\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    176\u001b[39m \u001b[33;03m        A stream of samples.\u001b[39;00m\n\u001b[32m    177\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43murl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocal_path\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-forecast/atdata/.venv/lib/python3.12/site-packages/webdataset/tariterators.py:103\u001b[39m, in \u001b[36murl_opener\u001b[39m\u001b[34m(data, handler, **kw)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exn:\n\u001b[32m    102\u001b[39m     exn.args = exn.args + (url,)\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexn\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    104\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-forecast/atdata/.venv/lib/python3.12/site-packages/webdataset/handlers.py:31\u001b[39m, in \u001b[36mreraise_exception\u001b[39m\u001b[34m(exn)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise_exception\u001b[39m(exn):\n\u001b[32m     23\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Re-raise the given exception.\u001b[39;00m\n\u001b[32m     24\u001b[39m \n\u001b[32m     25\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m \u001b[33;03m        The input exception.\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exn\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-forecast/atdata/.venv/lib/python3.12/site-packages/webdataset/tariterators.py:98\u001b[39m, in \u001b[36murl_opener\u001b[39m\u001b[34m(data, handler, **kw)\u001b[39m\n\u001b[32m     96\u001b[39m url = sample[\u001b[33m\"\u001b[39m\u001b[33murl\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     stream = \u001b[43mgopen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m     sample.update(stream=stream)\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m sample\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-forecast/atdata/.venv/lib/python3.12/site-packages/webdataset/gopen.py:591\u001b[39m, in \u001b[36mgopen\u001b[39m\u001b[34m(url, mode, bufsize, **kw)\u001b[39m\n\u001b[32m    589\u001b[39m handler = gopen_schemes[\u001b[33m\"\u001b[39m\u001b[33m__default__\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    590\u001b[39m handler = gopen_schemes.get(pr.scheme, handler)\n\u001b[32m--> \u001b[39m\u001b[32m591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-forecast/atdata/.venv/lib/python3.12/site-packages/webdataset/gopen.py:446\u001b[39m, in \u001b[36mgopen_error\u001b[39m\u001b[34m(url, *args, **kw)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgopen_error\u001b[39m(url, *args, **kw):\n\u001b[32m    436\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Raise a value error.\u001b[39;00m\n\u001b[32m    437\u001b[39m \n\u001b[32m    438\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    444\u001b[39m \u001b[33;03m        ValueError: Always raised with the URL and a message\u001b[39;00m\n\u001b[32m    445\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: no gopen handler defined\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: ('s3://analysis-hive/prototyping/data--4a5ff662-803b-4700-81f4-45f288f6e565--000000.tar: no gopen handler defined', 's3://analysis-hive/prototyping/data--4a5ff662-803b-4700-81f4-45f288f6e565--000000.tar')"
     ]
    }
   ],
   "source": [
    "from atdata import load_dataset\n",
    "\n",
    "# Load from local index\n",
    "ds = load_dataset( \"@local/proto-text-samples-2\",\n",
    "    index = index,\n",
    "    split = 'train',\n",
    ")\n",
    "\n",
    "# The index resolves the dataset name to URLs and schema\n",
    "for batch in ds.shuffled(batch_size=32):\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b30bb49",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* This is also getting linting errors on `load_dataset` that there are no matching overloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c2afcd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://analysis-hive/prototyping/data--4a5ff662-803b-4700-81f4-45f288f6e565--000000.tar'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4238ba",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* We're getting linting errors because of the protocol use for `AbstractIndex`; better to subclass, or is there a way for this to get the protocol adherence?\n",
    "* The S3 URI error is showing up here now because of how dataset loading works! The data is uploaded correctly on my end, but it can't be accessed because of this URI not being the correct way to access the data for `wds` streaming over `https`; we should think of how best to encode this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbedcd2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
